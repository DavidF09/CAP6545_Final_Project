{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "zxQ6Pi28LXVH",
      "metadata": {
        "id": "zxQ6Pi28LXVH"
      },
      "outputs": [],
      "source": [
        "# fundnn.preprocessor\n",
        "import torch\n",
        "import pickle\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def load_data(input_path, file_name):\n",
        "    \"\"\"\n",
        "    Preprocesses input node features and GECs dataSets.\n",
        "\n",
        "    Args:\n",
        "        input_path: Paths to input DataSets.\n",
        "        file_name (str): node features or GECs dataSets file name.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: 2D tensor of train, valid and test sets. Each row is a node, each column is a feature.\n",
        "    \"\"\"\n",
        "\n",
        "    file_path = input_path + file_name\n",
        "    with open(file_path, 'rb') as f:\n",
        "        datasets_dict = pickle.load(f)\n",
        "\n",
        "    if file_name == \"feature_dict.pkl\":\n",
        "        scaler = StandardScaler()\n",
        "        train_tensor = torch.FloatTensor(scaler.fit_transform(datasets_dict['train'].sort_index().values))\n",
        "        valid_tensor = torch.FloatTensor(scaler.transform(datasets_dict['valid'].sort_index().values))\n",
        "        test_tensor = torch.FloatTensor(scaler.transform(datasets_dict['test'].sort_index().values))\n",
        "\n",
        "        print('Node feature dimension:\\ntrain data:{}\\nvalid data:{}\\ntest data:{}\\n'\n",
        "              .format(train_tensor.shape, valid_tensor.shape, test_tensor.shape))\n",
        "        return train_tensor, valid_tensor, test_tensor\n",
        "\n",
        "    elif file_name == \"GECs_dict.pkl\":\n",
        "        train_tensor = torch.FloatTensor(datasets_dict['train'].sort_index().values)\n",
        "        valid_tensor = torch.FloatTensor(datasets_dict['valid'].sort_index().values)\n",
        "        test_array = datasets_dict['test'].sort_index()\n",
        "\n",
        "        print('GECS data dimension:\\ntrain data:{}\\nvalid data:{}\\ntest data:{}\\n'\n",
        "              .format(train_tensor.shape, valid_tensor.shape, test_array.shape))\n",
        "        return train_tensor, valid_tensor, test_array\n",
        "\n",
        "\n",
        "def get_dataloader(batch_size, node_train, gecs_train, node_valid, gecs_valid):\n",
        "    \"\"\"\n",
        "    Build batched data for training and validation sets.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): Batch size hyperparameter(see Class TranscriptionNet_Hyperparameters).\n",
        "        node_train (tensor): Tensor data for training sets of node features.\n",
        "        node_valid (tensor): Tensor data for testing sets of node features.\n",
        "        gecs_train (tensor): Tensor data for training sets of GECs data.\n",
        "        gecs_valid (tensor): Tensor data for testing sets of GECs data.\n",
        "\n",
        "    Returns:\n",
        "        Dataloader of train and valid sets.\n",
        "    \"\"\"\n",
        "    train_dataset = TensorDataset(node_train, gecs_train)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_dataset = TensorDataset(node_valid, gecs_valid)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "    return train_dataloader, valid_dataloader\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "hTf0dudDLMWD",
      "metadata": {
        "id": "hTf0dudDLMWD"
      },
      "outputs": [],
      "source": [
        "# dataprocess.py\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def load_rawdata(input_path):\n",
        "    \"\"\"\n",
        "    Load raw data(node feature and true GECs)\n",
        "    \"\"\"\n",
        "    datasets = []\n",
        "    file_paths = os.listdir(input_path)\n",
        "    for file in file_paths:\n",
        "        file_path = os.path.join(input_path, file)\n",
        "        data = pd.read_csv(file_path, sep=\",\", index_col=0).sort_index()\n",
        "        datasets.append(data)\n",
        "\n",
        "    RNAi = datasets[3].sort_index()\n",
        "    OE = datasets[2].sort_index()\n",
        "    CRISPR = datasets[0].sort_index()\n",
        "    node_feature = datasets[1].sort_index()\n",
        "    return node_feature, RNAi, OE, CRISPR\n",
        "\n",
        "\n",
        "def train_test_val_split(GECs, ratio_train, ratio_test, ratio_valid):\n",
        "    \"\"\"\n",
        "    Split the data into train, test, and validation sets.\n",
        "    \"\"\"\n",
        "    train, middle = train_test_split(GECs, train_size=ratio_train, test_size=ratio_test + ratio_valid, random_state=0)\n",
        "    ratio = ratio_valid / (1 - ratio_train)\n",
        "    test, validation = train_test_split(middle, test_size=ratio, random_state=0)\n",
        "\n",
        "    train = train.sort_index()\n",
        "    test = test.sort_index()\n",
        "    validation = validation.sort_index()\n",
        "    return train, test, validation\n",
        "\n",
        "\n",
        "def save_datasets(df_dict, file_path):\n",
        "    \"\"\"\n",
        "    Save the datasets as pickle files.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'wb') as f:\n",
        "        pickle.dump(df_dict, f)\n",
        "\n",
        "\n",
        "def datasets_split(GECs, feature, save_path):\n",
        "    \"\"\"\n",
        "    Datasets split and data scaler.\n",
        "    \"\"\"\n",
        "    GECs_filter = GECs[GECs.index.isin(feature.index)]\n",
        "    GECs_train, GECs_test, GECs_valid = train_test_val_split(GECs_filter, 0.7, 0.2, 0.1)\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    GECs_train_scaled = scaler.fit_transform(GECs_train.values)\n",
        "    GECs_test_scaled = scaler.fit_transform(GECs_test.values)\n",
        "    GECs_valid_scaled = scaler.fit_transform(GECs_valid.values)\n",
        "\n",
        "    GECs_train = pd.DataFrame(GECs_train_scaled, index=GECs_train.index, columns=GECs_filter.columns).sort_index()\n",
        "    GECs_test = pd.DataFrame(GECs_test_scaled, index=GECs_test.index, columns=GECs_filter.columns).sort_index()\n",
        "    GECs_valid = pd.DataFrame(GECs_valid_scaled, index=GECs_valid.index, columns=GECs_filter.columns).sort_index()\n",
        "\n",
        "    feature_train = feature[feature.index.isin(GECs_train.index)].sort_index()\n",
        "    feature_test = feature[feature.index.isin(GECs_test.index)].sort_index()\n",
        "    feature_valid = feature[feature.index.isin(GECs_valid.index)].sort_index()\n",
        "\n",
        "    GECs_dict = {'train': GECs_train, 'valid': GECs_valid, 'test': GECs_test}\n",
        "    feature_dict = {'train': feature_train, 'valid': feature_valid, 'test': feature_test}\n",
        "\n",
        "    save_datasets(GECs_dict, save_path + 'GECs_dict.pkl')\n",
        "    save_datasets(feature_dict, save_path + 'feature_dict.pkl')\n",
        "\n",
        "    return scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "YjyeonJ5LhYv",
      "metadata": {
        "id": "YjyeonJ5LhYv"
      },
      "outputs": [],
      "source": [
        "# config_parser\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class TranscriptionNet_Hyperparameters(object):\n",
        "    \"\"\"Defines the default TranscriptionNet config parameters.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # FunDNN Model\n",
        "        self.FunDNN_layers = 5  # Number of layers for FunDNN\n",
        "        self.FunDNN_epochs = 1000  # Number of epochs for FunDNN model training\n",
        "        self.FunDNN_batch_size = 32  # Number of GECs in each batch\n",
        "        self.FunDNN_hidden_nodes = 1024  # Number of nodes in each layers for FunDNN\n",
        "        self.FunDNN_dropout_rate = 0.1  # Dropout layer ratio of FunDNN\n",
        "        self.FunDNN_activation_func = nn.LeakyReLU  # Activation function between each layer of FunDNN\n",
        "        self.FunDNN_learning_rate = 0.00035  # Adadelta optimizer learning rate\n",
        "        self.FunDNN_RNAi_path = \"example_data/datasets/RNAi/\"  # RNAi GECs and node feature path\n",
        "        self.FunDNN_OE_path = \"example_data/datasets/RNAi/\"  # OE GECs and node feature path\n",
        "        self.FunDNN_CRISPR_path = \"example_data/datasets/RNAi/\"  # CRISPR GECs and node feature path\n",
        "        self.FunDNN_save_path = \"example_data/result/\"\n",
        "\n",
        "        # GenSAN Model\n",
        "        self.GenSAN_heads = 2  # Number of attention heads of row-wise self-attention block\n",
        "        self.GenSAN_blocks = 3  # Number of transformer encoder units for GenSAN\n",
        "        self.GenSAN_recycles = 3  # Recycle times of GenSAN model\n",
        "        self.GenSAN_epochs = 110  # Number of epochs for GenSAN model training\n",
        "        self.GenSAN_warmup_epochs = 5  # Number of warm-up epochs for GenSAN model training\n",
        "        self.GenSAN_batch_size = 32  # Number of GECs in each batch\n",
        "        self.GenSAN_GECs_dimension = 978  # The last dimension of GEC size(978).\n",
        "        self.GenSAN_hidden_nodes = 1024  # Number of nodes in each feed forward neural network layer\n",
        "        self.GenSAN_dropout_rate = 0.05  # Dropout layer ratio of GenSAN\n",
        "        self.GenSAN_learning_rate = 0.0000045  # Adam optimizer learning rate\n",
        "        self.GenSAN_weight_decay = 1e-5  # Adam optimizer weight_decay\n",
        "        self.GenSAN_save_path = \"example_data/result/\"\n",
        "\n",
        "        self.PMSELoss_beta = 0.1  # Weight hyperparameter of the combination of mse and pcc\n",
        "\n",
        "\n",
        "class Device:\n",
        "    \"\"\"Returns the currently used device by calling `Device()`.\n",
        "\n",
        "    Returns:\n",
        "        str: Either \"cuda\" or \"cpu\".\n",
        "    \"\"\"\n",
        "\n",
        "    _device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    def __new__(cls) -> str:\n",
        "        return cls._device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5iksooK7sJNP",
      "metadata": {
        "id": "5iksooK7sJNP"
      },
      "outputs": [],
      "source": [
        "# pmseloss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def PMSELoss(true, predict, beta):\n",
        "    \"\"\"\n",
        "    Loss function combining the mean squared error (MSE) and Pearson correlation losses.\n",
        "\n",
        "    Args:\n",
        "        true (tensor): Tensor data of true GECs data.\n",
        "        predict (tensor): Tensor data of GECs predicted by the model.\n",
        "        beta (float): Weight hyperparameter of the combination of mse and pcc(see Class TranscriptionNet_Hyperparameters).\n",
        "\n",
        "    Returns:\n",
        "        Float: Combination loss, mse loss and pearson_loss.\n",
        "    \"\"\"\n",
        "\n",
        "    error = true - predict\n",
        "    sqr_error = torch.square(error)\n",
        "    mse_loss = torch.mean(sqr_error)\n",
        "\n",
        "    m_pred = torch.mean(predict, dim=1, keepdim=True)\n",
        "    m_true = torch.mean(true, dim=1, keepdim=True)\n",
        "    pred_m, true_m = predict - m_pred, true - m_true\n",
        "\n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    pearson_loss = torch.mean(1 - cos(pred_m, true_m))\n",
        "\n",
        "    loss = (1 - beta) * mse_loss + beta * pearson_loss\n",
        "    return loss, mse_loss, pearson_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6Icc5lgLjet",
      "metadata": {
        "id": "b6Icc5lgLjet"
      },
      "outputs": [],
      "source": [
        "# fundnn.train_function\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# from config_parser import Device\n",
        "# from PMSELoss import PMSELoss\n",
        "\n",
        "\n",
        "def train_function(model, train_dataloader, optimizer, beta):\n",
        "    \"\"\"\n",
        "    FunDNN model training function.\n",
        "\n",
        "    Args:\n",
        "        model : FunDNN initialization model.\n",
        "        train_dataloader : Training set batch data.\n",
        "        optimizer : Adadelta optimizer.\n",
        "        beta (float): Weight hyperparameter of the combination of mse and pcc(see Class TranscriptionNet_Hyperparameters).\n",
        "\n",
        "    Returns:\n",
        "        Float: Combination loss, mse loss and pearson_loss of train sets.\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = 0\n",
        "    mse_loss = 0\n",
        "    pcc_loss = 0\n",
        "    num_batches = len(train_dataloader)\n",
        "\n",
        "    model.train()\n",
        "    for node_feature, gecs_data in train_dataloader:\n",
        "        node_feature = node_feature.to(Device())\n",
        "        gecs_data = gecs_data.to(Device())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predict_gecs = model(node_feature)\n",
        "        loss, mse, pcc = PMSELoss(gecs_data, predict_gecs, beta)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            train_loss += loss.item()\n",
        "            mse_loss += mse.item()\n",
        "            pcc_loss += pcc.item()\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    mse_loss /= num_batches\n",
        "    pcc_loss /= num_batches\n",
        "    return train_loss, mse_loss, pcc_loss\n",
        "\n",
        "\n",
        "def valid_function(model, valid_dataloader, beta):\n",
        "    \"\"\"\n",
        "    FunDNN model validation function.\n",
        "\n",
        "    Args:\n",
        "        model : The FunDNN model after training on the training set.\n",
        "        valid_dataloader : Validation set batch data.\n",
        "        beta (float): Weight hyperparameter of the combination of mse and pcc(see Class TranscriptionNet_Hyperparameters).\n",
        "\n",
        "    Returns:\n",
        "        Float: Combination loss, mse loss and pearson_loss of valid sets.\n",
        "    \"\"\"\n",
        "\n",
        "    valid_loss = 0\n",
        "    mse_loss = 0\n",
        "    pcc_loss = 0\n",
        "    num_batches = len(valid_dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for node_feature, gecs_data in valid_dataloader:\n",
        "            node_feature = node_feature.to(Device())\n",
        "            gecs_data = gecs_data.to(Device())\n",
        "\n",
        "            predict_gecs = model(node_feature)\n",
        "            loss, mse, pcc = PMSELoss(gecs_data, predict_gecs, beta)\n",
        "            valid_loss += loss.item()\n",
        "            mse_loss += mse.item()\n",
        "            pcc_loss += pcc.item()\n",
        "\n",
        "    valid_loss /= num_batches\n",
        "    mse_loss /= num_batches\n",
        "    pcc_loss /= num_batches\n",
        "    return valid_loss, mse_loss, pcc_loss\n",
        "\n",
        "\n",
        "# def test_evaluate(best_model, feature_test, gecs_test):\n",
        "#     \"\"\"\n",
        "#     FunDNN model test evaluation function.\n",
        "\n",
        "#     Args:\n",
        "#         best_model : The FunDNN model after all iterations of training.\n",
        "#         feature_test (tensor): Test set of node features.\n",
        "#         gecs_test (ndarray): Test set of GECs data.\n",
        "\n",
        "#     Returns:\n",
        "#         None\n",
        "#     \"\"\"\n",
        "\n",
        "#     feature_test = feature_test.to(Device())\n",
        "#     feature_test_predict = best_model(feature_test).cpu().detach().numpy()\n",
        "\n",
        "#     # feature_test_predict_df = pd.DataFrame(feature_test_predict, index=net_test.index)\n",
        "#     # feature_test_predict_df.to_csv(save_path + \"feature_test_predict.csv\", index=True)\n",
        "\n",
        "#     d = []\n",
        "#     pcc = []\n",
        "#     for i in range(feature_test_predict.shape[0]):\n",
        "#         pearson = np.corrcoef(feature_test_predict[i], gecs_test[i])[0, 1]\n",
        "#         item_d, _ = stats.ks_2samp(feature_test_predict[i], gecs_test[i])\n",
        "#         pcc.append(pearson)\n",
        "#         d.append(item_d)\n",
        "#     abs_pcc = abs(np.array(pcc))\n",
        "#     abs_pcc_mean = abs_pcc.mean()\n",
        "#     d_mean = np.array(d).mean()\n",
        "\n",
        "#     mse = mean_squared_error(gecs_test, feature_test_predict)\n",
        "\n",
        "#     print('=' * 30)\n",
        "#     print('test evaluate result:\\nAverage pcc: {}\\nAverage mse: {}\\nAverage D: {}'\n",
        "#           .format(abs_pcc_mean, mse, d_mean))\n",
        "#     print('=' * 30)\n",
        "\n",
        "#     # return abs_pearson\n",
        "\n",
        "# def test_evaluate(best_model, feature_test, gecs_test):\n",
        "#     \"\"\"\n",
        "#     FunDNN model test evaluation function.\n",
        "\n",
        "#     Args:\n",
        "#         best_model : The FunDNN model after all iterations of training.\n",
        "#         feature_test (tensor): Test set of node features.\n",
        "#         gecs_test (ndarray): Test set of GECs data.\n",
        "\n",
        "#     Returns:\n",
        "#         None\n",
        "#     \"\"\"\n",
        "\n",
        "#     try:\n",
        "#         print(\"Starting test_evaluate function...\")\n",
        "\n",
        "#         # Move the feature test to the device\n",
        "#         feature_test = feature_test.to(Device())\n",
        "#         print(f\"Feature test shape: {feature_test.shape}\")\n",
        "\n",
        "#         # Get the model predictions\n",
        "#         feature_test_predict = best_model(feature_test).cpu().detach().numpy()\n",
        "#         print(f\"Feature test prediction shape: {feature_test_predict.shape}\")\n",
        "\n",
        "#         # Check if there are NaN or infinite values in predictions\n",
        "#         if np.any(np.isnan(feature_test_predict)) or np.any(np.isinf(feature_test_predict)):\n",
        "#             print(\"Warning: NaN or Infinite values detected in predictions!\")\n",
        "\n",
        "#         # Check if there are NaN or infinite values in gecs_test\n",
        "#         if np.any(np.isnan(gecs_test)) or np.any(np.isinf(gecs_test)):\n",
        "#             print(\"Warning: NaN or Infinite values detected in gecs_test!\")\n",
        "\n",
        "#         # Ensure that feature_test_predict and gecs_test have the same number of samples\n",
        "#         if feature_test_predict.shape[0] != gecs_test.shape[0]:\n",
        "#             print(f\"Mismatch in number of samples: feature_test_predict has {feature_test_predict.shape[0]} samples, but gecs_test has {gecs_test.shape[0]} samples.\")\n",
        "#             return\n",
        "\n",
        "#         d = []\n",
        "#         pcc = []\n",
        "#         for i in range(feature_test_predict.shape[0]):\n",
        "#             # Print first 10 elements of feature_test_predict and gecs_test for debugging\n",
        "#             if i < 10:\n",
        "#                 print(f\"Sample {i} - feature_test_predict: {feature_test_predict[i][:10]} ...\")\n",
        "#                 print(f\"Sample {i} - gecs_test: {gecs_test[i][:10]} ...\")\n",
        "\n",
        "#             # Calculate Pearson correlation and KS test\n",
        "#             pearson = np.corrcoef(feature_test_predict[i], gecs_test[i])[0, 1]\n",
        "#             item_d, _ = stats.ks_2samp(feature_test_predict[i], gecs_test[i])\n",
        "\n",
        "#             pcc.append(pearson)\n",
        "#             d.append(item_d)\n",
        "\n",
        "#         abs_pcc = abs(np.array(pcc))\n",
        "#         abs_pcc_mean = abs_pcc.mean()\n",
        "#         d_mean = np.array(d).mean()\n",
        "\n",
        "#         # Calculate Mean Squared Error (MSE)\n",
        "#         mse = mean_squared_error(gecs_test, feature_test_predict)\n",
        "\n",
        "#         print('=' * 30)\n",
        "#         print('Test evaluate result:')\n",
        "#         print(f\"Average PCC: {abs_pcc_mean}\")\n",
        "#         print(f\"Average MSE: {mse}\")\n",
        "#         print(f\"Average D: {d_mean}\")\n",
        "#         print('=' * 30)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred in test_evaluate: {e}\")\n",
        "#         raise\n",
        "\n",
        "def test_evaluateFunDNN(best_model, feature_test, gecs_test):\n",
        "    \"\"\"\n",
        "    FunDNN model test evaluation function.\n",
        "\n",
        "    Args:\n",
        "        best_model : The FunDNN model after all iterations of training.\n",
        "        feature_test (tensor): Test set of node features.\n",
        "        gecs_test (ndarray): Test set of GECs data.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        feature_test = feature_test.to(Device())\n",
        "\n",
        "        feature_test_predict = best_model(feature_test).cpu().detach().numpy()\n",
        "\n",
        "        # debug stuff bc original wasnt working\n",
        "        # check if there are nan or infinite values in predictions\n",
        "        if np.any(np.isnan(feature_test_predict)) or np.any(np.isinf(feature_test_predict)):\n",
        "            print(\"warning: nan/infinite values detected in predictions\")\n",
        "\n",
        "        # check if there are nan or infinite values in gecs_test\n",
        "        if np.any(np.isnan(gecs_test)) or np.any(np.isinf(gecs_test)):\n",
        "            print(\"warning: nan or infinite values detected in gecs_test\")\n",
        "\n",
        "        # ensure feature_test_predict and gecs_test have the same number of samples\n",
        "        if feature_test_predict.shape[0] != gecs_test.shape[0]:\n",
        "            print(f\"mismatch in number of samples: feature_test_predict has {feature_test_predict.shape[0]} samples, but gecs_test has {gecs_test.shape[0]} samples.\")\n",
        "            return\n",
        "\n",
        "        # # check the type and structure of gecs_test\n",
        "        # print(f\"Type of gecs_test: {type(gecs_test)}\")\n",
        "        # print(f\"Index of gecs_test (first 10 rows): {gecs_test.index[:10]}\")\n",
        "\n",
        "        # Use .iloc to access positional indexing\n",
        "        d = []\n",
        "        pcc = []\n",
        "        for i in range(feature_test_predict.shape[0]):\n",
        "            # if i < 10:\n",
        "                # print(f\"Sample {i} - feature_test_predict: {feature_test_predict[i][:10]} ...\")\n",
        "                # Access gecs_test using .iloc for positional indexing\n",
        "                # print(f\"Sample {i} - gecs_test (iloc): {gecs_test.iloc[i][:10]} ...\")\n",
        "\n",
        "\n",
        "            # calculate pearson correlation and ks test\n",
        "            pearson = np.corrcoef(feature_test_predict[i], gecs_test.iloc[i])[0, 1]\n",
        "            item_d, _ = stats.ks_2samp(feature_test_predict[i], gecs_test.iloc[i])\n",
        "\n",
        "            pcc.append(pearson)\n",
        "            d.append(item_d)\n",
        "\n",
        "        abs_pcc = abs(np.array(pcc))\n",
        "        abs_pcc_mean = abs_pcc.mean()\n",
        "        d_mean = np.array(d).mean()\n",
        "\n",
        "        # calculate MSE\n",
        "        mse = mean_squared_error(gecs_test, feature_test_predict)\n",
        "\n",
        "        print('=' * 30)\n",
        "        print('Test evaluate result:')\n",
        "        print(f\"Average PCC: {abs_pcc_mean}\")\n",
        "        print(f\"Average MSE: {mse}\")\n",
        "        print(f\"Average D: {d_mean}\")\n",
        "        print('=' * 30)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred in test_evaluate: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def plot_loss_figure(epochs, train_loss, valid_loss):\n",
        "    \"\"\"\n",
        "    Draw the training loss value image\n",
        "\n",
        "    Args:\n",
        "        epochs (int): Number of epochs for FunDNN model training\n",
        "        train_loss (list): List of training set loss values.\n",
        "        valid_loss (list): List of valid set loss values.\n",
        "    \"\"\"\n",
        "    plt.rcParams['font.family'] = 'Arial'\n",
        "    plt.rcParams['xtick.labelsize'] = 22\n",
        "    plt.rcParams['ytick.labelsize'] = 22\n",
        "    plt.rcParams['axes.titlesize'] = 28\n",
        "    plt.rcParams['axes.labelsize'] = 28\n",
        "    plt.rcParams['legend.fontsize'] = 22\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "    plt.figure(figsize=(7, 7), dpi=144)\n",
        "\n",
        "    plt.title('Loss figure')\n",
        "    plt.plot(range(epochs), train_loss, color='red', linestyle='--', label='train loss', linewidth=2)\n",
        "    plt.plot(range(epochs), valid_loss, color='dodgerblue', linestyle='-', label='valid loss', linewidth=2)\n",
        "    plt.legend(loc='upper right', frameon=False)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss values')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def trainFunDNN(epochs, model, train_dataloader, valid_dataloader, optimizer, beta):\n",
        "    \"\"\"\n",
        "    Train the FunDNN model.\n",
        "\n",
        "    Args:\n",
        "        epochs (int): Number of epochs for FunDNN model training\n",
        "        model (nn.Module): FunDNN model\n",
        "        train_dataloader : Training set batch data.\n",
        "        valid_dataloader : Validation set batch data.\n",
        "        optimizer : Adadelta optimizer.\n",
        "        beta (float): Weight hyperparameter of the combination of mse and pcc(see Class TranscriptionNet_Hyperparameters).\n",
        "    Returns:\n",
        "        best_model (nn.Module): The trained FunDNN model with the lowest validation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    min_loss = float(\"inf\")\n",
        "    best_model = None\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        tra_loss, tra_mse_loss, tra_cor_loss = train_function(model=model,\n",
        "                                                              train_dataloader=train_dataloader,\n",
        "                                                              optimizer=optimizer,\n",
        "                                                              beta=beta)\n",
        "        val_loss, val_mse_loss, val_cor_loss = valid_function(model=model,\n",
        "                                                              valid_dataloader=valid_dataloader,\n",
        "                                                              beta=beta)\n",
        "        train_losses.append(tra_loss)\n",
        "        valid_losses.append(val_loss)\n",
        "\n",
        "        if val_loss < min_loss:\n",
        "            min_loss = val_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "        print('end of epoch:{:3d} | time:{:5.2f}s | train loss:{:5.5f} | valid loss:{:5.5f} | train MseLoss:{:5.5f} | '\n",
        "              'train PccLoss:{:5.5f} | valid MseLoss:{:5.5f} | valid PccLoss:{:5.5f}'\n",
        "              .format(epoch, (time.time() - epoch_start_time), tra_loss, val_loss, tra_mse_loss, tra_cor_loss,\n",
        "                      val_mse_loss, val_cor_loss))\n",
        "\n",
        "    # plot_loss_figure(epochs, train_losses, valid_losses)\n",
        "\n",
        "    return best_model\n",
        "\n",
        "\n",
        "def feature_predict(best_model, node_feature, save_path, name):\n",
        "    \"\"\"\n",
        "    Predict the pre-GECs\n",
        "\n",
        "    Args:\n",
        "        best_model (nn.Module): The trained FunDNN model with the lowest validation loss.\n",
        "        node_feature: All network nodes embedded features.\n",
        "        save_path: Path to save the trained model.\n",
        "        name: GECs type(RNAi, OE or CRISPR)\n",
        "\n",
        "    Returns:\n",
        "        pre_GECs (dataframe): pre-GECs.\n",
        "    \"\"\"\n",
        "    node_feature_tensor = torch.FloatTensor(node_feature.values)\n",
        "    node_feature_tensor = node_feature_tensor.to(Device())\n",
        "\n",
        "    node_feature_predict = best_model(node_feature_tensor).cpu().detach().numpy()\n",
        "\n",
        "    node_feature_predict = pd.DataFrame(node_feature_predict, index=node_feature.index).sort_index()\n",
        "    node_feature_predict.to_csv(save_path + name + \"_pre_GECs.csv\", index=True, sep=\",\")\n",
        "\n",
        "    print('\\npredict finish:\\npre_GECs size:{}\\n'.format(node_feature_predict.shape))\n",
        "    return node_feature_predict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "L25_wxSGLaLw",
      "metadata": {
        "id": "L25_wxSGLaLw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# from config_parser import Device\n",
        "# from FunDNN.train_function import train, test_evaluate, feature_predict\n",
        "\n",
        "\n",
        "class FunDNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers: int,\n",
        "                 hidden_nodes: int,\n",
        "                 activate_func: nn.Module,\n",
        "                 dropout_rate: float):\n",
        "        \"\"\"\n",
        "        The FunDNN model.\n",
        "\n",
        "        Args:\n",
        "            num_layers (int): Number of layers for FunDNN.\n",
        "            hidden_nodes (int): Number of nodes in each layers for FunDNN.\n",
        "            activate_func (nn.Module): Activation function between each layer of FunDNN.\n",
        "            dropout_rate (float, optional): Dropout layer ratio of FunDNN.\n",
        "        \"\"\"\n",
        "\n",
        "        super(FunDNN, self).__init__()\n",
        "        layers = [nn.Linear(512, hidden_nodes), activate_func(), nn.Dropout(dropout_rate)]\n",
        "        for _ in range(num_layers - 3):\n",
        "            layers.extend([nn.Linear(hidden_nodes, hidden_nodes), activate_func(), nn.Dropout(dropout_rate)])\n",
        "        layers.extend([nn.Linear(hidden_nodes, hidden_nodes), nn.Tanh(), nn.Dropout(dropout_rate)])\n",
        "        layers.append(nn.Linear(hidden_nodes, 978))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass logic.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): 2D tensor of node features.Each row is a node, each column is a feature.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: 2D tensor of .\n",
        "        \"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# def run_model(num_layers, hidden_nodes, activate_func, dropout_rate,\n",
        "#               learning_rate, epochs, train_dataloader, valid_dataloader, beta,\n",
        "#               feature_test, gecs_test, save_path, node_feature, name):\n",
        "#     \"\"\"\n",
        "#     FunDNN model training process.\n",
        "\n",
        "#     Args:\n",
        "#         num_layers (int): Number of layers for FunDNN.\n",
        "#         hidden_nodes (int): Number of nodes in each layers for FunDNN.\n",
        "#         activate_func (nn.Module): Activation function between each layer of FunDNN.\n",
        "#         dropout_rate (float): Dropout layer ratio of FunDNN\n",
        "#         learning_rate (float): Adadelta optimizer learning rate\n",
        "#         epochs (int): Number of epochs for FunDNN model training\n",
        "#         train_dataloader: Training set batch data.\n",
        "#         valid_dataloader: Validation set batch data.\n",
        "#         beta (float): Weight hyperparameter of the combination of mse and pcc(see Class TranscriptionNet_Hyperparameters).\n",
        "#         feature_test (tensor): Test set of node features.\n",
        "#         gecs_test (ndarray): Test set of GECs data.\n",
        "#         save_path: Path to save the trained model.\n",
        "#         node_feature: All network nodes embedded features.\n",
        "#         name: GECs type(RNAi, OE or CRISPR)\n",
        "\n",
        "#     Returns:\n",
        "#         best_model (nn.Module): The trained FunDNN model with the lowest validation loss.\n",
        "#     \"\"\"\n",
        "\n",
        "#     model = FunDNN(num_layers=num_layers,\n",
        "#                    hidden_nodes=hidden_nodes,\n",
        "#                    activate_func=activate_func,\n",
        "#                    dropout_rate=dropout_rate)\n",
        "#     model = model.to(Device())\n",
        "\n",
        "#     optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#     best_model = train(epochs=epochs,\n",
        "#                        model=model,\n",
        "#                        train_dataloader=train_dataloader,\n",
        "#                        valid_dataloader=valid_dataloader,\n",
        "#                        optimizer=optimizer,\n",
        "#                        beta=beta)\n",
        "\n",
        "#     test_evaluate(best_model, feature_test, gecs_test)\n",
        "\n",
        "#     # torch.save(best_model, save_path + \"FunDNN best model.pt\")\n",
        "\n",
        "#     pre_GECs = feature_predict(best_model=best_model,\n",
        "#                                node_feature=node_feature,\n",
        "#                                save_path=save_path,\n",
        "#                                name=name)\n",
        "#     return pre_GECs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "BNaaXeMprXZx",
      "metadata": {
        "id": "BNaaXeMprXZx"
      },
      "outputs": [],
      "source": [
        "# fundnn.model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from config_parser import Device\n",
        "# from FunDNN.train_function import train, test_evaluate, feature_predict\n",
        "\n",
        "\n",
        "class FunDNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers: int,\n",
        "                 hidden_nodes: int,\n",
        "                 activate_func: nn.Module,\n",
        "                 dropout_rate: float):\n",
        "        \"\"\"\n",
        "        The FunDNN model.\n",
        "\n",
        "        Args:\n",
        "            num_layers (int): Number of layers for FunDNN.\n",
        "            hidden_nodes (int): Number of nodes in each layers for FunDNN.\n",
        "            activate_func (nn.Module): Activation function between each layer of FunDNN.\n",
        "            dropout_rate (float, optional): Dropout layer ratio of FunDNN.\n",
        "        \"\"\"\n",
        "\n",
        "        super(FunDNN, self).__init__()\n",
        "        layers = [nn.Linear(512, hidden_nodes), activate_func(), nn.Dropout(dropout_rate)]\n",
        "        for _ in range(num_layers - 3):\n",
        "            layers.extend([nn.Linear(hidden_nodes, hidden_nodes), activate_func(), nn.Dropout(dropout_rate)])\n",
        "        layers.extend([nn.Linear(hidden_nodes, hidden_nodes), nn.Tanh(), nn.Dropout(dropout_rate)])\n",
        "        layers.append(nn.Linear(hidden_nodes, 978))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass logic.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): 2D tensor of node features.Each row is a node, each column is a feature.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: 2D tensor of .\n",
        "        \"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "def run_model(num_layers, hidden_nodes, activate_func, dropout_rate,\n",
        "              learning_rate, epochs, train_dataloader, valid_dataloader, beta,\n",
        "              feature_test, gecs_test, save_path, node_feature, name, warmup_epoch):\n",
        "    \"\"\"\n",
        "    FunDNN model training process.\n",
        "\n",
        "    Args:\n",
        "        num_layers (int): Number of layers for FunDNN.\n",
        "        hidden_nodes (int): Number of nodes in each layers for FunDNN.\n",
        "        activate_func (nn.Module): Activation function between each layer of FunDNN.\n",
        "        dropout_rate (float): Dropout layer ratio of FunDNN\n",
        "        learning_rate (float): Adadelta optimizer learning rate\n",
        "        epochs (int): Number of epochs for FunDNN model training\n",
        "        train_dataloader: Training set batch data.\n",
        "        valid_dataloader: Validation set batch data.\n",
        "        beta (float): Weight hyperparameter of the combination of mse and pcc(see Class TranscriptionNet_Hyperparameters).\n",
        "        feature_test (tensor): Test set of node features.\n",
        "        gecs_test (ndarray): Test set of GECs data.\n",
        "        save_path: Path to save the trained model.\n",
        "        node_feature: All network nodes embedded features.\n",
        "        name: GECs type(RNAi, OE or CRISPR)\n",
        "\n",
        "    Returns:\n",
        "        best_model (nn.Module): The trained FunDNN model with the lowest validation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    model = FunDNN(num_layers=num_layers,\n",
        "                   hidden_nodes=hidden_nodes,\n",
        "                   activate_func=activate_func,\n",
        "                   dropout_rate=dropout_rate)\n",
        "    model = model.to(Device())\n",
        "\n",
        "    optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_model = trainFunDNN(epochs=epochs,\n",
        "                       model=model,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       valid_dataloader=valid_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       beta=beta)\n",
        "                      #  warmup_epoch=warmup_epoch,\n",
        "                      #  learning_rate=learning_rate)\n",
        "    # print(\"here\")\n",
        "    test_evaluateFunDNN(best_model, feature_test, gecs_test)\n",
        "    # print(\"here2\")\n",
        "    # torch.save(best_model, save_path + \"FunDNN best model.pt\")\n",
        "    # print(pre_GECs.index)\n",
        "\n",
        "    pre_GECs = feature_predict(best_model=best_model,\n",
        "                               node_feature=node_feature,\n",
        "                               save_path=save_path,\n",
        "                               name=name)\n",
        "    # print(pre_GECs.index)\n",
        "    return pre_GECs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "46gbXOVorkml",
      "metadata": {
        "id": "46gbXOVorkml"
      },
      "outputs": [],
      "source": [
        "# gensan.preprocessor\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def load_GECs(input_path, file_name):\n",
        "    \"\"\"\n",
        "    Preprocesses input GECs dataSets.\n",
        "\n",
        "    Args:\n",
        "        input_path: Paths to input DataSets.\n",
        "        file_name (str): GECs dataSets folder name.\n",
        "\n",
        "    Returns:\n",
        "        Dataframe: train, valid and test sets. Each row is a node, each column is a feature.\n",
        "    \"\"\"\n",
        "    file_path = input_path + file_name\n",
        "    with open(file_path, 'rb') as f:\n",
        "        datasets_dict = pickle.load(f)\n",
        "\n",
        "    train = datasets_dict[\"train\"].sort_index()\n",
        "    valid = datasets_dict[\"valid\"].sort_index()\n",
        "    test = datasets_dict[\"test\"].sort_index()\n",
        "    return train, valid, test\n",
        "\n",
        "\n",
        "def GECs_combine(true_GECs, predict_GECs):\n",
        "    \"\"\"\n",
        "    Combines true and predicted GECs dataSets.\n",
        "\n",
        "    Args:\n",
        "        true_GECs (DataFrame): True GECs dataSets.\n",
        "        predict_GECs (DataFrame): Predicted GECs dataSets.\n",
        "\n",
        "    Returns:\n",
        "        Dataframe: Combined dataSets.\n",
        "    \"\"\"\n",
        "    predict = predict_GECs[~ predict_GECs.index.isin(true_GECs.index)]\n",
        "    predict.columns = true_GECs.columns\n",
        "    combine = pd.concat([true_GECs, predict], axis=0).sort_index()\n",
        "    return combine\n",
        "\n",
        "\n",
        "def get_datasets(GECs, pre_GECs, combine1, combine2):\n",
        "    \"\"\"\n",
        "    Gets train, valid and test sets.\n",
        "\n",
        "    Args:\n",
        "        GECs (DataFrame): True GECs dataSets(train, valid or test).\n",
        "        pre_GECs (DataFrame): Predicted GECs dataSets.\n",
        "        combine1 (DataFrame): Combined GECs dataSets.\n",
        "        combine2 (DataFrame): Combined GECs dataSets.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: train, valid or test sets.\n",
        "    \"\"\"\n",
        "\n",
        "    index = GECs.index.sort_values()\n",
        "\n",
        "    # input\n",
        "    pre_gecs_item = pre_GECs[pre_GECs.index.isin(index)].sort_index()\n",
        "    combine1_item = combine1[combine1.index.isin(index)].sort_index()\n",
        "    combine2_item = combine2[combine2.index.isin(index)].sort_index()\n",
        "\n",
        "    datasets = []\n",
        "    for i in range(len(index)):\n",
        "        single_pre_gecs = pre_gecs_item[pre_gecs_item.index == index[i]].values\n",
        "        single_combine1 = combine1_item[combine1_item.index == index[i]].values\n",
        "        single_combine2 = combine2_item[combine2_item.index == index[i]].values\n",
        "        single_data = np.concatenate((single_pre_gecs, single_combine1, single_combine2), axis=0)\n",
        "        datasets.append(single_data)\n",
        "\n",
        "    datasets_array = np.array(datasets)\n",
        "    return datasets_array\n",
        "\n",
        "\n",
        "def datasets_scaled(trainSets, validSets, testSets):\n",
        "    \"\"\"\n",
        "    Standardization of training, validation, and testing data sets.\n",
        "    \"\"\"\n",
        "\n",
        "    train_gecs, train_cmap, train_gene = trainSets.shape\n",
        "    valid_gecs, valid_cmap, valid_gene = validSets.shape\n",
        "    test_gecs, test_cmap, test_gene = testSets.shape\n",
        "\n",
        "    trainSets_item = trainSets.reshape((train_gecs, train_cmap * train_gene))\n",
        "    validSets_item = validSets.reshape((valid_gecs, valid_cmap * valid_gene))\n",
        "    testSets_item = testSets.reshape((test_gecs, test_cmap * test_gene))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    train_scaled_item = scaler.fit_transform(trainSets_item)\n",
        "    valid_scaled_item = scaler.transform(validSets_item)\n",
        "    test_scaled_item = scaler.transform(testSets_item)\n",
        "\n",
        "    train_scaled = train_scaled_item.reshape((train_gecs, train_cmap, train_gene))\n",
        "    valid_scaled = valid_scaled_item.reshape((valid_gecs, valid_cmap, valid_gene))\n",
        "    test_scaled = test_scaled_item.reshape((test_gecs, test_cmap, test_gene))\n",
        "\n",
        "    train = torch.FloatTensor(train_scaled)\n",
        "    valid = torch.FloatTensor(valid_scaled)\n",
        "    test = torch.FloatTensor(test_scaled)\n",
        "    return train, valid, test\n",
        "\n",
        "\n",
        "def get_pre_GECs(pre_GECs, combine1, combine2):\n",
        "    \"\"\"\n",
        "    GenSAN model prediction data\n",
        "\n",
        "    Args:\n",
        "        pre_GECs (DataFrame): Predicted GECs dataSets.\n",
        "        combine1 (DataFrame): Combined GECs dataSets.\n",
        "        combine2 (DataFrame): Combined GECs dataSets.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: 3D Tensor of prediction data.\n",
        "    \"\"\"\n",
        "\n",
        "    index = pre_GECs.index.sort_values()\n",
        "\n",
        "    pre_GECs = pre_GECs.sort_index()\n",
        "    combine1 = combine1.sort_index()\n",
        "    combine2 = combine2.sort_index()\n",
        "\n",
        "    predict_data = []\n",
        "    for i in range(len(index)):\n",
        "        pre_GECs_item = pre_GECs[pre_GECs.index == index[i]].values\n",
        "        combine1_item = combine1[combine1.index == index[i]].values\n",
        "        combine2_item = combine2[combine2.index == index[i]].values\n",
        "        predict_data_item = np.concatenate((pre_GECs_item, combine1_item, combine2_item), axis=0)\n",
        "        predict_data.append(predict_data_item)\n",
        "\n",
        "    predict_data = torch.FloatTensor(np.array(predict_data))\n",
        "\n",
        "    return predict_data\n",
        "\n",
        "\n",
        "def GenSAN_preprocessor(true_GECs1, true_GECs2, predict_GECs1, predict_GECs2, pre_GECS, input_path, file_name):\n",
        "    \"\"\"\n",
        "    GenSAN data preprocessing\n",
        "    \"\"\"\n",
        "\n",
        "    combine_GECs1 = GECs_combine(true_GECs1, predict_GECs1)\n",
        "    combine_GECs2 = GECs_combine(true_GECs2, predict_GECs2)\n",
        "\n",
        "    train_GECs, valid_GECs, test_GECs = load_GECs(input_path, file_name)\n",
        "\n",
        "    train = get_datasets(train_GECs, pre_GECS, combine_GECs1, combine_GECs2)\n",
        "    valid = get_datasets(valid_GECs, pre_GECS, combine_GECs1, combine_GECs2)\n",
        "    test = get_datasets(test_GECs, pre_GECS, combine_GECs1, combine_GECs2)\n",
        "\n",
        "    train_data, valid_data, test_data = datasets_scaled(train, valid, test)\n",
        "\n",
        "    print('pre-GECS dimension:\\ntrain data:{}\\nvalid data:{}\\ntest data:{}\\n'\n",
        "              .format(train_data.shape, valid_data.shape, test_data.shape))\n",
        "\n",
        "    return train_data, valid_data, test_data, combine_GECs1, combine_GECs2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "SnuHr8ZhsWIM",
      "metadata": {
        "id": "SnuHr8ZhsWIM"
      },
      "outputs": [],
      "source": [
        "# gensan utils\n",
        "import math\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import cos, pi\n",
        "import torch.nn.functional as f\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self,\n",
        "                 GECs_dimension: int,\n",
        "                 hidden_nodes: int,\n",
        "                 dropout_rate: float):\n",
        "        \"\"\"\n",
        "        Feed forward neural network layer.\n",
        "\n",
        "        Args:\n",
        "            GECs_dimension (int): The last dimension of GEC size(978).\n",
        "            hidden_nodes (int): Number of nodes in each layers for Feed forward neural network.\n",
        "            dropout_rate (float, optional): Dropout layer ratio of Feed forward neural network.\n",
        "        \"\"\"\n",
        "\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w1 = nn.Linear(GECs_dimension, hidden_nodes)\n",
        "        self.w2 = nn.Linear(hidden_nodes, GECs_dimension)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(self.dropout(f.leaky_relu(self.w1(x))))\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dimension=978, eps=1e-6):\n",
        "        \"\"\"\n",
        "        LayerNorm module.\n",
        "\n",
        "        Args:\n",
        "            dimension (int, optional): The dimension of norm.Defaults to 978\n",
        "            eps (float, optional): A value added to the denominator for numerical stability. Defaults to 1e-6.\n",
        "        \"\"\"\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a2 = nn.Parameter(torch.ones(dimension))\n",
        "        self.b2 = nn.Parameter(torch.zeros(dimension))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"输入参数x代表来自上一层的输出\"\"\"\n",
        "        mean = x.mean(-1, keepdims=True)\n",
        "        std = x.std(-1, keepdims=True)\n",
        "        return self.a2 * (x - mean) / (std + self.eps) + self.b2\n",
        "\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    def __init__(self, dropout_rate):\n",
        "        \"\"\"\n",
        "        SublayerConnection module.\n",
        "\n",
        "        Args:\n",
        "            dropout_rate (float): Dropout rate of each sub-layer.\n",
        "        \"\"\"\n",
        "        super(SublayerConnection, self).__init__()\n",
        "\n",
        "        self.norm = LayerNorm()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"\n",
        "        Forward pass logic.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input of the previous layer or sub-layer.\n",
        "            sublayer (nn.Module): The sublayer function in the sublayer connection.\n",
        "        \"\"\"\n",
        "\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "def attention(query, key, value, dropout=None):\n",
        "    \"\"\"\n",
        "    Implementation of attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        query: A tensor of queries.\n",
        "        key: A tensor of key.\n",
        "        value: A tensor of values.\n",
        "        dropout (nn.Dropout): The dropout layer.Default is None\n",
        "    \"\"\"\n",
        "\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.as_tensor(torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k))\n",
        "\n",
        "    p_attn = f.softmax(scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "\n",
        "    return torch.matmul(p_attn, value)\n",
        "\n",
        "\n",
        "class ColumnAttention(nn.Module):\n",
        "    def __init__(self, dropout_rate):\n",
        "        \"\"\"\n",
        "        column-wise attention.\n",
        "\n",
        "        Args:\n",
        "            dropout_rate (float): Dropout layer ratio of column-wise attention.\n",
        "        \"\"\"\n",
        "        super(ColumnAttention, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        \"\"\"\n",
        "        Forward pass logic.\n",
        "\n",
        "        Args:\n",
        "            query (Tensor): A tensor of queries.\n",
        "            key (Tensor): A tensor of key.\n",
        "            value (Tensor): A tensor of values.\n",
        "        \"\"\"\n",
        "        query = torch.transpose(query, -1, -2)\n",
        "        key = torch.transpose(key, -1, -2)\n",
        "        value = torch.transpose(value, -1, -2)\n",
        "\n",
        "        col_attn = attention(query, key, value, dropout=self.dropout)\n",
        "\n",
        "        col_attn = torch.transpose(col_attn, -1, -2)\n",
        "\n",
        "        return col_attn\n",
        "\n",
        "\n",
        "def clones(module, num_clone):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_clone)])\n",
        "\n",
        "\n",
        "class RowAttention(nn.Module):\n",
        "    def __init__(self, heads, GECs_dimension, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Multi-head attention.\n",
        "\n",
        "        Args:\n",
        "            heads (int): The number of heads.\n",
        "            GECs_dimension (int): The last dimension of GEC size(978).\n",
        "            dropout_rate (float): The dropout layer.Default is None\n",
        "        \"\"\"\n",
        "\n",
        "        super(RowAttention, self).__init__()\n",
        "\n",
        "        assert GECs_dimension % heads == 0\n",
        "\n",
        "        self.d_k = GECs_dimension // heads\n",
        "        self.heads = heads\n",
        "        self.linears = clones(nn.Linear(GECs_dimension, GECs_dimension), 4)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        batch_size = query.size(0)\n",
        "        query, key, value = \\\n",
        "            [model(x).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n",
        "             for model, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        x = attention(query, key, value, dropout=self.dropout)\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.heads * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 GECs_dimension: int,\n",
        "                 hidden_nodes: int,\n",
        "                 dropout_rate: float):\n",
        "        \"\"\"\n",
        "        Generator.\n",
        "\n",
        "        Args:\n",
        "            GECs_dimension (int): The last dimension of GEC size(978).\n",
        "            hidden_nodes (int): Number of nodes in each layer.\n",
        "            dropout_rate (float, optional): Dropout layer ratio.\n",
        "        \"\"\"\n",
        "\n",
        "        super(Generator, self).__init__()\n",
        "        self.w1 = nn.Linear(GECs_dimension, hidden_nodes)\n",
        "        self.w2 = nn.Linear(hidden_nodes, GECs_dimension)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output_item = self.w2(self.dropout(torch.tanh(self.w1(x))))\n",
        "        output = output_item[:, 0, :]\n",
        "        return output\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, warmup_epoch, current_epoch, max_epoch, lr_min=0, lr_max=0.1, warmup=True):\n",
        "    \"\"\"\n",
        "    Cosine preheating mechanism regulates learning rate.\n",
        "\n",
        "    Args:\n",
        "        optimizer (nn.Module): Adam optimizer.\n",
        "        warmup_epoch (int): Number of warm-up epochs.\n",
        "        current_epoch (int): Current epoch.\n",
        "        max_epoch (int): Number of epochs for model training.\n",
        "        lr_min (float, optional): Minimum learning rate. The default is 0\n",
        "        lr_max (float, optional): Max learning rate. The default is 0.1\n",
        "        warmup (bool, optional)\n",
        "    \"\"\"\n",
        "    warmup_epoch = warmup_epoch if warmup else 0\n",
        "    if current_epoch < warmup_epoch:\n",
        "        lr = lr_max * current_epoch / warmup_epoch\n",
        "    else:\n",
        "        lr = lr_min + (lr_max - lr_min) * (\n",
        "                1 + cos(pi * (current_epoch - warmup_epoch) / (max_epoch - warmup_epoch))) / 2\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "SEYnsjCGrpon",
      "metadata": {
        "id": "SEYnsjCGrpon"
      },
      "outputs": [],
      "source": [
        "# gensan_train_function\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# from config_parser import Device\n",
        "# from PMSELoss import PMSELoss\n",
        "# from GenSAN.utils import adjust_learning_rate\n",
        "# from GenSAN.model import GenSAN_model\n",
        "\n",
        "\n",
        "def train_function(model, train_dataloader, optimizer, beta):\n",
        "    \"\"\"\n",
        "    GenSAN model training function.\n",
        "\n",
        "    Args:\n",
        "        model : GenSAN initialization model.\n",
        "        train_dataloader : Training set batch data.\n",
        "        optimizer : Adam optimizer.\n",
        "        beta (float): Weight hyperparameter of the combination of mse and pcc(see Class TranscriptionNet_Hyperparameters).\n",
        "\n",
        "    Returns:\n",
        "        Float: Combination loss, mse loss and pearson_loss of train sets.\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = 0\n",
        "    mse_loss = 0\n",
        "    pcc_loss = 0\n",
        "    num_batches = len(train_dataloader)\n",
        "\n",
        "    model.train()\n",
        "    for node_feature, gecs_data in train_dataloader:\n",
        "        node_feature = node_feature.to(Device())\n",
        "        gecs_data = gecs_data.to(Device())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predict_gecs = model(node_feature)\n",
        "        loss, mse, pcc = PMSELoss(gecs_data, predict_gecs, beta)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            train_loss += loss.item()\n",
        "            mse_loss += mse.item()\n",
        "            pcc_loss += pcc.item()\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    mse_loss /= num_batches\n",
        "    pcc_loss /= num_batches\n",
        "    return train_loss, mse_loss, pcc_loss\n",
        "\n",
        "\n",
        "def valid_function(model, valid_dataloader, beta):\n",
        "    \"\"\"\n",
        "    GenSAN model validation function.\n",
        "\n",
        "    Args:\n",
        "        model : The GenSAN model after training on the training set.\n",
        "        valid_dataloader : Validation set batch data.\n",
        "        beta (float): Weight hyperparameter of the combination of mse and pcc(see Class TranscriptionNet_Hyperparameters).\n",
        "\n",
        "    Returns:\n",
        "        Float: Combination loss, mse loss and pearson_loss of valid sets.\n",
        "    \"\"\"\n",
        "\n",
        "    valid_loss = 0\n",
        "    mse_loss = 0\n",
        "    pcc_loss = 0\n",
        "    num_batches = len(valid_dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for node_feature, gecs_data in valid_dataloader:\n",
        "            node_feature = node_feature.to(Device())\n",
        "            gecs_data = gecs_data.to(Device())\n",
        "\n",
        "            predict_gecs = model(node_feature)\n",
        "            loss, mse, pcc = PMSELoss(gecs_data, predict_gecs, beta)\n",
        "            valid_loss += loss.item()\n",
        "            mse_loss += mse.item()\n",
        "            pcc_loss += pcc.item()\n",
        "\n",
        "    valid_loss /= num_batches\n",
        "    mse_loss /= num_batches\n",
        "    pcc_loss /= num_batches\n",
        "    return valid_loss, mse_loss, pcc_loss\n",
        "\n",
        "\n",
        "# def test_evaluateGenSAN(best_model, pre_gecs_test, gecs_test):\n",
        "#     \"\"\"\n",
        "#     GenSAN model test evaluation function.\n",
        "\n",
        "#     Args:\n",
        "#         best_model : The GenSAN model after all iterations of training.\n",
        "#         pre_gecs_test (tensor): Test set of pre-GECs.\n",
        "#         gecs_test (ndarray): Test set of GECs data.\n",
        "#     \"\"\"\n",
        "\n",
        "#     feature_test = pre_gecs_test.to(Device())\n",
        "#     feature_test_predict = best_model(feature_test).cpu().detach().numpy()\n",
        "\n",
        "#     # feature_test_predict_df = pd.DataFrame(feature_test_predict, index=net_test.index)\n",
        "#     # feature_test_predict_df.to_csv(save_path + \"feature_test_predict.csv\", index=True)\n",
        "\n",
        "#     d = []\n",
        "#     pcc = []\n",
        "#     for i in range(feature_test_predict.shape[0]):\n",
        "#         pearson = np.corrcoef(feature_test_predict[i], gecs_test[i])[0, 1]\n",
        "#         item_d, _ = stats.ks_2samp(feature_test_predict[i], gecs_test[i])\n",
        "#         pcc.append(pearson)\n",
        "#         d.append(item_d)\n",
        "#     abs_pcc = abs(np.array(pcc))\n",
        "#     abs_pcc_mean = abs_pcc.mean()\n",
        "#     d_mean = np.array(d).mean()\n",
        "\n",
        "#     mse = mean_squared_error(gecs_test, feature_test_predict)\n",
        "\n",
        "#     print('=' * 30)\n",
        "#     print('test evaluate result:\\nAverage pcc: {}\\nAverage mse: {}\\nAverage D: {}'\n",
        "#           .format(abs_pcc_mean, mse, d_mean))\n",
        "#     print('=' * 30)\n",
        "\n",
        "#     # return abs_pearson\n",
        "\n",
        "\n",
        "def test_evaluateGenSAN(best_model, pre_gecs_test, gecs_test):\n",
        "    \"\"\"\n",
        "    GenSAN model test evaluation function.\n",
        "\n",
        "    Args:\n",
        "        best_model : The FunDNN model after all iterations of training.\n",
        "        feature_test (tensor): Test set of node features.\n",
        "        gecs_test (ndarray): Test set of GECs data.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        feature_test = pre_gecs_test.to(Device())\n",
        "\n",
        "        feature_test_predict = best_model(feature_test).cpu().detach().numpy()\n",
        "\n",
        "        # debug stuff bc original wasnt working\n",
        "        # check if there are nan or infinite values in predictions\n",
        "        if np.any(np.isnan(feature_test_predict)) or np.any(np.isinf(feature_test_predict)):\n",
        "            print(\"warning: nan/infinite values detected in predictions\")\n",
        "\n",
        "        # check if there are nan or infinite values in gecs_test\n",
        "        if np.any(np.isnan(gecs_test)) or np.any(np.isinf(gecs_test)):\n",
        "            print(\"warning: nan or infinite values detected in gecs_test\")\n",
        "\n",
        "        # ensure feature_test_predict and gecs_test have the same number of samples\n",
        "        if feature_test_predict.shape[0] != gecs_test.shape[0]:\n",
        "            print(f\"mismatch in number of samples: feature_test_predict has {feature_test_predict.shape[0]} samples, but gecs_test has {gecs_test.shape[0]} samples.\")\n",
        "            return\n",
        "\n",
        "        # # check the type and structure of gecs_test\n",
        "        # print(f\"Type of gecs_test: {type(gecs_test)}\")\n",
        "        # print(f\"Index of gecs_test (first 10 rows): {gecs_test.index[:10]}\")\n",
        "\n",
        "        # Use .iloc to access positional indexing\n",
        "        d = []\n",
        "        pcc = []\n",
        "        for i in range(feature_test_predict.shape[0]):\n",
        "            # if i < 10:\n",
        "                # print(f\"Sample {i} - feature_test_predict: {feature_test_predict[i][:10]} ...\")\n",
        "                # Access gecs_test using .iloc for positional indexing\n",
        "                # print(f\"Sample {i} - gecs_test (iloc): {gecs_test.iloc[i][:10]} ...\")\n",
        "\n",
        "\n",
        "            # calculate pearson correlation and ks test\n",
        "            pearson = np.corrcoef(feature_test_predict[i], gecs_test.iloc[i])[0, 1]\n",
        "            item_d, _ = stats.ks_2samp(feature_test_predict[i], gecs_test.iloc[i])\n",
        "\n",
        "            pcc.append(pearson)\n",
        "            d.append(item_d)\n",
        "\n",
        "        abs_pcc = abs(np.array(pcc))\n",
        "        abs_pcc_mean = abs_pcc.mean()\n",
        "        d_mean = np.array(d).mean()\n",
        "\n",
        "        # calculate MSE\n",
        "        mse = mean_squared_error(gecs_test, feature_test_predict)\n",
        "\n",
        "        print('=' * 30)\n",
        "        print('Test evaluate result:')\n",
        "        print(f\"Average PCC: {abs_pcc_mean}\")\n",
        "        print(f\"Average MSE: {mse}\")\n",
        "        print(f\"Average D: {d_mean}\")\n",
        "        print('=' * 30)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred in test_evaluate: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def plot_loss_figure(epochs, train_loss, valid_loss):\n",
        "    \"\"\"\n",
        "    Draw the training loss value image\n",
        "\n",
        "    Args:\n",
        "        epochs (int): Number of epochs for FunDNN model training\n",
        "        train_loss (list): List of training set loss values.\n",
        "        valid_loss (list): List of valid set loss values.\n",
        "    \"\"\"\n",
        "    plt.rcParams['font.family'] = 'Arial'\n",
        "    plt.rcParams['xtick.labelsize'] = 22\n",
        "    plt.rcParams['ytick.labelsize'] = 22\n",
        "    plt.rcParams['axes.titlesize'] = 28\n",
        "    plt.rcParams['axes.labelsize'] = 28\n",
        "    plt.rcParams['legend.fontsize'] = 22\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "    plt.figure(figsize=(7, 7), dpi=144)\n",
        "\n",
        "    plt.title('Loss figure')\n",
        "    plt.plot(range(epochs), train_loss, color='red', linestyle='--', label='train loss', linewidth=2)\n",
        "    plt.plot(range(epochs), valid_loss, color='dodgerblue', linestyle='-', label='valid loss', linewidth=2)\n",
        "    plt.legend(loc='upper right', frameon=False)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss values')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train(epochs, model, train_dataloader, valid_dataloader, optimizer, beta, warmup_epoch, learning_rate):\n",
        "    \"\"\"\n",
        "    Train the GenSAN model.\n",
        "\n",
        "    Args:\n",
        "        epochs (int): Number of epochs for GenSAN model training\n",
        "        model (nn.Module): GenSAN model\n",
        "        train_dataloader : Training set batch data.\n",
        "        valid_dataloader : Validation set batch data.\n",
        "        optimizer : Adam optimizer.\n",
        "        beta (float): Weight hyperparameter of the combination of mse and pcc(see Class TranscriptionNet_Hyperparameters).\n",
        "        warmup_epoch (int): Number of warm-up epochs for GenSAN model training\n",
        "        learning_rate (float): The initial learning rate.\n",
        "    Returns:\n",
        "        best_model (nn.Module): The trained GenSAN model with the lowest validation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    min_loss = float(\"inf\")\n",
        "    best_model = None\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        tra_loss, tra_mse_loss, tra_cor_loss = train_function(model=model,\n",
        "                                                              train_dataloader=train_dataloader,\n",
        "                                                              optimizer=optimizer,\n",
        "                                                              beta=beta)\n",
        "        val_loss, val_mse_loss, val_cor_loss = valid_function(model=model,\n",
        "                                                              valid_dataloader=valid_dataloader,\n",
        "                                                              beta=beta)\n",
        "        train_losses.append(tra_loss)\n",
        "        valid_losses.append(val_loss)\n",
        "\n",
        "        if val_loss < min_loss:\n",
        "            min_loss = val_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "        adjust_learning_rate(optimizer=optimizer, warmup_epoch=warmup_epoch, current_epoch=epoch, max_epoch=epochs,\n",
        "                             lr_min=0, lr_max=learning_rate, warmup=True)\n",
        "\n",
        "        print('end of epoch:{:3d} | time:{:5.2f}s | train loss:{:5.5f} | valid loss:{:5.5f} | train MseLoss:{:5.5f} | '\n",
        "              'train PccLoss:{:5.5f} | valid MseLoss:{:5.5f} | valid PccLoss:{:5.5f}'\n",
        "              .format(epoch, (time.time() - epoch_start_time), tra_loss, val_loss, tra_mse_loss, tra_cor_loss,\n",
        "                      val_mse_loss, val_cor_loss))\n",
        "\n",
        "    # plot_loss_figure(epochs, train_losses, valid_losses)\n",
        "\n",
        "    return best_model\n",
        "\n",
        "\n",
        "def feature_predict(input_matrix, best_model, pre_GECs, scaler, save_path, name, length=64):\n",
        "    \"\"\"\n",
        "    Predict the GECs and inverse MinMaxScaler\n",
        "\n",
        "    Args:\n",
        "        input_matrix (tensor): 3D tensor composed of pre-GECs of RNAi, OE, and CRISPR.\n",
        "        best_model (nn.Module): The trained GenSAN model with the lowest validation loss.\n",
        "        length (int): Divide the length of input_matrix.Defaults to 64.\n",
        "        pre_GECs (dataframe): Single pre-GECs data(RNAi, OE or CRISPR).\n",
        "        scaler: MinMaxScaler object.\n",
        "        save_path : Path to save predict GECs.\n",
        "        name (str): GECs type(RNAi, OE or CRISPR)\n",
        "    Returns:\n",
        "        predict_GECs (dataframe): predict GECs.\n",
        "    \"\"\"\n",
        "\n",
        "    sub_matrix = [input_matrix[i:i + length, :, :] for i in range(0, len(input_matrix), length)]\n",
        "    predict_GECs = []\n",
        "    for sub in range(len(sub_matrix)):\n",
        "        inputs = sub_matrix[sub].to(Device())\n",
        "        sub_predict_GECs = pd.DataFrame(best_model(inputs).cpu().detach().numpy())\n",
        "        predict_GECs.append(sub_predict_GECs)\n",
        "    predict_GECs = pd.concat(predict_GECs, ignore_index=True)\n",
        "\n",
        "    inverse_predict_GECs = scaler.inverse_transform(predict_GECs.values)\n",
        "    inverse_predict_GECs = pd.DataFrame(inverse_predict_GECs, index=pre_GECs.index)\n",
        "    inverse_predict_GECs.to_csv(save_path + name + \"_predict_GECs.csv\", index=True, sep=\",\")\n",
        "\n",
        "    print('\\npredict finish:\\npredict GECs:{}\\n'.format(inverse_predict_GECs.shape))\n",
        "    return predict_GECs\n",
        "\n",
        "\n",
        "def run_GenSAN_model(blocks, GECs_dimension, hidden_nodes, heads, dropout_rate, recycles,\n",
        "                     learning_rate, weight_decay,\n",
        "                     epochs, train_dataloader, valid_dataloader, beta, warmup_epoch,\n",
        "                     pre_gecs_test, gecs_test, save_path,\n",
        "                     input_matrix, length, pre_GECs, scaler, name):\n",
        "    \"\"\"\n",
        "    FunDNN model training process.\n",
        "\n",
        "    Args:\n",
        "        blocks (int): Number of transformer encoder units for GenSAN.\n",
        "        GECs_dimension (int): The last dimension of GEC size(978).\n",
        "        hidden_nodes (int): Number of nodes in each feed forward neural network layer.\n",
        "        heads (int): Number of attention heads of row-wise self-attention block.\n",
        "        dropout_rate (float): Dropout layer ratio of GenSAN.\n",
        "        recycles (int): Recycle times of GenSAN model.\n",
        "        learning_rate (float): Adam optimizer initial learning rate.\n",
        "        weight_decay (float): Adam optimizer weight decay parameter.\n",
        "        epochs (int): Number of epochs for GenSAN model training\n",
        "        train_dataloader : Training set batch data.\n",
        "        valid_dataloader : Validation set batch data.\n",
        "        beta (float): Weight hyperparameter of the combination of mse and pcc(see Class TranscriptionNet_Hyperparameters).\n",
        "        warmup_epoch (int):Number of warm-up epochs for GenSAN model training\n",
        "        pre_gecs_test (tensor): Test set of pre-GECs..\n",
        "        gecs_test (ndarray): Test set of GECs data.\n",
        "        save_path : Path to save predict GECs and best model.\n",
        "        input_matrix (tensor): 3D tensor composed of pre-GECs of RNAi, OE, and CRISPR.\n",
        "        length (int): Divide the length of input_matrix\n",
        "        pre_GECs (dataframe): Single pre-GECs data(RNAi, OE or CRISPR).\n",
        "        scaler: MinMaxScaler object.\n",
        "        name (str): GECs type(RNAi, OE or CRISPR)\n",
        "\n",
        "    Returns:\n",
        "        best_model (nn.Module): The trained GenSAN model with the lowest validation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    model = GenSAN_model(blocks=blocks,\n",
        "                         GECs_dimension=GECs_dimension,\n",
        "                         hidden_nodes=hidden_nodes,\n",
        "                         heads=heads,\n",
        "                         dropout_rate=dropout_rate,\n",
        "                         recycles=recycles)\n",
        "    model = model.to(Device())\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate,\n",
        "                                 weight_decay=weight_decay)\n",
        "\n",
        "    best_model = train(epochs=epochs,\n",
        "                       model=model,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       valid_dataloader=valid_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       beta=beta,\n",
        "                       warmup_epoch=warmup_epoch,\n",
        "                       learning_rate=learning_rate)\n",
        "    print(\"here\")\n",
        "    test_evaluateGenSAN(best_model, pre_gecs_test, gecs_test)\n",
        "    print(\"here2\")\n",
        "    # torch.save(best_model, save_path + \"FunDNN best model.pt\")\n",
        "\n",
        "    predict_GECs = feature_predict(input_matrix=input_matrix,\n",
        "                                   length=length,\n",
        "                                   best_model=best_model,\n",
        "                                   pre_GECs=pre_GECs,\n",
        "                                   scaler=scaler,\n",
        "                                   save_path=save_path,\n",
        "                                   name=name)\n",
        "    return predict_GECs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "dHxMw0tqrsZ4",
      "metadata": {
        "id": "dHxMw0tqrsZ4"
      },
      "outputs": [],
      "source": [
        "# gensan_module\n",
        "import torch.nn as nn\n",
        "# from GenSAN.utils import clones, SublayerConnection, LayerNorm\n",
        "\n",
        "\n",
        "class BlockLayer(nn.Module):\n",
        "    def __init__(self, GECs_dimension, col_attn, row_attn, feed_forward, dropout_rate):\n",
        "        \"\"\"\n",
        "        Single transformer encoder block.\n",
        "\n",
        "        Args:\n",
        "            GECs_dimension (int): The last dimension of GEC size(978).\n",
        "            col_attn (nn.Module): attention object for column attention.\n",
        "            row_attn (nn.Module): Multi-head attention object for row attention.\n",
        "            feed_forward (nn.Module): Feed forward neural network object.\n",
        "            dropout_rate (float, optional): Dropout layer ratio.\n",
        "        \"\"\"\n",
        "        super(BlockLayer, self).__init__()\n",
        "        self.GECs_dimension = GECs_dimension\n",
        "        self.col_attn = col_attn\n",
        "        self.row_attn = row_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(dropout_rate), 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sublayer[0](x, lambda x: self.row_attn(x, x, x))\n",
        "\n",
        "        x = self.sublayer[1](x, lambda x: self.col_attn(x, x, x))\n",
        "\n",
        "        output = self.sublayer[2](x, self.feed_forward)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Blocks(nn.Module):\n",
        "    def __init__(self, layer, blocks):\n",
        "        \"\"\"\n",
        "        Transformer encoder blocks.\n",
        "\n",
        "        Args:\n",
        "            layer (nn.Module): Single transformer encoder block.\n",
        "            blocks (int): Number of transformer encoder units for GenSAN.\n",
        "        \"\"\"\n",
        "\n",
        "        super(Blocks, self).__init__()\n",
        "        self.layers = clones(layer, blocks)\n",
        "        self.norm = LayerNorm()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class BlocksRecycling(nn.Module):\n",
        "    def __init__(self, block_layers, recycles):\n",
        "        \"\"\"\n",
        "        Transformer encoder blocks with recycling\n",
        "\n",
        "        Args:\n",
        "            block_layers (nn.Module): Single transformer encoder block.\n",
        "            recycles (int): Recycle times of Transformer encoder blocks\n",
        "        \"\"\"\n",
        "\n",
        "        super(BlocksRecycling, self).__init__()\n",
        "        self.models = clones(block_layers, recycles)\n",
        "        self.norm = LayerNorm()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for model in self.models:\n",
        "            x = model(x)\n",
        "            x = self.norm(x)\n",
        "            x += x\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "4mHv-Vda-Yqy",
      "metadata": {
        "id": "4mHv-Vda-Yqy"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch.nn as nn\n",
        "# from GenSAN.utils import ColumnAttention, RowAttention, PositionwiseFeedForward, Generator\n",
        "# from GenSAN.module import BlockLayer, Blocks, BlocksRecycling\n",
        "\n",
        "\n",
        "class GenSAN(nn.Module):\n",
        "    def __init__(self, util, generator):\n",
        "        \"\"\"\n",
        "        GenSAN model\n",
        "\n",
        "        Args:\n",
        "            util (nn.Module): Transformer encoder blocks with recycling.\n",
        "            generator (nn.Module): Generator block.\n",
        "        \"\"\"\n",
        "        super(GenSAN, self).__init__()\n",
        "        self.util = util\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.util(x)\n",
        "        return self.generator(output)\n",
        "\n",
        "\n",
        "def GenSAN_model(blocks=3, GECs_dimension=978, hidden_nodes=1024, heads=6, dropout_rate=0.2, recycles=1):\n",
        "    \"\"\"\n",
        "    GenSAN model instantiate.\n",
        "\n",
        "    Args:\n",
        "        blocks (int): Number of transformer encoder units for GenSAN\n",
        "        GECs_dimension (int): The last dimension of GEC size(978).\n",
        "        hidden_nodes (int): Number of nodes in each feed forward neural network layer\n",
        "        heads (int): Number of attention heads of row-wise self-attention block\n",
        "        dropout_rate (float): Dropout layer ratio of GenSAN\n",
        "        recycles (int): Recycle times of GenSAN model\n",
        "    \"\"\"\n",
        "\n",
        "    c = copy.deepcopy\n",
        "\n",
        "    col_attn = ColumnAttention(dropout_rate)\n",
        "    row_attn = RowAttention(heads, GECs_dimension, dropout_rate)\n",
        "\n",
        "    ff = PositionwiseFeedForward(GECs_dimension, hidden_nodes, dropout_rate)\n",
        "\n",
        "    model = GenSAN(\n",
        "        BlocksRecycling(Blocks(BlockLayer(GECs_dimension, c(col_attn), c(row_attn), c(ff), dropout_rate), blocks),\n",
        "                        recycles),\n",
        "        Generator(GECs_dimension, hidden_nodes, dropout_rate))\n",
        "\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c49c64cf-0837-4153-8017-dc636a04542f",
      "metadata": {
        "id": "c49c64cf-0837-4153-8017-dc636a04542f"
      },
      "outputs": [],
      "source": [
        "# from Data_process import load_rawdata, datasets_split\n",
        "# from FunDNN.Model import run_model\n",
        "# from FunDNN.preprocessor import load_data, get_dataloader\n",
        "# from GenSAN.preprocessor import GenSAN_preprocessor, get_pre_GECs\n",
        "# from GenSAN.train_function import run_GenSAN_model\n",
        "# from config_parser import TranscriptionNet_Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e14e189-3152-45cd-9ec5-af8cdc77c88c",
      "metadata": {
        "id": "2e14e189-3152-45cd-9ec5-af8cdc77c88c"
      },
      "source": [
        "Load TranscriptionNet model hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "35f90329-7085-43ba-b500-e1bb66c5560b",
      "metadata": {
        "id": "35f90329-7085-43ba-b500-e1bb66c5560b"
      },
      "outputs": [],
      "source": [
        "config = TranscriptionNet_Hyperparameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bde441ff-5802-4266-8fcc-173df9c27a6c",
      "metadata": {
        "id": "bde441ff-5802-4266-8fcc-173df9c27a6c"
      },
      "source": [
        "Load raw data and get training, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8SMw1J8H2FhP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SMw1J8H2FhP",
        "outputId": "417db192-efcb-4fdd-db78-0aec40460248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1f444069-d036-40c4-968a-931f8827a2ec",
      "metadata": {
        "id": "1f444069-d036-40c4-968a-931f8827a2ec"
      },
      "outputs": [],
      "source": [
        "node_feature, _, _, CRISPR_GECs = load_rawdata(\"drive/MyDrive/raw_data/\")\n",
        "\n",
        "# RNAi_MMScaler = datasets_split(RNAi_GECs, node_feature, \"example_data/datasets/RNAi/\")\n",
        "# OE_MMScaler = datasets_split(OE_GECs, node_feature, \"example_data/datasets/OE/\")\n",
        "CRISPR_MMScaler = datasets_split(CRISPR_GECs, node_feature, \"drive/MyDrive/CRISPR/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "132966f2-f2c8-4012-a6de-22a9f1e015a7",
      "metadata": {
        "id": "132966f2-f2c8-4012-a6de-22a9f1e015a7"
      },
      "source": [
        "CRISPR FunDNN model to predict CRISPR_pre-GECs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MC4nhzzLu28J",
      "metadata": {
        "id": "MC4nhzzLu28J"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "5df2ac6c-5913-4b4b-a8d5-1b99f806cf51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5df2ac6c-5913-4b4b-a8d5-1b99f806cf51",
        "outputId": "3f366ae8-2e6c-4bca-91c1-adc47402582f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node feature dimension:\n",
            "train data:torch.Size([264, 512])\n",
            "valid data:torch.Size([38, 512])\n",
            "test data:torch.Size([76, 512])\n",
            "\n",
            "GECS data dimension:\n",
            "train data:torch.Size([264, 978])\n",
            "valid data:torch.Size([38, 978])\n",
            "test data:(76, 978)\n",
            "\n",
            "end of epoch:  0 | time: 0.05s | train loss:0.22452 | valid loss:0.31303 | train MseLoss:0.13947 | train PccLoss:0.99000 | valid MseLoss:0.23694 | valid PccLoss:0.99785\n",
            "end of epoch:  1 | time: 0.05s | train loss:0.22597 | valid loss:0.30239 | train MseLoss:0.14093 | train PccLoss:0.99135 | valid MseLoss:0.22586 | valid PccLoss:0.99119\n",
            "end of epoch:  2 | time: 0.05s | train loss:0.22534 | valid loss:0.32083 | train MseLoss:0.14058 | train PccLoss:0.98822 | valid MseLoss:0.24626 | valid PccLoss:0.99199\n",
            "end of epoch:  3 | time: 0.05s | train loss:0.22563 | valid loss:0.31455 | train MseLoss:0.14083 | train PccLoss:0.98881 | valid MseLoss:0.23926 | valid PccLoss:0.99220\n",
            "end of epoch:  4 | time: 0.05s | train loss:0.22614 | valid loss:0.29943 | train MseLoss:0.14171 | train PccLoss:0.98602 | valid MseLoss:0.22280 | valid PccLoss:0.98904\n",
            "end of epoch:  5 | time: 0.05s | train loss:0.22491 | valid loss:0.32090 | train MseLoss:0.14028 | train PccLoss:0.98659 | valid MseLoss:0.24652 | valid PccLoss:0.99027\n",
            "end of epoch:  6 | time: 0.05s | train loss:0.22395 | valid loss:0.34074 | train MseLoss:0.13927 | train PccLoss:0.98608 | valid MseLoss:0.26754 | valid PccLoss:0.99948\n",
            "end of epoch:  7 | time: 0.05s | train loss:0.22467 | valid loss:0.32406 | train MseLoss:0.14033 | train PccLoss:0.98373 | valid MseLoss:0.24933 | valid PccLoss:0.99666\n",
            "end of epoch:  8 | time: 0.05s | train loss:0.22418 | valid loss:0.30681 | train MseLoss:0.13991 | train PccLoss:0.98268 | valid MseLoss:0.23084 | valid PccLoss:0.99054\n",
            "end of epoch:  9 | time: 0.05s | train loss:0.22559 | valid loss:0.32793 | train MseLoss:0.14115 | train PccLoss:0.98553 | valid MseLoss:0.25433 | valid PccLoss:0.99032\n",
            "end of epoch: 10 | time: 0.05s | train loss:0.22395 | valid loss:0.34655 | train MseLoss:0.13960 | train PccLoss:0.98305 | valid MseLoss:0.27485 | valid PccLoss:0.99179\n",
            "end of epoch: 11 | time: 0.05s | train loss:0.22560 | valid loss:0.32060 | train MseLoss:0.14159 | train PccLoss:0.98166 | valid MseLoss:0.24613 | valid PccLoss:0.99091\n",
            "end of epoch: 12 | time: 0.05s | train loss:0.22633 | valid loss:0.31956 | train MseLoss:0.14224 | train PccLoss:0.98308 | valid MseLoss:0.24506 | valid PccLoss:0.99004\n",
            "end of epoch: 13 | time: 0.11s | train loss:0.22454 | valid loss:0.31981 | train MseLoss:0.14018 | train PccLoss:0.98376 | valid MseLoss:0.24527 | valid PccLoss:0.99075\n",
            "end of epoch: 14 | time: 0.13s | train loss:0.22528 | valid loss:0.30304 | train MseLoss:0.14119 | train PccLoss:0.98207 | valid MseLoss:0.22647 | valid PccLoss:0.99218\n",
            "end of epoch: 15 | time: 0.10s | train loss:0.22707 | valid loss:0.32088 | train MseLoss:0.14335 | train PccLoss:0.98053 | valid MseLoss:0.24602 | valid PccLoss:0.99461\n",
            "end of epoch: 16 | time: 0.08s | train loss:0.22474 | valid loss:0.30419 | train MseLoss:0.14074 | train PccLoss:0.98080 | valid MseLoss:0.22801 | valid PccLoss:0.98972\n",
            "end of epoch: 17 | time: 0.11s | train loss:0.22210 | valid loss:0.31291 | train MseLoss:0.13811 | train PccLoss:0.97807 | valid MseLoss:0.23702 | valid PccLoss:0.99586\n",
            "end of epoch: 18 | time: 0.12s | train loss:0.22720 | valid loss:0.31387 | train MseLoss:0.14334 | train PccLoss:0.98198 | valid MseLoss:0.23848 | valid PccLoss:0.99230\n",
            "end of epoch: 19 | time: 0.16s | train loss:0.22661 | valid loss:0.31942 | train MseLoss:0.14283 | train PccLoss:0.98064 | valid MseLoss:0.24379 | valid PccLoss:1.00003\n",
            "end of epoch: 20 | time: 0.13s | train loss:0.22106 | valid loss:0.31983 | train MseLoss:0.13708 | train PccLoss:0.97690 | valid MseLoss:0.24502 | valid PccLoss:0.99311\n",
            "end of epoch: 21 | time: 0.09s | train loss:0.22307 | valid loss:0.33306 | train MseLoss:0.13925 | train PccLoss:0.97744 | valid MseLoss:0.26038 | valid PccLoss:0.98719\n",
            "end of epoch: 22 | time: 0.09s | train loss:0.22271 | valid loss:0.31413 | train MseLoss:0.13873 | train PccLoss:0.97847 | valid MseLoss:0.23996 | valid PccLoss:0.98163\n",
            "end of epoch: 23 | time: 0.07s | train loss:0.22337 | valid loss:0.33597 | train MseLoss:0.13963 | train PccLoss:0.97696 | valid MseLoss:0.26306 | valid PccLoss:0.99218\n",
            "end of epoch: 24 | time: 0.15s | train loss:0.22331 | valid loss:0.32140 | train MseLoss:0.13955 | train PccLoss:0.97713 | valid MseLoss:0.24653 | valid PccLoss:0.99521\n",
            "end of epoch: 25 | time: 0.10s | train loss:0.22329 | valid loss:0.32424 | train MseLoss:0.13970 | train PccLoss:0.97551 | valid MseLoss:0.25050 | valid PccLoss:0.98783\n",
            "end of epoch: 26 | time: 0.08s | train loss:0.22557 | valid loss:0.31734 | train MseLoss:0.14197 | train PccLoss:0.97801 | valid MseLoss:0.24321 | valid PccLoss:0.98452\n",
            "end of epoch: 27 | time: 0.06s | train loss:0.22310 | valid loss:0.34249 | train MseLoss:0.13946 | train PccLoss:0.97579 | valid MseLoss:0.26995 | valid PccLoss:0.99536\n",
            "end of epoch: 28 | time: 0.06s | train loss:0.22397 | valid loss:0.34307 | train MseLoss:0.14059 | train PccLoss:0.97437 | valid MseLoss:0.27071 | valid PccLoss:0.99436\n",
            "end of epoch: 29 | time: 0.10s | train loss:0.22494 | valid loss:0.34020 | train MseLoss:0.14173 | train PccLoss:0.97389 | valid MseLoss:0.26777 | valid PccLoss:0.99205\n",
            "end of epoch: 30 | time: 0.08s | train loss:0.22125 | valid loss:0.33625 | train MseLoss:0.13768 | train PccLoss:0.97342 | valid MseLoss:0.26370 | valid PccLoss:0.98921\n",
            "end of epoch: 31 | time: 0.14s | train loss:0.22490 | valid loss:0.33187 | train MseLoss:0.14192 | train PccLoss:0.97169 | valid MseLoss:0.25859 | valid PccLoss:0.99131\n",
            "end of epoch: 32 | time: 0.10s | train loss:0.22493 | valid loss:0.31653 | train MseLoss:0.14171 | train PccLoss:0.97389 | valid MseLoss:0.24149 | valid PccLoss:0.99196\n",
            "end of epoch: 33 | time: 0.07s | train loss:0.22549 | valid loss:0.32923 | train MseLoss:0.14257 | train PccLoss:0.97183 | valid MseLoss:0.25616 | valid PccLoss:0.98692\n",
            "end of epoch: 34 | time: 0.11s | train loss:0.22652 | valid loss:0.32035 | train MseLoss:0.14374 | train PccLoss:0.97159 | valid MseLoss:0.24601 | valid PccLoss:0.98943\n",
            "end of epoch: 35 | time: 0.07s | train loss:0.22190 | valid loss:0.33374 | train MseLoss:0.13877 | train PccLoss:0.97000 | valid MseLoss:0.26076 | valid PccLoss:0.99062\n",
            "end of epoch: 36 | time: 0.10s | train loss:0.22522 | valid loss:0.34863 | train MseLoss:0.14245 | train PccLoss:0.97014 | valid MseLoss:0.27776 | valid PccLoss:0.98651\n",
            "end of epoch: 37 | time: 0.07s | train loss:0.22719 | valid loss:0.31638 | train MseLoss:0.14452 | train PccLoss:0.97121 | valid MseLoss:0.24043 | valid PccLoss:0.99987\n",
            "end of epoch: 38 | time: 0.07s | train loss:0.22367 | valid loss:0.32464 | train MseLoss:0.14091 | train PccLoss:0.96852 | valid MseLoss:0.25005 | valid PccLoss:0.99601\n",
            "end of epoch: 39 | time: 0.07s | train loss:0.22069 | valid loss:0.31740 | train MseLoss:0.13758 | train PccLoss:0.96872 | valid MseLoss:0.24240 | valid PccLoss:0.99231\n",
            "end of epoch: 40 | time: 0.07s | train loss:0.22588 | valid loss:0.32067 | train MseLoss:0.14318 | train PccLoss:0.97021 | valid MseLoss:0.24755 | valid PccLoss:0.97875\n",
            "end of epoch: 41 | time: 0.08s | train loss:0.22256 | valid loss:0.30910 | train MseLoss:0.13971 | train PccLoss:0.96825 | valid MseLoss:0.23259 | valid PccLoss:0.99767\n",
            "end of epoch: 42 | time: 0.09s | train loss:0.22419 | valid loss:0.32546 | train MseLoss:0.14189 | train PccLoss:0.96493 | valid MseLoss:0.25166 | valid PccLoss:0.98961\n",
            "end of epoch: 43 | time: 0.07s | train loss:0.22228 | valid loss:0.31929 | train MseLoss:0.13943 | train PccLoss:0.96796 | valid MseLoss:0.24467 | valid PccLoss:0.99083\n",
            "end of epoch: 44 | time: 0.08s | train loss:0.22169 | valid loss:0.33182 | train MseLoss:0.13885 | train PccLoss:0.96716 | valid MseLoss:0.25834 | valid PccLoss:0.99315\n",
            "end of epoch: 45 | time: 0.09s | train loss:0.22430 | valid loss:0.32132 | train MseLoss:0.14191 | train PccLoss:0.96578 | valid MseLoss:0.24724 | valid PccLoss:0.98799\n",
            "end of epoch: 46 | time: 0.07s | train loss:0.22030 | valid loss:0.31325 | train MseLoss:0.13748 | train PccLoss:0.96571 | valid MseLoss:0.23800 | valid PccLoss:0.99054\n",
            "end of epoch: 47 | time: 0.10s | train loss:0.22388 | valid loss:0.32489 | train MseLoss:0.14166 | train PccLoss:0.96392 | valid MseLoss:0.25014 | valid PccLoss:0.99759\n",
            "end of epoch: 48 | time: 0.09s | train loss:0.22102 | valid loss:0.30807 | train MseLoss:0.13840 | train PccLoss:0.96465 | valid MseLoss:0.23300 | valid PccLoss:0.98365\n",
            "end of epoch: 49 | time: 0.07s | train loss:0.22101 | valid loss:0.32827 | train MseLoss:0.13857 | train PccLoss:0.96297 | valid MseLoss:0.25534 | valid PccLoss:0.98466\n",
            "end of epoch: 50 | time: 0.09s | train loss:0.22309 | valid loss:0.30653 | train MseLoss:0.14074 | train PccLoss:0.96426 | valid MseLoss:0.23126 | valid PccLoss:0.98397\n",
            "end of epoch: 51 | time: 0.06s | train loss:0.22127 | valid loss:0.33198 | train MseLoss:0.13915 | train PccLoss:0.96039 | valid MseLoss:0.25904 | valid PccLoss:0.98845\n",
            "end of epoch: 52 | time: 0.06s | train loss:0.22103 | valid loss:0.33596 | train MseLoss:0.13863 | train PccLoss:0.96265 | valid MseLoss:0.26291 | valid PccLoss:0.99340\n",
            "end of epoch: 53 | time: 0.06s | train loss:0.22195 | valid loss:0.30078 | train MseLoss:0.13959 | train PccLoss:0.96316 | valid MseLoss:0.22472 | valid PccLoss:0.98536\n",
            "end of epoch: 54 | time: 0.08s | train loss:0.22104 | valid loss:0.30778 | train MseLoss:0.13888 | train PccLoss:0.96042 | valid MseLoss:0.23248 | valid PccLoss:0.98541\n",
            "end of epoch: 55 | time: 0.07s | train loss:0.22100 | valid loss:0.33096 | train MseLoss:0.13905 | train PccLoss:0.95855 | valid MseLoss:0.25760 | valid PccLoss:0.99125\n",
            "end of epoch: 56 | time: 0.14s | train loss:0.21985 | valid loss:0.30936 | train MseLoss:0.13765 | train PccLoss:0.95964 | valid MseLoss:0.23440 | valid PccLoss:0.98396\n",
            "end of epoch: 57 | time: 0.09s | train loss:0.21929 | valid loss:0.32659 | train MseLoss:0.13712 | train PccLoss:0.95884 | valid MseLoss:0.25327 | valid PccLoss:0.98647\n",
            "end of epoch: 58 | time: 0.17s | train loss:0.22267 | valid loss:0.31532 | train MseLoss:0.14081 | train PccLoss:0.95945 | valid MseLoss:0.24062 | valid PccLoss:0.98761\n",
            "end of epoch: 59 | time: 0.16s | train loss:0.22480 | valid loss:0.31693 | train MseLoss:0.14322 | train PccLoss:0.95905 | valid MseLoss:0.24179 | valid PccLoss:0.99318\n",
            "end of epoch: 60 | time: 0.12s | train loss:0.22080 | valid loss:0.31156 | train MseLoss:0.13882 | train PccLoss:0.95862 | valid MseLoss:0.23732 | valid PccLoss:0.97975\n",
            "end of epoch: 61 | time: 0.10s | train loss:0.22257 | valid loss:0.32890 | train MseLoss:0.14113 | train PccLoss:0.95561 | valid MseLoss:0.25552 | valid PccLoss:0.98926\n",
            "end of epoch: 62 | time: 0.05s | train loss:0.22071 | valid loss:0.30972 | train MseLoss:0.13895 | train PccLoss:0.95652 | valid MseLoss:0.23445 | valid PccLoss:0.98710\n",
            "end of epoch: 63 | time: 0.05s | train loss:0.21903 | valid loss:0.30796 | train MseLoss:0.13708 | train PccLoss:0.95656 | valid MseLoss:0.23320 | valid PccLoss:0.98076\n",
            "end of epoch: 64 | time: 0.05s | train loss:0.22334 | valid loss:0.34844 | train MseLoss:0.14190 | train PccLoss:0.95631 | valid MseLoss:0.27682 | valid PccLoss:0.99300\n",
            "end of epoch: 65 | time: 0.05s | train loss:0.22182 | valid loss:0.31101 | train MseLoss:0.14054 | train PccLoss:0.95329 | valid MseLoss:0.23673 | valid PccLoss:0.97954\n",
            "end of epoch: 66 | time: 0.05s | train loss:0.22148 | valid loss:0.32913 | train MseLoss:0.13996 | train PccLoss:0.95516 | valid MseLoss:0.25508 | valid PccLoss:0.99559\n",
            "end of epoch: 67 | time: 0.05s | train loss:0.21938 | valid loss:0.32181 | train MseLoss:0.13767 | train PccLoss:0.95475 | valid MseLoss:0.24794 | valid PccLoss:0.98663\n",
            "end of epoch: 68 | time: 0.05s | train loss:0.22004 | valid loss:0.31954 | train MseLoss:0.13865 | train PccLoss:0.95249 | valid MseLoss:0.24551 | valid PccLoss:0.98575\n",
            "end of epoch: 69 | time: 0.05s | train loss:0.21884 | valid loss:0.33105 | train MseLoss:0.13749 | train PccLoss:0.95092 | valid MseLoss:0.25873 | valid PccLoss:0.98192\n",
            "end of epoch: 70 | time: 0.05s | train loss:0.22224 | valid loss:0.33382 | train MseLoss:0.14118 | train PccLoss:0.95182 | valid MseLoss:0.26090 | valid PccLoss:0.99014\n",
            "end of epoch: 71 | time: 0.05s | train loss:0.22149 | valid loss:0.31768 | train MseLoss:0.14005 | train PccLoss:0.95443 | valid MseLoss:0.24319 | valid PccLoss:0.98810\n",
            "end of epoch: 72 | time: 0.05s | train loss:0.22261 | valid loss:0.30032 | train MseLoss:0.14152 | train PccLoss:0.95241 | valid MseLoss:0.22469 | valid PccLoss:0.98099\n",
            "end of epoch: 73 | time: 0.05s | train loss:0.21864 | valid loss:0.31025 | train MseLoss:0.13746 | train PccLoss:0.94927 | valid MseLoss:0.23587 | valid PccLoss:0.97972\n",
            "end of epoch: 74 | time: 0.05s | train loss:0.21921 | valid loss:0.32105 | train MseLoss:0.13792 | train PccLoss:0.95076 | valid MseLoss:0.24823 | valid PccLoss:0.97652\n",
            "end of epoch: 75 | time: 0.05s | train loss:0.22069 | valid loss:0.30239 | train MseLoss:0.13945 | train PccLoss:0.95185 | valid MseLoss:0.22652 | valid PccLoss:0.98523\n",
            "end of epoch: 76 | time: 0.05s | train loss:0.21981 | valid loss:0.32213 | train MseLoss:0.13887 | train PccLoss:0.94829 | valid MseLoss:0.24862 | valid PccLoss:0.98375\n",
            "end of epoch: 77 | time: 0.05s | train loss:0.22025 | valid loss:0.32226 | train MseLoss:0.13928 | train PccLoss:0.94899 | valid MseLoss:0.24864 | valid PccLoss:0.98487\n",
            "end of epoch: 78 | time: 0.05s | train loss:0.21988 | valid loss:0.34735 | train MseLoss:0.13892 | train PccLoss:0.94856 | valid MseLoss:0.27613 | valid PccLoss:0.98837\n",
            "end of epoch: 79 | time: 0.05s | train loss:0.22148 | valid loss:0.33118 | train MseLoss:0.14072 | train PccLoss:0.94832 | valid MseLoss:0.25866 | valid PccLoss:0.98393\n",
            "end of epoch: 80 | time: 0.05s | train loss:0.22048 | valid loss:0.31552 | train MseLoss:0.13973 | train PccLoss:0.94732 | valid MseLoss:0.24091 | valid PccLoss:0.98702\n",
            "end of epoch: 81 | time: 0.05s | train loss:0.22043 | valid loss:0.33541 | train MseLoss:0.13964 | train PccLoss:0.94754 | valid MseLoss:0.26262 | valid PccLoss:0.99047\n",
            "end of epoch: 82 | time: 0.05s | train loss:0.22062 | valid loss:0.33972 | train MseLoss:0.14006 | train PccLoss:0.94570 | valid MseLoss:0.26751 | valid PccLoss:0.98964\n",
            "end of epoch: 83 | time: 0.05s | train loss:0.22091 | valid loss:0.32046 | train MseLoss:0.14030 | train PccLoss:0.94637 | valid MseLoss:0.24664 | valid PccLoss:0.98485\n",
            "end of epoch: 84 | time: 0.05s | train loss:0.22025 | valid loss:0.31463 | train MseLoss:0.13961 | train PccLoss:0.94600 | valid MseLoss:0.23989 | valid PccLoss:0.98729\n",
            "end of epoch: 85 | time: 0.05s | train loss:0.21918 | valid loss:0.32838 | train MseLoss:0.13880 | train PccLoss:0.94257 | valid MseLoss:0.25634 | valid PccLoss:0.97674\n",
            "end of epoch: 86 | time: 0.06s | train loss:0.22211 | valid loss:0.30885 | train MseLoss:0.14205 | train PccLoss:0.94268 | valid MseLoss:0.23367 | valid PccLoss:0.98553\n",
            "end of epoch: 87 | time: 0.05s | train loss:0.22337 | valid loss:0.30794 | train MseLoss:0.14337 | train PccLoss:0.94340 | valid MseLoss:0.23276 | valid PccLoss:0.98456\n",
            "end of epoch: 88 | time: 0.05s | train loss:0.22001 | valid loss:0.30815 | train MseLoss:0.13963 | train PccLoss:0.94344 | valid MseLoss:0.23392 | valid PccLoss:0.97624\n",
            "end of epoch: 89 | time: 0.05s | train loss:0.22137 | valid loss:0.33543 | train MseLoss:0.14113 | train PccLoss:0.94351 | valid MseLoss:0.26360 | valid PccLoss:0.98196\n",
            "end of epoch: 90 | time: 0.05s | train loss:0.21704 | valid loss:0.33064 | train MseLoss:0.13673 | train PccLoss:0.93981 | valid MseLoss:0.25719 | valid PccLoss:0.99167\n",
            "end of epoch: 91 | time: 0.05s | train loss:0.22052 | valid loss:0.31970 | train MseLoss:0.14048 | train PccLoss:0.94081 | valid MseLoss:0.24653 | valid PccLoss:0.97824\n",
            "end of epoch: 92 | time: 0.05s | train loss:0.21746 | valid loss:0.31640 | train MseLoss:0.13694 | train PccLoss:0.94210 | valid MseLoss:0.24150 | valid PccLoss:0.99051\n",
            "end of epoch: 93 | time: 0.05s | train loss:0.21831 | valid loss:0.31462 | train MseLoss:0.13821 | train PccLoss:0.93917 | valid MseLoss:0.24010 | valid PccLoss:0.98525\n",
            "end of epoch: 94 | time: 0.05s | train loss:0.21812 | valid loss:0.30915 | train MseLoss:0.13787 | train PccLoss:0.94033 | valid MseLoss:0.23486 | valid PccLoss:0.97778\n",
            "end of epoch: 95 | time: 0.05s | train loss:0.21778 | valid loss:0.34242 | train MseLoss:0.13765 | train PccLoss:0.93899 | valid MseLoss:0.27171 | valid PccLoss:0.97879\n",
            "end of epoch: 96 | time: 0.05s | train loss:0.21842 | valid loss:0.32942 | train MseLoss:0.13837 | train PccLoss:0.93879 | valid MseLoss:0.25718 | valid PccLoss:0.97964\n",
            "end of epoch: 97 | time: 0.05s | train loss:0.22066 | valid loss:0.30672 | train MseLoss:0.14075 | train PccLoss:0.93983 | valid MseLoss:0.23098 | valid PccLoss:0.98837\n",
            "end of epoch: 98 | time: 0.05s | train loss:0.21934 | valid loss:0.31971 | train MseLoss:0.13944 | train PccLoss:0.93845 | valid MseLoss:0.24584 | valid PccLoss:0.98459\n",
            "end of epoch: 99 | time: 0.05s | train loss:0.21830 | valid loss:0.30952 | train MseLoss:0.13825 | train PccLoss:0.93880 | valid MseLoss:0.23473 | valid PccLoss:0.98270\n",
            "end of epoch:100 | time: 0.05s | train loss:0.21673 | valid loss:0.32367 | train MseLoss:0.13672 | train PccLoss:0.93690 | valid MseLoss:0.25111 | valid PccLoss:0.97671\n",
            "end of epoch:101 | time: 0.05s | train loss:0.21764 | valid loss:0.31166 | train MseLoss:0.13776 | train PccLoss:0.93658 | valid MseLoss:0.23716 | valid PccLoss:0.98209\n",
            "end of epoch:102 | time: 0.05s | train loss:0.21700 | valid loss:0.34707 | train MseLoss:0.13688 | train PccLoss:0.93807 | valid MseLoss:0.27683 | valid PccLoss:0.97920\n",
            "end of epoch:103 | time: 0.05s | train loss:0.21754 | valid loss:0.32654 | train MseLoss:0.13786 | train PccLoss:0.93468 | valid MseLoss:0.25328 | valid PccLoss:0.98582\n",
            "end of epoch:104 | time: 0.05s | train loss:0.21794 | valid loss:0.31955 | train MseLoss:0.13808 | train PccLoss:0.93665 | valid MseLoss:0.24576 | valid PccLoss:0.98361\n",
            "end of epoch:105 | time: 0.05s | train loss:0.21686 | valid loss:0.32404 | train MseLoss:0.13726 | train PccLoss:0.93330 | valid MseLoss:0.24962 | valid PccLoss:0.99375\n",
            "end of epoch:106 | time: 0.05s | train loss:0.21848 | valid loss:0.31784 | train MseLoss:0.13890 | train PccLoss:0.93471 | valid MseLoss:0.24386 | valid PccLoss:0.98373\n",
            "end of epoch:107 | time: 0.05s | train loss:0.21899 | valid loss:0.31545 | train MseLoss:0.13969 | train PccLoss:0.93271 | valid MseLoss:0.24104 | valid PccLoss:0.98510\n",
            "end of epoch:108 | time: 0.05s | train loss:0.21771 | valid loss:0.34153 | train MseLoss:0.13815 | train PccLoss:0.93376 | valid MseLoss:0.26994 | valid PccLoss:0.98575\n",
            "end of epoch:109 | time: 0.05s | train loss:0.21708 | valid loss:0.31108 | train MseLoss:0.13767 | train PccLoss:0.93172 | valid MseLoss:0.23742 | valid PccLoss:0.97396\n",
            "end of epoch:110 | time: 0.05s | train loss:0.21830 | valid loss:0.32292 | train MseLoss:0.13889 | train PccLoss:0.93304 | valid MseLoss:0.24905 | valid PccLoss:0.98776\n",
            "end of epoch:111 | time: 0.05s | train loss:0.21867 | valid loss:0.32070 | train MseLoss:0.13957 | train PccLoss:0.93058 | valid MseLoss:0.24690 | valid PccLoss:0.98489\n",
            "end of epoch:112 | time: 0.05s | train loss:0.21661 | valid loss:0.31285 | train MseLoss:0.13733 | train PccLoss:0.93019 | valid MseLoss:0.23739 | valid PccLoss:0.99201\n",
            "end of epoch:113 | time: 0.05s | train loss:0.22045 | valid loss:0.32626 | train MseLoss:0.14147 | train PccLoss:0.93125 | valid MseLoss:0.25389 | valid PccLoss:0.97754\n",
            "end of epoch:114 | time: 0.05s | train loss:0.21679 | valid loss:0.33055 | train MseLoss:0.13755 | train PccLoss:0.92989 | valid MseLoss:0.25820 | valid PccLoss:0.98177\n",
            "end of epoch:115 | time: 0.05s | train loss:0.21690 | valid loss:0.31879 | train MseLoss:0.13785 | train PccLoss:0.92833 | valid MseLoss:0.24566 | valid PccLoss:0.97689\n",
            "end of epoch:116 | time: 0.05s | train loss:0.21883 | valid loss:0.31310 | train MseLoss:0.13983 | train PccLoss:0.92988 | valid MseLoss:0.23897 | valid PccLoss:0.98025\n",
            "end of epoch:117 | time: 0.05s | train loss:0.21763 | valid loss:0.29844 | train MseLoss:0.13878 | train PccLoss:0.92726 | valid MseLoss:0.22213 | valid PccLoss:0.98527\n",
            "end of epoch:118 | time: 0.05s | train loss:0.21888 | valid loss:0.29292 | train MseLoss:0.14016 | train PccLoss:0.92731 | valid MseLoss:0.21696 | valid PccLoss:0.97656\n",
            "end of epoch:119 | time: 0.05s | train loss:0.21571 | valid loss:0.34384 | train MseLoss:0.13660 | train PccLoss:0.92774 | valid MseLoss:0.27308 | valid PccLoss:0.98066\n",
            "end of epoch:120 | time: 0.05s | train loss:0.21602 | valid loss:0.29674 | train MseLoss:0.13708 | train PccLoss:0.92647 | valid MseLoss:0.22073 | valid PccLoss:0.98078\n",
            "end of epoch:121 | time: 0.06s | train loss:0.21808 | valid loss:0.32404 | train MseLoss:0.13941 | train PccLoss:0.92618 | valid MseLoss:0.25118 | valid PccLoss:0.97973\n",
            "end of epoch:122 | time: 0.06s | train loss:0.21595 | valid loss:0.32342 | train MseLoss:0.13715 | train PccLoss:0.92515 | valid MseLoss:0.25030 | valid PccLoss:0.98148\n",
            "end of epoch:123 | time: 0.06s | train loss:0.21897 | valid loss:0.33280 | train MseLoss:0.14059 | train PccLoss:0.92438 | valid MseLoss:0.26122 | valid PccLoss:0.97697\n",
            "end of epoch:124 | time: 0.06s | train loss:0.21642 | valid loss:0.31840 | train MseLoss:0.13777 | train PccLoss:0.92425 | valid MseLoss:0.24487 | valid PccLoss:0.98019\n",
            "end of epoch:125 | time: 0.06s | train loss:0.21741 | valid loss:0.31207 | train MseLoss:0.13862 | train PccLoss:0.92658 | valid MseLoss:0.23812 | valid PccLoss:0.97762\n",
            "end of epoch:126 | time: 0.06s | train loss:0.21737 | valid loss:0.34050 | train MseLoss:0.13861 | train PccLoss:0.92626 | valid MseLoss:0.26863 | valid PccLoss:0.98735\n",
            "end of epoch:127 | time: 0.06s | train loss:0.21788 | valid loss:0.30340 | train MseLoss:0.13936 | train PccLoss:0.92452 | valid MseLoss:0.22793 | valid PccLoss:0.98263\n",
            "end of epoch:128 | time: 0.06s | train loss:0.21802 | valid loss:0.30194 | train MseLoss:0.13957 | train PccLoss:0.92414 | valid MseLoss:0.22679 | valid PccLoss:0.97829\n",
            "end of epoch:129 | time: 0.06s | train loss:0.21978 | valid loss:0.30957 | train MseLoss:0.14143 | train PccLoss:0.92493 | valid MseLoss:0.23552 | valid PccLoss:0.97603\n",
            "end of epoch:130 | time: 0.06s | train loss:0.21779 | valid loss:0.30811 | train MseLoss:0.13950 | train PccLoss:0.92238 | valid MseLoss:0.23334 | valid PccLoss:0.98102\n",
            "end of epoch:131 | time: 0.06s | train loss:0.21973 | valid loss:0.34818 | train MseLoss:0.14161 | train PccLoss:0.92285 | valid MseLoss:0.27690 | valid PccLoss:0.98969\n",
            "end of epoch:132 | time: 0.06s | train loss:0.21630 | valid loss:0.33247 | train MseLoss:0.13790 | train PccLoss:0.92193 | valid MseLoss:0.26031 | valid PccLoss:0.98199\n",
            "end of epoch:133 | time: 0.06s | train loss:0.21672 | valid loss:0.29503 | train MseLoss:0.13825 | train PccLoss:0.92294 | valid MseLoss:0.21863 | valid PccLoss:0.98267\n",
            "end of epoch:134 | time: 0.06s | train loss:0.21760 | valid loss:0.30717 | train MseLoss:0.13940 | train PccLoss:0.92138 | valid MseLoss:0.23176 | valid PccLoss:0.98586\n",
            "end of epoch:135 | time: 0.06s | train loss:0.21639 | valid loss:0.30830 | train MseLoss:0.13818 | train PccLoss:0.92033 | valid MseLoss:0.23288 | valid PccLoss:0.98711\n",
            "end of epoch:136 | time: 0.06s | train loss:0.21544 | valid loss:0.31093 | train MseLoss:0.13736 | train PccLoss:0.91813 | valid MseLoss:0.23617 | valid PccLoss:0.98371\n",
            "end of epoch:137 | time: 0.06s | train loss:0.21551 | valid loss:0.31637 | train MseLoss:0.13744 | train PccLoss:0.91811 | valid MseLoss:0.24153 | valid PccLoss:0.98993\n",
            "end of epoch:138 | time: 0.05s | train loss:0.21702 | valid loss:0.32305 | train MseLoss:0.13901 | train PccLoss:0.91910 | valid MseLoss:0.24955 | valid PccLoss:0.98455\n",
            "end of epoch:139 | time: 0.05s | train loss:0.22001 | valid loss:0.33131 | train MseLoss:0.14219 | train PccLoss:0.92034 | valid MseLoss:0.25970 | valid PccLoss:0.97581\n",
            "end of epoch:140 | time: 0.05s | train loss:0.21615 | valid loss:0.32728 | train MseLoss:0.13786 | train PccLoss:0.92078 | valid MseLoss:0.25369 | valid PccLoss:0.98959\n",
            "end of epoch:141 | time: 0.05s | train loss:0.21787 | valid loss:0.31588 | train MseLoss:0.14013 | train PccLoss:0.91755 | valid MseLoss:0.24162 | valid PccLoss:0.98428\n",
            "end of epoch:142 | time: 0.05s | train loss:0.21905 | valid loss:0.32775 | train MseLoss:0.14151 | train PccLoss:0.91693 | valid MseLoss:0.25525 | valid PccLoss:0.98024\n",
            "end of epoch:143 | time: 0.06s | train loss:0.21440 | valid loss:0.31940 | train MseLoss:0.13641 | train PccLoss:0.91630 | valid MseLoss:0.24537 | valid PccLoss:0.98570\n",
            "end of epoch:144 | time: 0.06s | train loss:0.21732 | valid loss:0.31927 | train MseLoss:0.13966 | train PccLoss:0.91621 | valid MseLoss:0.24728 | valid PccLoss:0.96718\n",
            "end of epoch:145 | time: 0.06s | train loss:0.21714 | valid loss:0.33574 | train MseLoss:0.13976 | train PccLoss:0.91362 | valid MseLoss:0.26348 | valid PccLoss:0.98607\n",
            "end of epoch:146 | time: 0.06s | train loss:0.21840 | valid loss:0.31126 | train MseLoss:0.14097 | train PccLoss:0.91525 | valid MseLoss:0.23793 | valid PccLoss:0.97127\n",
            "end of epoch:147 | time: 0.06s | train loss:0.21504 | valid loss:0.30684 | train MseLoss:0.13742 | train PccLoss:0.91364 | valid MseLoss:0.23262 | valid PccLoss:0.97482\n",
            "end of epoch:148 | time: 0.06s | train loss:0.21713 | valid loss:0.31132 | train MseLoss:0.13969 | train PccLoss:0.91412 | valid MseLoss:0.23703 | valid PccLoss:0.97998\n",
            "end of epoch:149 | time: 0.06s | train loss:0.21480 | valid loss:0.31413 | train MseLoss:0.13719 | train PccLoss:0.91328 | valid MseLoss:0.24051 | valid PccLoss:0.97667\n",
            "end of epoch:150 | time: 0.06s | train loss:0.21980 | valid loss:0.34369 | train MseLoss:0.14273 | train PccLoss:0.91339 | valid MseLoss:0.27281 | valid PccLoss:0.98170\n",
            "end of epoch:151 | time: 0.05s | train loss:0.21542 | valid loss:0.32050 | train MseLoss:0.13798 | train PccLoss:0.91240 | valid MseLoss:0.24772 | valid PccLoss:0.97551\n",
            "end of epoch:152 | time: 0.05s | train loss:0.21630 | valid loss:0.33115 | train MseLoss:0.13920 | train PccLoss:0.91024 | valid MseLoss:0.25778 | valid PccLoss:0.99153\n",
            "end of epoch:153 | time: 0.06s | train loss:0.21544 | valid loss:0.30867 | train MseLoss:0.13822 | train PccLoss:0.91036 | valid MseLoss:0.23424 | valid PccLoss:0.97858\n",
            "end of epoch:154 | time: 0.05s | train loss:0.21611 | valid loss:0.31432 | train MseLoss:0.13923 | train PccLoss:0.90807 | valid MseLoss:0.24038 | valid PccLoss:0.97974\n",
            "end of epoch:155 | time: 0.05s | train loss:0.21605 | valid loss:0.33717 | train MseLoss:0.13887 | train PccLoss:0.91065 | valid MseLoss:0.26524 | valid PccLoss:0.98453\n",
            "end of epoch:156 | time: 0.06s | train loss:0.21440 | valid loss:0.32612 | train MseLoss:0.13702 | train PccLoss:0.91081 | valid MseLoss:0.25391 | valid PccLoss:0.97604\n",
            "end of epoch:157 | time: 0.07s | train loss:0.21445 | valid loss:0.30508 | train MseLoss:0.13734 | train PccLoss:0.90846 | valid MseLoss:0.22919 | valid PccLoss:0.98813\n",
            "end of epoch:158 | time: 0.06s | train loss:0.21502 | valid loss:0.31481 | train MseLoss:0.13800 | train PccLoss:0.90823 | valid MseLoss:0.24197 | valid PccLoss:0.97036\n",
            "end of epoch:159 | time: 0.06s | train loss:0.21490 | valid loss:0.30540 | train MseLoss:0.13792 | train PccLoss:0.90769 | valid MseLoss:0.23039 | valid PccLoss:0.98051\n",
            "end of epoch:160 | time: 0.06s | train loss:0.21352 | valid loss:0.33975 | train MseLoss:0.13633 | train PccLoss:0.90827 | valid MseLoss:0.26809 | valid PccLoss:0.98466\n",
            "end of epoch:161 | time: 0.07s | train loss:0.21569 | valid loss:0.33076 | train MseLoss:0.13896 | train PccLoss:0.90629 | valid MseLoss:0.25831 | valid PccLoss:0.98278\n",
            "end of epoch:162 | time: 0.06s | train loss:0.21516 | valid loss:0.32981 | train MseLoss:0.13824 | train PccLoss:0.90746 | valid MseLoss:0.25755 | valid PccLoss:0.98012\n",
            "end of epoch:163 | time: 0.06s | train loss:0.21425 | valid loss:0.31582 | train MseLoss:0.13751 | train PccLoss:0.90488 | valid MseLoss:0.24243 | valid PccLoss:0.97633\n",
            "end of epoch:164 | time: 0.06s | train loss:0.21622 | valid loss:0.31851 | train MseLoss:0.13952 | train PccLoss:0.90652 | valid MseLoss:0.24569 | valid PccLoss:0.97382\n",
            "end of epoch:165 | time: 0.06s | train loss:0.21661 | valid loss:0.35979 | train MseLoss:0.14005 | train PccLoss:0.90570 | valid MseLoss:0.29022 | valid PccLoss:0.98598\n",
            "end of epoch:166 | time: 0.07s | train loss:0.21451 | valid loss:0.30530 | train MseLoss:0.13796 | train PccLoss:0.90345 | valid MseLoss:0.23082 | valid PccLoss:0.97562\n",
            "end of epoch:167 | time: 0.06s | train loss:0.21397 | valid loss:0.30385 | train MseLoss:0.13717 | train PccLoss:0.90515 | valid MseLoss:0.22994 | valid PccLoss:0.96903\n",
            "end of epoch:168 | time: 0.06s | train loss:0.21501 | valid loss:0.31215 | train MseLoss:0.13850 | train PccLoss:0.90356 | valid MseLoss:0.23762 | valid PccLoss:0.98290\n",
            "end of epoch:169 | time: 0.07s | train loss:0.21363 | valid loss:0.32206 | train MseLoss:0.13711 | train PccLoss:0.90234 | valid MseLoss:0.25039 | valid PccLoss:0.96712\n",
            "end of epoch:170 | time: 0.06s | train loss:0.21523 | valid loss:0.30158 | train MseLoss:0.13883 | train PccLoss:0.90282 | valid MseLoss:0.22694 | valid PccLoss:0.97338\n",
            "end of epoch:171 | time: 0.06s | train loss:0.21322 | valid loss:0.33537 | train MseLoss:0.13651 | train PccLoss:0.90356 | valid MseLoss:0.26371 | valid PccLoss:0.98031\n",
            "end of epoch:172 | time: 0.06s | train loss:0.21539 | valid loss:0.31848 | train MseLoss:0.13928 | train PccLoss:0.90046 | valid MseLoss:0.24506 | valid PccLoss:0.97931\n",
            "end of epoch:173 | time: 0.06s | train loss:0.21375 | valid loss:0.32971 | train MseLoss:0.13725 | train PccLoss:0.90228 | valid MseLoss:0.25841 | valid PccLoss:0.97139\n",
            "end of epoch:174 | time: 0.05s | train loss:0.21631 | valid loss:0.33002 | train MseLoss:0.13995 | train PccLoss:0.90357 | valid MseLoss:0.25720 | valid PccLoss:0.98532\n",
            "end of epoch:175 | time: 0.05s | train loss:0.21344 | valid loss:0.30315 | train MseLoss:0.13723 | train PccLoss:0.89937 | valid MseLoss:0.22959 | valid PccLoss:0.96517\n",
            "end of epoch:176 | time: 0.05s | train loss:0.21369 | valid loss:0.33053 | train MseLoss:0.13741 | train PccLoss:0.90015 | valid MseLoss:0.25954 | valid PccLoss:0.96952\n",
            "end of epoch:177 | time: 0.05s | train loss:0.21379 | valid loss:0.31619 | train MseLoss:0.13761 | train PccLoss:0.89944 | valid MseLoss:0.24281 | valid PccLoss:0.97667\n",
            "end of epoch:178 | time: 0.05s | train loss:0.21258 | valid loss:0.30939 | train MseLoss:0.13633 | train PccLoss:0.89890 | valid MseLoss:0.23471 | valid PccLoss:0.98145\n",
            "end of epoch:179 | time: 0.05s | train loss:0.21603 | valid loss:0.31076 | train MseLoss:0.14048 | train PccLoss:0.89602 | valid MseLoss:0.23624 | valid PccLoss:0.98147\n",
            "end of epoch:180 | time: 0.06s | train loss:0.21293 | valid loss:0.31149 | train MseLoss:0.13688 | train PccLoss:0.89739 | valid MseLoss:0.23831 | valid PccLoss:0.97016\n",
            "end of epoch:181 | time: 0.05s | train loss:0.21338 | valid loss:0.32707 | train MseLoss:0.13743 | train PccLoss:0.89693 | valid MseLoss:0.25502 | valid PccLoss:0.97548\n",
            "end of epoch:182 | time: 0.05s | train loss:0.21305 | valid loss:0.30637 | train MseLoss:0.13712 | train PccLoss:0.89635 | valid MseLoss:0.23138 | valid PccLoss:0.98124\n",
            "end of epoch:183 | time: 0.05s | train loss:0.21669 | valid loss:0.32187 | train MseLoss:0.14125 | train PccLoss:0.89563 | valid MseLoss:0.24998 | valid PccLoss:0.96889\n",
            "end of epoch:184 | time: 0.05s | train loss:0.21217 | valid loss:0.30105 | train MseLoss:0.13627 | train PccLoss:0.89521 | valid MseLoss:0.22635 | valid PccLoss:0.97332\n",
            "end of epoch:185 | time: 0.05s | train loss:0.21349 | valid loss:0.31092 | train MseLoss:0.13795 | train PccLoss:0.89332 | valid MseLoss:0.23711 | valid PccLoss:0.97514\n",
            "end of epoch:186 | time: 0.05s | train loss:0.21283 | valid loss:0.30033 | train MseLoss:0.13702 | train PccLoss:0.89513 | valid MseLoss:0.22460 | valid PccLoss:0.98187\n",
            "end of epoch:187 | time: 0.05s | train loss:0.21661 | valid loss:0.29760 | train MseLoss:0.14120 | train PccLoss:0.89534 | valid MseLoss:0.22240 | valid PccLoss:0.97440\n",
            "end of epoch:188 | time: 0.05s | train loss:0.21273 | valid loss:0.32283 | train MseLoss:0.13687 | train PccLoss:0.89544 | valid MseLoss:0.24957 | valid PccLoss:0.98218\n",
            "end of epoch:189 | time: 0.05s | train loss:0.21415 | valid loss:0.30977 | train MseLoss:0.13859 | train PccLoss:0.89416 | valid MseLoss:0.23621 | valid PccLoss:0.97175\n",
            "end of epoch:190 | time: 0.05s | train loss:0.21342 | valid loss:0.31657 | train MseLoss:0.13825 | train PccLoss:0.88995 | valid MseLoss:0.24296 | valid PccLoss:0.97904\n",
            "end of epoch:191 | time: 0.05s | train loss:0.21174 | valid loss:0.33093 | train MseLoss:0.13617 | train PccLoss:0.89188 | valid MseLoss:0.25957 | valid PccLoss:0.97324\n",
            "end of epoch:192 | time: 0.05s | train loss:0.21231 | valid loss:0.31810 | train MseLoss:0.13686 | train PccLoss:0.89136 | valid MseLoss:0.24439 | valid PccLoss:0.98145\n",
            "end of epoch:193 | time: 0.05s | train loss:0.21429 | valid loss:0.32871 | train MseLoss:0.13890 | train PccLoss:0.89274 | valid MseLoss:0.25470 | valid PccLoss:0.99481\n",
            "end of epoch:194 | time: 0.05s | train loss:0.21344 | valid loss:0.33154 | train MseLoss:0.13828 | train PccLoss:0.88992 | valid MseLoss:0.25940 | valid PccLoss:0.98079\n",
            "end of epoch:195 | time: 0.05s | train loss:0.21341 | valid loss:0.33881 | train MseLoss:0.13827 | train PccLoss:0.88974 | valid MseLoss:0.26697 | valid PccLoss:0.98537\n",
            "end of epoch:196 | time: 0.05s | train loss:0.21448 | valid loss:0.32420 | train MseLoss:0.13931 | train PccLoss:0.89100 | valid MseLoss:0.25065 | valid PccLoss:0.98614\n",
            "end of epoch:197 | time: 0.05s | train loss:0.21201 | valid loss:0.33667 | train MseLoss:0.13690 | train PccLoss:0.88796 | valid MseLoss:0.26481 | valid PccLoss:0.98336\n",
            "end of epoch:198 | time: 0.05s | train loss:0.21317 | valid loss:0.31873 | train MseLoss:0.13814 | train PccLoss:0.88845 | valid MseLoss:0.24485 | valid PccLoss:0.98365\n",
            "end of epoch:199 | time: 0.05s | train loss:0.21484 | valid loss:0.31493 | train MseLoss:0.13959 | train PccLoss:0.89211 | valid MseLoss:0.24302 | valid PccLoss:0.96211\n",
            "end of epoch:200 | time: 0.05s | train loss:0.21402 | valid loss:0.33040 | train MseLoss:0.13921 | train PccLoss:0.88733 | valid MseLoss:0.25784 | valid PccLoss:0.98344\n",
            "end of epoch:201 | time: 0.06s | train loss:0.21388 | valid loss:0.33880 | train MseLoss:0.13873 | train PccLoss:0.89018 | valid MseLoss:0.26719 | valid PccLoss:0.98323\n",
            "end of epoch:202 | time: 0.05s | train loss:0.21115 | valid loss:0.31003 | train MseLoss:0.13614 | train PccLoss:0.88627 | valid MseLoss:0.23560 | valid PccLoss:0.97993\n",
            "end of epoch:203 | time: 0.05s | train loss:0.21427 | valid loss:0.32138 | train MseLoss:0.13942 | train PccLoss:0.88794 | valid MseLoss:0.24908 | valid PccLoss:0.97212\n",
            "end of epoch:204 | time: 0.05s | train loss:0.21223 | valid loss:0.31115 | train MseLoss:0.13766 | train PccLoss:0.88341 | valid MseLoss:0.23698 | valid PccLoss:0.97872\n",
            "end of epoch:205 | time: 0.05s | train loss:0.21287 | valid loss:0.32195 | train MseLoss:0.13817 | train PccLoss:0.88519 | valid MseLoss:0.24813 | valid PccLoss:0.98633\n",
            "end of epoch:206 | time: 0.05s | train loss:0.21258 | valid loss:0.32029 | train MseLoss:0.13804 | train PccLoss:0.88342 | valid MseLoss:0.24627 | valid PccLoss:0.98642\n",
            "end of epoch:207 | time: 0.05s | train loss:0.21512 | valid loss:0.30888 | train MseLoss:0.14066 | train PccLoss:0.88528 | valid MseLoss:0.23451 | valid PccLoss:0.97826\n",
            "end of epoch:208 | time: 0.05s | train loss:0.21895 | valid loss:0.32145 | train MseLoss:0.14442 | train PccLoss:0.88977 | valid MseLoss:0.24818 | valid PccLoss:0.98089\n",
            "end of epoch:209 | time: 0.05s | train loss:0.21504 | valid loss:0.32245 | train MseLoss:0.14054 | train PccLoss:0.88557 | valid MseLoss:0.25011 | valid PccLoss:0.97346\n",
            "end of epoch:210 | time: 0.05s | train loss:0.21373 | valid loss:0.32496 | train MseLoss:0.13925 | train PccLoss:0.88404 | valid MseLoss:0.25261 | valid PccLoss:0.97615\n",
            "end of epoch:211 | time: 0.05s | train loss:0.21371 | valid loss:0.31756 | train MseLoss:0.13949 | train PccLoss:0.88173 | valid MseLoss:0.24543 | valid PccLoss:0.96667\n",
            "end of epoch:212 | time: 0.05s | train loss:0.21180 | valid loss:0.30354 | train MseLoss:0.13726 | train PccLoss:0.88266 | valid MseLoss:0.22910 | valid PccLoss:0.97353\n",
            "end of epoch:213 | time: 0.05s | train loss:0.21378 | valid loss:0.34732 | train MseLoss:0.13939 | train PccLoss:0.88324 | valid MseLoss:0.27672 | valid PccLoss:0.98267\n",
            "end of epoch:214 | time: 0.05s | train loss:0.21186 | valid loss:0.31983 | train MseLoss:0.13778 | train PccLoss:0.87862 | valid MseLoss:0.24624 | valid PccLoss:0.98209\n",
            "end of epoch:215 | time: 0.05s | train loss:0.21203 | valid loss:0.31137 | train MseLoss:0.13763 | train PccLoss:0.88170 | valid MseLoss:0.23799 | valid PccLoss:0.97183\n",
            "end of epoch:216 | time: 0.05s | train loss:0.21319 | valid loss:0.30544 | train MseLoss:0.13911 | train PccLoss:0.87995 | valid MseLoss:0.23192 | valid PccLoss:0.96719\n",
            "end of epoch:217 | time: 0.05s | train loss:0.21333 | valid loss:0.30579 | train MseLoss:0.13920 | train PccLoss:0.88057 | valid MseLoss:0.23240 | valid PccLoss:0.96635\n",
            "end of epoch:218 | time: 0.05s | train loss:0.21103 | valid loss:0.33171 | train MseLoss:0.13689 | train PccLoss:0.87826 | valid MseLoss:0.25963 | valid PccLoss:0.98048\n",
            "end of epoch:219 | time: 0.05s | train loss:0.21222 | valid loss:0.34110 | train MseLoss:0.13820 | train PccLoss:0.87841 | valid MseLoss:0.27043 | valid PccLoss:0.97709\n",
            "end of epoch:220 | time: 0.05s | train loss:0.21022 | valid loss:0.31164 | train MseLoss:0.13598 | train PccLoss:0.87836 | valid MseLoss:0.23879 | valid PccLoss:0.96724\n",
            "end of epoch:221 | time: 0.05s | train loss:0.21038 | valid loss:0.34118 | train MseLoss:0.13635 | train PccLoss:0.87669 | valid MseLoss:0.26977 | valid PccLoss:0.98389\n",
            "end of epoch:222 | time: 0.05s | train loss:0.20956 | valid loss:0.31038 | train MseLoss:0.13545 | train PccLoss:0.87656 | valid MseLoss:0.23724 | valid PccLoss:0.96865\n",
            "end of epoch:223 | time: 0.05s | train loss:0.21048 | valid loss:0.32355 | train MseLoss:0.13645 | train PccLoss:0.87667 | valid MseLoss:0.25166 | valid PccLoss:0.97048\n",
            "end of epoch:224 | time: 0.05s | train loss:0.21027 | valid loss:0.32560 | train MseLoss:0.13617 | train PccLoss:0.87722 | valid MseLoss:0.25198 | valid PccLoss:0.98813\n",
            "end of epoch:225 | time: 0.05s | train loss:0.21173 | valid loss:0.32601 | train MseLoss:0.13769 | train PccLoss:0.87802 | valid MseLoss:0.25595 | valid PccLoss:0.95655\n",
            "end of epoch:226 | time: 0.05s | train loss:0.21450 | valid loss:0.33126 | train MseLoss:0.14068 | train PccLoss:0.87889 | valid MseLoss:0.26014 | valid PccLoss:0.97133\n",
            "end of epoch:227 | time: 0.05s | train loss:0.21152 | valid loss:0.30644 | train MseLoss:0.13770 | train PccLoss:0.87590 | valid MseLoss:0.23283 | valid PccLoss:0.96902\n",
            "end of epoch:228 | time: 0.05s | train loss:0.21006 | valid loss:0.31487 | train MseLoss:0.13622 | train PccLoss:0.87461 | valid MseLoss:0.24092 | valid PccLoss:0.98046\n",
            "end of epoch:229 | time: 0.05s | train loss:0.21083 | valid loss:0.32640 | train MseLoss:0.13712 | train PccLoss:0.87421 | valid MseLoss:0.25354 | valid PccLoss:0.98208\n",
            "end of epoch:230 | time: 0.05s | train loss:0.21098 | valid loss:0.31818 | train MseLoss:0.13723 | train PccLoss:0.87477 | valid MseLoss:0.24539 | valid PccLoss:0.97334\n",
            "end of epoch:231 | time: 0.05s | train loss:0.20959 | valid loss:0.32163 | train MseLoss:0.13598 | train PccLoss:0.87207 | valid MseLoss:0.24976 | valid PccLoss:0.96847\n",
            "end of epoch:232 | time: 0.05s | train loss:0.21301 | valid loss:0.30497 | train MseLoss:0.13928 | train PccLoss:0.87660 | valid MseLoss:0.23088 | valid PccLoss:0.97175\n",
            "end of epoch:233 | time: 0.05s | train loss:0.20844 | valid loss:0.31146 | train MseLoss:0.13493 | train PccLoss:0.86999 | valid MseLoss:0.23877 | valid PccLoss:0.96567\n",
            "end of epoch:234 | time: 0.05s | train loss:0.20957 | valid loss:0.30452 | train MseLoss:0.13618 | train PccLoss:0.87016 | valid MseLoss:0.23148 | valid PccLoss:0.96188\n",
            "end of epoch:235 | time: 0.05s | train loss:0.21036 | valid loss:0.31912 | train MseLoss:0.13699 | train PccLoss:0.87068 | valid MseLoss:0.24720 | valid PccLoss:0.96648\n",
            "end of epoch:236 | time: 0.05s | train loss:0.21105 | valid loss:0.32149 | train MseLoss:0.13787 | train PccLoss:0.86963 | valid MseLoss:0.24951 | valid PccLoss:0.96934\n",
            "end of epoch:237 | time: 0.05s | train loss:0.21127 | valid loss:0.30192 | train MseLoss:0.13841 | train PccLoss:0.86698 | valid MseLoss:0.22715 | valid PccLoss:0.97490\n",
            "end of epoch:238 | time: 0.05s | train loss:0.21065 | valid loss:0.32885 | train MseLoss:0.13730 | train PccLoss:0.87079 | valid MseLoss:0.25688 | valid PccLoss:0.97650\n",
            "end of epoch:239 | time: 0.05s | train loss:0.21101 | valid loss:0.31068 | train MseLoss:0.13792 | train PccLoss:0.86883 | valid MseLoss:0.23783 | valid PccLoss:0.96632\n",
            "end of epoch:240 | time: 0.05s | train loss:0.21056 | valid loss:0.32067 | train MseLoss:0.13777 | train PccLoss:0.86568 | valid MseLoss:0.24884 | valid PccLoss:0.96712\n",
            "end of epoch:241 | time: 0.05s | train loss:0.20801 | valid loss:0.32475 | train MseLoss:0.13486 | train PccLoss:0.86636 | valid MseLoss:0.25341 | valid PccLoss:0.96683\n",
            "end of epoch:242 | time: 0.05s | train loss:0.20931 | valid loss:0.32491 | train MseLoss:0.13657 | train PccLoss:0.86392 | valid MseLoss:0.25296 | valid PccLoss:0.97241\n",
            "end of epoch:243 | time: 0.05s | train loss:0.21310 | valid loss:0.31400 | train MseLoss:0.14041 | train PccLoss:0.86728 | valid MseLoss:0.24091 | valid PccLoss:0.97187\n",
            "end of epoch:244 | time: 0.05s | train loss:0.21054 | valid loss:0.33280 | train MseLoss:0.13766 | train PccLoss:0.86643 | valid MseLoss:0.26071 | valid PccLoss:0.98165\n",
            "end of epoch:245 | time: 0.05s | train loss:0.20862 | valid loss:0.29897 | train MseLoss:0.13582 | train PccLoss:0.86376 | valid MseLoss:0.22324 | valid PccLoss:0.98052\n",
            "end of epoch:246 | time: 0.05s | train loss:0.21020 | valid loss:0.31387 | train MseLoss:0.13710 | train PccLoss:0.86818 | valid MseLoss:0.24139 | valid PccLoss:0.96619\n",
            "end of epoch:247 | time: 0.05s | train loss:0.20936 | valid loss:0.30959 | train MseLoss:0.13649 | train PccLoss:0.86519 | valid MseLoss:0.23700 | valid PccLoss:0.96290\n",
            "end of epoch:248 | time: 0.05s | train loss:0.21066 | valid loss:0.31450 | train MseLoss:0.13805 | train PccLoss:0.86409 | valid MseLoss:0.24281 | valid PccLoss:0.95975\n",
            "end of epoch:249 | time: 0.05s | train loss:0.21155 | valid loss:0.34486 | train MseLoss:0.13881 | train PccLoss:0.86619 | valid MseLoss:0.27568 | valid PccLoss:0.96742\n",
            "end of epoch:250 | time: 0.05s | train loss:0.21283 | valid loss:0.31764 | train MseLoss:0.14048 | train PccLoss:0.86390 | valid MseLoss:0.24515 | valid PccLoss:0.97008\n",
            "end of epoch:251 | time: 0.05s | train loss:0.20973 | valid loss:0.31892 | train MseLoss:0.13721 | train PccLoss:0.86244 | valid MseLoss:0.24691 | valid PccLoss:0.96700\n",
            "end of epoch:252 | time: 0.05s | train loss:0.20905 | valid loss:0.31980 | train MseLoss:0.13656 | train PccLoss:0.86138 | valid MseLoss:0.24711 | valid PccLoss:0.97407\n",
            "end of epoch:253 | time: 0.05s | train loss:0.20950 | valid loss:0.32042 | train MseLoss:0.13731 | train PccLoss:0.85922 | valid MseLoss:0.24842 | valid PccLoss:0.96848\n",
            "end of epoch:254 | time: 0.05s | train loss:0.20998 | valid loss:0.31505 | train MseLoss:0.13744 | train PccLoss:0.86285 | valid MseLoss:0.24246 | valid PccLoss:0.96834\n",
            "end of epoch:255 | time: 0.05s | train loss:0.20931 | valid loss:0.31074 | train MseLoss:0.13663 | train PccLoss:0.86340 | valid MseLoss:0.23857 | valid PccLoss:0.96026\n",
            "end of epoch:256 | time: 0.05s | train loss:0.21408 | valid loss:0.31695 | train MseLoss:0.14169 | train PccLoss:0.86555 | valid MseLoss:0.24424 | valid PccLoss:0.97128\n",
            "end of epoch:257 | time: 0.05s | train loss:0.20978 | valid loss:0.30797 | train MseLoss:0.13734 | train PccLoss:0.86175 | valid MseLoss:0.23463 | valid PccLoss:0.96809\n",
            "end of epoch:258 | time: 0.05s | train loss:0.20915 | valid loss:0.31132 | train MseLoss:0.13678 | train PccLoss:0.86048 | valid MseLoss:0.23946 | valid PccLoss:0.95809\n",
            "end of epoch:259 | time: 0.05s | train loss:0.21141 | valid loss:0.33495 | train MseLoss:0.13950 | train PccLoss:0.85866 | valid MseLoss:0.26425 | valid PccLoss:0.97125\n",
            "end of epoch:260 | time: 0.05s | train loss:0.20845 | valid loss:0.32873 | train MseLoss:0.13619 | train PccLoss:0.85873 | valid MseLoss:0.25727 | valid PccLoss:0.97194\n",
            "end of epoch:261 | time: 0.05s | train loss:0.20950 | valid loss:0.30636 | train MseLoss:0.13730 | train PccLoss:0.85931 | valid MseLoss:0.23410 | valid PccLoss:0.95666\n",
            "end of epoch:262 | time: 0.05s | train loss:0.20997 | valid loss:0.30496 | train MseLoss:0.13769 | train PccLoss:0.86045 | valid MseLoss:0.23069 | valid PccLoss:0.97342\n",
            "end of epoch:263 | time: 0.05s | train loss:0.21071 | valid loss:0.31474 | train MseLoss:0.13873 | train PccLoss:0.85852 | valid MseLoss:0.24086 | valid PccLoss:0.97973\n",
            "end of epoch:264 | time: 0.05s | train loss:0.20826 | valid loss:0.32301 | train MseLoss:0.13600 | train PccLoss:0.85859 | valid MseLoss:0.25142 | valid PccLoss:0.96730\n",
            "end of epoch:265 | time: 0.05s | train loss:0.21100 | valid loss:0.33585 | train MseLoss:0.13903 | train PccLoss:0.85877 | valid MseLoss:0.26526 | valid PccLoss:0.97110\n",
            "end of epoch:266 | time: 0.05s | train loss:0.20909 | valid loss:0.30636 | train MseLoss:0.13753 | train PccLoss:0.85317 | valid MseLoss:0.23268 | valid PccLoss:0.96946\n",
            "end of epoch:267 | time: 0.05s | train loss:0.21166 | valid loss:0.31253 | train MseLoss:0.14007 | train PccLoss:0.85598 | valid MseLoss:0.23868 | valid PccLoss:0.97720\n",
            "end of epoch:268 | time: 0.05s | train loss:0.20649 | valid loss:0.34463 | train MseLoss:0.13453 | train PccLoss:0.85417 | valid MseLoss:0.27529 | valid PccLoss:0.96866\n",
            "end of epoch:269 | time: 0.05s | train loss:0.20822 | valid loss:0.32229 | train MseLoss:0.13639 | train PccLoss:0.85473 | valid MseLoss:0.25044 | valid PccLoss:0.96893\n",
            "end of epoch:270 | time: 0.05s | train loss:0.20886 | valid loss:0.31747 | train MseLoss:0.13718 | train PccLoss:0.85407 | valid MseLoss:0.24500 | valid PccLoss:0.96972\n",
            "end of epoch:271 | time: 0.05s | train loss:0.21318 | valid loss:0.31359 | train MseLoss:0.14173 | train PccLoss:0.85627 | valid MseLoss:0.24040 | valid PccLoss:0.97235\n",
            "end of epoch:272 | time: 0.05s | train loss:0.20779 | valid loss:0.30592 | train MseLoss:0.13572 | train PccLoss:0.85642 | valid MseLoss:0.23204 | valid PccLoss:0.97084\n",
            "end of epoch:273 | time: 0.05s | train loss:0.20881 | valid loss:0.33938 | train MseLoss:0.13693 | train PccLoss:0.85574 | valid MseLoss:0.26860 | valid PccLoss:0.97640\n",
            "end of epoch:274 | time: 0.05s | train loss:0.20985 | valid loss:0.32372 | train MseLoss:0.13864 | train PccLoss:0.85077 | valid MseLoss:0.25154 | valid PccLoss:0.97331\n",
            "end of epoch:275 | time: 0.05s | train loss:0.20787 | valid loss:0.31891 | train MseLoss:0.13620 | train PccLoss:0.85294 | valid MseLoss:0.24603 | valid PccLoss:0.97482\n",
            "end of epoch:276 | time: 0.05s | train loss:0.20891 | valid loss:0.31129 | train MseLoss:0.13722 | train PccLoss:0.85416 | valid MseLoss:0.23999 | valid PccLoss:0.95294\n",
            "end of epoch:277 | time: 0.05s | train loss:0.20898 | valid loss:0.30926 | train MseLoss:0.13774 | train PccLoss:0.85020 | valid MseLoss:0.23515 | valid PccLoss:0.97626\n",
            "end of epoch:278 | time: 0.05s | train loss:0.20666 | valid loss:0.31050 | train MseLoss:0.13486 | train PccLoss:0.85284 | valid MseLoss:0.23810 | valid PccLoss:0.96206\n",
            "end of epoch:279 | time: 0.05s | train loss:0.21039 | valid loss:0.32118 | train MseLoss:0.13907 | train PccLoss:0.85224 | valid MseLoss:0.25033 | valid PccLoss:0.95877\n",
            "end of epoch:280 | time: 0.05s | train loss:0.20965 | valid loss:0.31773 | train MseLoss:0.13834 | train PccLoss:0.85144 | valid MseLoss:0.24617 | valid PccLoss:0.96171\n",
            "end of epoch:281 | time: 0.05s | train loss:0.21168 | valid loss:0.32864 | train MseLoss:0.14058 | train PccLoss:0.85164 | valid MseLoss:0.25708 | valid PccLoss:0.97261\n",
            "end of epoch:282 | time: 0.05s | train loss:0.20653 | valid loss:0.30996 | train MseLoss:0.13509 | train PccLoss:0.84955 | valid MseLoss:0.23692 | valid PccLoss:0.96737\n",
            "end of epoch:283 | time: 0.05s | train loss:0.21000 | valid loss:0.29183 | train MseLoss:0.13928 | train PccLoss:0.84647 | valid MseLoss:0.21725 | valid PccLoss:0.96302\n",
            "end of epoch:284 | time: 0.05s | train loss:0.20849 | valid loss:0.30683 | train MseLoss:0.13717 | train PccLoss:0.85044 | valid MseLoss:0.23360 | valid PccLoss:0.96593\n",
            "end of epoch:285 | time: 0.05s | train loss:0.20676 | valid loss:0.32052 | train MseLoss:0.13547 | train PccLoss:0.84836 | valid MseLoss:0.24868 | valid PccLoss:0.96713\n",
            "end of epoch:286 | time: 0.05s | train loss:0.20895 | valid loss:0.30900 | train MseLoss:0.13809 | train PccLoss:0.84671 | valid MseLoss:0.23664 | valid PccLoss:0.96031\n",
            "end of epoch:287 | time: 0.05s | train loss:0.20688 | valid loss:0.30881 | train MseLoss:0.13572 | train PccLoss:0.84727 | valid MseLoss:0.23714 | valid PccLoss:0.95379\n",
            "end of epoch:288 | time: 0.05s | train loss:0.20827 | valid loss:0.29317 | train MseLoss:0.13755 | train PccLoss:0.84473 | valid MseLoss:0.21930 | valid PccLoss:0.95805\n",
            "end of epoch:289 | time: 0.05s | train loss:0.20669 | valid loss:0.33189 | train MseLoss:0.13545 | train PccLoss:0.84783 | valid MseLoss:0.26075 | valid PccLoss:0.97213\n",
            "end of epoch:290 | time: 0.05s | train loss:0.20763 | valid loss:0.33523 | train MseLoss:0.13663 | train PccLoss:0.84662 | valid MseLoss:0.26628 | valid PccLoss:0.95575\n",
            "end of epoch:291 | time: 0.05s | train loss:0.20756 | valid loss:0.33732 | train MseLoss:0.13684 | train PccLoss:0.84407 | valid MseLoss:0.26771 | valid PccLoss:0.96375\n",
            "end of epoch:292 | time: 0.05s | train loss:0.20756 | valid loss:0.30915 | train MseLoss:0.13663 | train PccLoss:0.84591 | valid MseLoss:0.23626 | valid PccLoss:0.96511\n",
            "end of epoch:293 | time: 0.05s | train loss:0.20604 | valid loss:0.34686 | train MseLoss:0.13521 | train PccLoss:0.84347 | valid MseLoss:0.27655 | valid PccLoss:0.97965\n",
            "end of epoch:294 | time: 0.05s | train loss:0.20660 | valid loss:0.31993 | train MseLoss:0.13588 | train PccLoss:0.84311 | valid MseLoss:0.24875 | valid PccLoss:0.96060\n",
            "end of epoch:295 | time: 0.05s | train loss:0.20606 | valid loss:0.32445 | train MseLoss:0.13501 | train PccLoss:0.84554 | valid MseLoss:0.25409 | valid PccLoss:0.95764\n",
            "end of epoch:296 | time: 0.05s | train loss:0.20716 | valid loss:0.33363 | train MseLoss:0.13643 | train PccLoss:0.84376 | valid MseLoss:0.26107 | valid PccLoss:0.98665\n",
            "end of epoch:297 | time: 0.05s | train loss:0.20890 | valid loss:0.30089 | train MseLoss:0.13860 | train PccLoss:0.84164 | valid MseLoss:0.22871 | valid PccLoss:0.95052\n",
            "end of epoch:298 | time: 0.05s | train loss:0.20543 | valid loss:0.33651 | train MseLoss:0.13472 | train PccLoss:0.84178 | valid MseLoss:0.26765 | valid PccLoss:0.95623\n",
            "end of epoch:299 | time: 0.05s | train loss:0.20526 | valid loss:0.32680 | train MseLoss:0.13444 | train PccLoss:0.84272 | valid MseLoss:0.25345 | valid PccLoss:0.98702\n",
            "end of epoch:300 | time: 0.05s | train loss:0.20667 | valid loss:0.30807 | train MseLoss:0.13622 | train PccLoss:0.84071 | valid MseLoss:0.23624 | valid PccLoss:0.95458\n",
            "end of epoch:301 | time: 0.05s | train loss:0.20936 | valid loss:0.31702 | train MseLoss:0.13887 | train PccLoss:0.84378 | valid MseLoss:0.24537 | valid PccLoss:0.96182\n",
            "end of epoch:302 | time: 0.05s | train loss:0.20667 | valid loss:0.29739 | train MseLoss:0.13623 | train PccLoss:0.84057 | valid MseLoss:0.22474 | valid PccLoss:0.95132\n",
            "end of epoch:303 | time: 0.05s | train loss:0.20891 | valid loss:0.29958 | train MseLoss:0.13867 | train PccLoss:0.84104 | valid MseLoss:0.22457 | valid PccLoss:0.97470\n",
            "end of epoch:304 | time: 0.05s | train loss:0.20509 | valid loss:0.31665 | train MseLoss:0.13455 | train PccLoss:0.83993 | valid MseLoss:0.24344 | valid PccLoss:0.97555\n",
            "end of epoch:305 | time: 0.07s | train loss:0.20542 | valid loss:0.33156 | train MseLoss:0.13533 | train PccLoss:0.83622 | valid MseLoss:0.26273 | valid PccLoss:0.95104\n",
            "end of epoch:306 | time: 0.07s | train loss:0.20677 | valid loss:0.30025 | train MseLoss:0.13663 | train PccLoss:0.83804 | valid MseLoss:0.22626 | valid PccLoss:0.96622\n",
            "end of epoch:307 | time: 0.06s | train loss:0.20662 | valid loss:0.30294 | train MseLoss:0.13665 | train PccLoss:0.83640 | valid MseLoss:0.22877 | valid PccLoss:0.97046\n",
            "end of epoch:308 | time: 0.05s | train loss:0.20627 | valid loss:0.31670 | train MseLoss:0.13590 | train PccLoss:0.83957 | valid MseLoss:0.24282 | valid PccLoss:0.98169\n",
            "end of epoch:309 | time: 0.05s | train loss:0.20606 | valid loss:0.32129 | train MseLoss:0.13580 | train PccLoss:0.83840 | valid MseLoss:0.25166 | valid PccLoss:0.94793\n",
            "end of epoch:310 | time: 0.05s | train loss:0.20741 | valid loss:0.29876 | train MseLoss:0.13754 | train PccLoss:0.83629 | valid MseLoss:0.22466 | valid PccLoss:0.96559\n",
            "end of epoch:311 | time: 0.05s | train loss:0.20916 | valid loss:0.31248 | train MseLoss:0.13959 | train PccLoss:0.83534 | valid MseLoss:0.23995 | valid PccLoss:0.96529\n",
            "end of epoch:312 | time: 0.05s | train loss:0.20651 | valid loss:0.31524 | train MseLoss:0.13648 | train PccLoss:0.83673 | valid MseLoss:0.24351 | valid PccLoss:0.96079\n",
            "end of epoch:313 | time: 0.05s | train loss:0.20507 | valid loss:0.29887 | train MseLoss:0.13500 | train PccLoss:0.83569 | valid MseLoss:0.22572 | valid PccLoss:0.95716\n",
            "end of epoch:314 | time: 0.05s | train loss:0.20509 | valid loss:0.32612 | train MseLoss:0.13516 | train PccLoss:0.83445 | valid MseLoss:0.25323 | valid PccLoss:0.98218\n",
            "end of epoch:315 | time: 0.05s | train loss:0.20857 | valid loss:0.30957 | train MseLoss:0.13932 | train PccLoss:0.83179 | valid MseLoss:0.23604 | valid PccLoss:0.97130\n",
            "end of epoch:316 | time: 0.05s | train loss:0.20560 | valid loss:0.30690 | train MseLoss:0.13588 | train PccLoss:0.83308 | valid MseLoss:0.23332 | valid PccLoss:0.96910\n",
            "end of epoch:317 | time: 0.05s | train loss:0.20538 | valid loss:0.30978 | train MseLoss:0.13552 | train PccLoss:0.83404 | valid MseLoss:0.23624 | valid PccLoss:0.97173\n",
            "end of epoch:318 | time: 0.05s | train loss:0.20530 | valid loss:0.31213 | train MseLoss:0.13587 | train PccLoss:0.83019 | valid MseLoss:0.24003 | valid PccLoss:0.96106\n",
            "end of epoch:319 | time: 0.05s | train loss:0.20661 | valid loss:0.32438 | train MseLoss:0.13684 | train PccLoss:0.83448 | valid MseLoss:0.25388 | valid PccLoss:0.95890\n",
            "end of epoch:320 | time: 0.05s | train loss:0.20749 | valid loss:0.32137 | train MseLoss:0.13793 | train PccLoss:0.83348 | valid MseLoss:0.25139 | valid PccLoss:0.95116\n",
            "end of epoch:321 | time: 0.05s | train loss:0.20733 | valid loss:0.33940 | train MseLoss:0.13768 | train PccLoss:0.83418 | valid MseLoss:0.26820 | valid PccLoss:0.98022\n",
            "end of epoch:322 | time: 0.05s | train loss:0.20345 | valid loss:0.31811 | train MseLoss:0.13387 | train PccLoss:0.82970 | valid MseLoss:0.24481 | valid PccLoss:0.97785\n",
            "end of epoch:323 | time: 0.05s | train loss:0.20615 | valid loss:0.32569 | train MseLoss:0.13696 | train PccLoss:0.82883 | valid MseLoss:0.25560 | valid PccLoss:0.95647\n",
            "end of epoch:324 | time: 0.06s | train loss:0.20659 | valid loss:0.29641 | train MseLoss:0.13740 | train PccLoss:0.82931 | valid MseLoss:0.22350 | valid PccLoss:0.95262\n",
            "end of epoch:325 | time: 0.05s | train loss:0.20529 | valid loss:0.31463 | train MseLoss:0.13597 | train PccLoss:0.82916 | valid MseLoss:0.24185 | valid PccLoss:0.96963\n",
            "end of epoch:326 | time: 0.05s | train loss:0.20802 | valid loss:0.32859 | train MseLoss:0.13883 | train PccLoss:0.83068 | valid MseLoss:0.25784 | valid PccLoss:0.96532\n",
            "end of epoch:327 | time: 0.05s | train loss:0.20773 | valid loss:0.30280 | train MseLoss:0.13878 | train PccLoss:0.82828 | valid MseLoss:0.22844 | valid PccLoss:0.97202\n",
            "end of epoch:328 | time: 0.05s | train loss:0.20492 | valid loss:0.31090 | train MseLoss:0.13552 | train PccLoss:0.82957 | valid MseLoss:0.23740 | valid PccLoss:0.97239\n",
            "end of epoch:329 | time: 0.05s | train loss:0.20939 | valid loss:0.32222 | train MseLoss:0.14008 | train PccLoss:0.83315 | valid MseLoss:0.24762 | valid PccLoss:0.99357\n",
            "end of epoch:330 | time: 0.05s | train loss:0.20689 | valid loss:0.32969 | train MseLoss:0.13804 | train PccLoss:0.82654 | valid MseLoss:0.25900 | valid PccLoss:0.96589\n",
            "end of epoch:331 | time: 0.05s | train loss:0.20585 | valid loss:0.30692 | train MseLoss:0.13669 | train PccLoss:0.82832 | valid MseLoss:0.23142 | valid PccLoss:0.98641\n",
            "end of epoch:332 | time: 0.05s | train loss:0.20684 | valid loss:0.31762 | train MseLoss:0.13780 | train PccLoss:0.82823 | valid MseLoss:0.24533 | valid PccLoss:0.96825\n",
            "end of epoch:333 | time: 0.05s | train loss:0.20434 | valid loss:0.33877 | train MseLoss:0.13547 | train PccLoss:0.82418 | valid MseLoss:0.26851 | valid PccLoss:0.97113\n",
            "end of epoch:334 | time: 0.05s | train loss:0.20591 | valid loss:0.30160 | train MseLoss:0.13694 | train PccLoss:0.82664 | valid MseLoss:0.22911 | valid PccLoss:0.95399\n",
            "end of epoch:335 | time: 0.05s | train loss:0.20512 | valid loss:0.32747 | train MseLoss:0.13602 | train PccLoss:0.82700 | valid MseLoss:0.25675 | valid PccLoss:0.96397\n",
            "end of epoch:336 | time: 0.05s | train loss:0.20505 | valid loss:0.30839 | train MseLoss:0.13609 | train PccLoss:0.82570 | valid MseLoss:0.23427 | valid PccLoss:0.97553\n",
            "end of epoch:337 | time: 0.05s | train loss:0.20752 | valid loss:0.34144 | train MseLoss:0.13871 | train PccLoss:0.82680 | valid MseLoss:0.27063 | valid PccLoss:0.97875\n",
            "end of epoch:338 | time: 0.05s | train loss:0.20549 | valid loss:0.32142 | train MseLoss:0.13648 | train PccLoss:0.82661 | valid MseLoss:0.25077 | valid PccLoss:0.95727\n",
            "end of epoch:339 | time: 0.05s | train loss:0.20280 | valid loss:0.30433 | train MseLoss:0.13396 | train PccLoss:0.82236 | valid MseLoss:0.23225 | valid PccLoss:0.95306\n",
            "end of epoch:340 | time: 0.05s | train loss:0.20529 | valid loss:0.35163 | train MseLoss:0.13620 | train PccLoss:0.82708 | valid MseLoss:0.28403 | valid PccLoss:0.96004\n",
            "end of epoch:341 | time: 0.05s | train loss:0.20349 | valid loss:0.35193 | train MseLoss:0.13491 | train PccLoss:0.82074 | valid MseLoss:0.28262 | valid PccLoss:0.97571\n",
            "end of epoch:342 | time: 0.05s | train loss:0.20415 | valid loss:0.33500 | train MseLoss:0.13557 | train PccLoss:0.82130 | valid MseLoss:0.26698 | valid PccLoss:0.94716\n",
            "end of epoch:343 | time: 0.05s | train loss:0.20275 | valid loss:0.34320 | train MseLoss:0.13418 | train PccLoss:0.81993 | valid MseLoss:0.27368 | valid PccLoss:0.96893\n",
            "end of epoch:344 | time: 0.05s | train loss:0.20600 | valid loss:0.30332 | train MseLoss:0.13768 | train PccLoss:0.82092 | valid MseLoss:0.22978 | valid PccLoss:0.96515\n",
            "end of epoch:345 | time: 0.06s | train loss:0.20759 | valid loss:0.30392 | train MseLoss:0.13934 | train PccLoss:0.82185 | valid MseLoss:0.23218 | valid PccLoss:0.94957\n",
            "end of epoch:346 | time: 0.05s | train loss:0.20531 | valid loss:0.31410 | train MseLoss:0.13688 | train PccLoss:0.82115 | valid MseLoss:0.24110 | valid PccLoss:0.97107\n",
            "end of epoch:347 | time: 0.05s | train loss:0.20315 | valid loss:0.32226 | train MseLoss:0.13485 | train PccLoss:0.81788 | valid MseLoss:0.25258 | valid PccLoss:0.94939\n",
            "end of epoch:348 | time: 0.05s | train loss:0.20349 | valid loss:0.31590 | train MseLoss:0.13518 | train PccLoss:0.81825 | valid MseLoss:0.24345 | valid PccLoss:0.96794\n",
            "end of epoch:349 | time: 0.05s | train loss:0.20452 | valid loss:0.30800 | train MseLoss:0.13636 | train PccLoss:0.81801 | valid MseLoss:0.23478 | valid PccLoss:0.96695\n",
            "end of epoch:350 | time: 0.05s | train loss:0.20522 | valid loss:0.31757 | train MseLoss:0.13724 | train PccLoss:0.81702 | valid MseLoss:0.24414 | valid PccLoss:0.97841\n",
            "end of epoch:351 | time: 0.05s | train loss:0.20579 | valid loss:0.30299 | train MseLoss:0.13783 | train PccLoss:0.81737 | valid MseLoss:0.23062 | valid PccLoss:0.95435\n",
            "end of epoch:352 | time: 0.05s | train loss:0.20328 | valid loss:0.30851 | train MseLoss:0.13537 | train PccLoss:0.81445 | valid MseLoss:0.23507 | valid PccLoss:0.96943\n",
            "end of epoch:353 | time: 0.05s | train loss:0.20701 | valid loss:0.31747 | train MseLoss:0.13894 | train PccLoss:0.81963 | valid MseLoss:0.24502 | valid PccLoss:0.96957\n",
            "end of epoch:354 | time: 0.05s | train loss:0.20288 | valid loss:0.32291 | train MseLoss:0.13468 | train PccLoss:0.81667 | valid MseLoss:0.25264 | valid PccLoss:0.95535\n",
            "end of epoch:355 | time: 0.05s | train loss:0.20397 | valid loss:0.32972 | train MseLoss:0.13607 | train PccLoss:0.81508 | valid MseLoss:0.25839 | valid PccLoss:0.97168\n",
            "end of epoch:356 | time: 0.05s | train loss:0.20433 | valid loss:0.32973 | train MseLoss:0.13651 | train PccLoss:0.81471 | valid MseLoss:0.25775 | valid PccLoss:0.97753\n",
            "end of epoch:357 | time: 0.05s | train loss:0.20496 | valid loss:0.31398 | train MseLoss:0.13699 | train PccLoss:0.81669 | valid MseLoss:0.24179 | valid PccLoss:0.96365\n",
            "end of epoch:358 | time: 0.05s | train loss:0.20554 | valid loss:0.31626 | train MseLoss:0.13753 | train PccLoss:0.81764 | valid MseLoss:0.24371 | valid PccLoss:0.96914\n",
            "end of epoch:359 | time: 0.05s | train loss:0.20714 | valid loss:0.30705 | train MseLoss:0.13913 | train PccLoss:0.81925 | valid MseLoss:0.23447 | valid PccLoss:0.96026\n",
            "end of epoch:360 | time: 0.05s | train loss:0.20488 | valid loss:0.29616 | train MseLoss:0.13712 | train PccLoss:0.81479 | valid MseLoss:0.22376 | valid PccLoss:0.94775\n",
            "end of epoch:361 | time: 0.05s | train loss:0.20570 | valid loss:0.31468 | train MseLoss:0.13789 | train PccLoss:0.81597 | valid MseLoss:0.24196 | valid PccLoss:0.96912\n",
            "end of epoch:362 | time: 0.05s | train loss:0.20245 | valid loss:0.33677 | train MseLoss:0.13457 | train PccLoss:0.81338 | valid MseLoss:0.26833 | valid PccLoss:0.95277\n",
            "end of epoch:363 | time: 0.05s | train loss:0.20530 | valid loss:0.31534 | train MseLoss:0.13736 | train PccLoss:0.81671 | valid MseLoss:0.24294 | valid PccLoss:0.96688\n",
            "end of epoch:364 | time: 0.05s | train loss:0.20278 | valid loss:0.33335 | train MseLoss:0.13509 | train PccLoss:0.81194 | valid MseLoss:0.26230 | valid PccLoss:0.97279\n",
            "end of epoch:365 | time: 0.05s | train loss:0.20531 | valid loss:0.31365 | train MseLoss:0.13739 | train PccLoss:0.81665 | valid MseLoss:0.24203 | valid PccLoss:0.95830\n",
            "end of epoch:366 | time: 0.06s | train loss:0.20388 | valid loss:0.30085 | train MseLoss:0.13605 | train PccLoss:0.81433 | valid MseLoss:0.22675 | valid PccLoss:0.96776\n",
            "end of epoch:367 | time: 0.05s | train loss:0.20289 | valid loss:0.32036 | train MseLoss:0.13531 | train PccLoss:0.81109 | valid MseLoss:0.24794 | valid PccLoss:0.97212\n",
            "end of epoch:368 | time: 0.05s | train loss:0.20341 | valid loss:0.31430 | train MseLoss:0.13584 | train PccLoss:0.81147 | valid MseLoss:0.24235 | valid PccLoss:0.96182\n",
            "end of epoch:369 | time: 0.05s | train loss:0.20333 | valid loss:0.31005 | train MseLoss:0.13598 | train PccLoss:0.80951 | valid MseLoss:0.23859 | valid PccLoss:0.95314\n",
            "end of epoch:370 | time: 0.05s | train loss:0.20081 | valid loss:0.31457 | train MseLoss:0.13360 | train PccLoss:0.80568 | valid MseLoss:0.24190 | valid PccLoss:0.96858\n",
            "end of epoch:371 | time: 0.05s | train loss:0.20194 | valid loss:0.31635 | train MseLoss:0.13447 | train PccLoss:0.80922 | valid MseLoss:0.24431 | valid PccLoss:0.96469\n",
            "end of epoch:372 | time: 0.05s | train loss:0.20292 | valid loss:0.31789 | train MseLoss:0.13529 | train PccLoss:0.81155 | valid MseLoss:0.24602 | valid PccLoss:0.96471\n",
            "end of epoch:373 | time: 0.05s | train loss:0.20429 | valid loss:0.31646 | train MseLoss:0.13710 | train PccLoss:0.80894 | valid MseLoss:0.24311 | valid PccLoss:0.97663\n",
            "end of epoch:374 | time: 0.05s | train loss:0.20555 | valid loss:0.31336 | train MseLoss:0.13869 | train PccLoss:0.80734 | valid MseLoss:0.24282 | valid PccLoss:0.94823\n",
            "end of epoch:375 | time: 0.06s | train loss:0.20201 | valid loss:0.29522 | train MseLoss:0.13449 | train PccLoss:0.80974 | valid MseLoss:0.22079 | valid PccLoss:0.96515\n",
            "end of epoch:376 | time: 0.07s | train loss:0.20654 | valid loss:0.30501 | train MseLoss:0.13950 | train PccLoss:0.80986 | valid MseLoss:0.23187 | valid PccLoss:0.96330\n",
            "end of epoch:377 | time: 0.06s | train loss:0.20353 | valid loss:0.31942 | train MseLoss:0.13644 | train PccLoss:0.80731 | valid MseLoss:0.24842 | valid PccLoss:0.95844\n",
            "end of epoch:378 | time: 0.06s | train loss:0.20219 | valid loss:0.31581 | train MseLoss:0.13499 | train PccLoss:0.80705 | valid MseLoss:0.24437 | valid PccLoss:0.95883\n",
            "end of epoch:379 | time: 0.06s | train loss:0.20051 | valid loss:0.33136 | train MseLoss:0.13313 | train PccLoss:0.80691 | valid MseLoss:0.26376 | valid PccLoss:0.93974\n",
            "end of epoch:380 | time: 0.06s | train loss:0.20355 | valid loss:0.33119 | train MseLoss:0.13658 | train PccLoss:0.80632 | valid MseLoss:0.26185 | valid PccLoss:0.95520\n",
            "end of epoch:381 | time: 0.06s | train loss:0.20182 | valid loss:0.32331 | train MseLoss:0.13501 | train PccLoss:0.80308 | valid MseLoss:0.25249 | valid PccLoss:0.96070\n",
            "end of epoch:382 | time: 0.06s | train loss:0.20077 | valid loss:0.29334 | train MseLoss:0.13366 | train PccLoss:0.80477 | valid MseLoss:0.21988 | valid PccLoss:0.95443\n",
            "end of epoch:383 | time: 0.06s | train loss:0.20299 | valid loss:0.31245 | train MseLoss:0.13584 | train PccLoss:0.80734 | valid MseLoss:0.24114 | valid PccLoss:0.95423\n",
            "end of epoch:384 | time: 0.07s | train loss:0.20198 | valid loss:0.31105 | train MseLoss:0.13496 | train PccLoss:0.80514 | valid MseLoss:0.23917 | valid PccLoss:0.95795\n",
            "end of epoch:385 | time: 0.06s | train loss:0.20152 | valid loss:0.32215 | train MseLoss:0.13474 | train PccLoss:0.80256 | valid MseLoss:0.25185 | valid PccLoss:0.95484\n",
            "end of epoch:386 | time: 0.05s | train loss:0.20372 | valid loss:0.31229 | train MseLoss:0.13728 | train PccLoss:0.80169 | valid MseLoss:0.24117 | valid PccLoss:0.95236\n",
            "end of epoch:387 | time: 0.05s | train loss:0.20080 | valid loss:0.30259 | train MseLoss:0.13393 | train PccLoss:0.80263 | valid MseLoss:0.23130 | valid PccLoss:0.94418\n",
            "end of epoch:388 | time: 0.06s | train loss:0.20186 | valid loss:0.31724 | train MseLoss:0.13485 | train PccLoss:0.80494 | valid MseLoss:0.24466 | valid PccLoss:0.97052\n",
            "end of epoch:389 | time: 0.05s | train loss:0.20448 | valid loss:0.31473 | train MseLoss:0.13820 | train PccLoss:0.80103 | valid MseLoss:0.24411 | valid PccLoss:0.95035\n",
            "end of epoch:390 | time: 0.06s | train loss:0.20364 | valid loss:0.33204 | train MseLoss:0.13734 | train PccLoss:0.80030 | valid MseLoss:0.26100 | valid PccLoss:0.97139\n",
            "end of epoch:391 | time: 0.05s | train loss:0.20194 | valid loss:0.30409 | train MseLoss:0.13524 | train PccLoss:0.80225 | valid MseLoss:0.23171 | valid PccLoss:0.95554\n",
            "end of epoch:392 | time: 0.06s | train loss:0.20267 | valid loss:0.29491 | train MseLoss:0.13612 | train PccLoss:0.80156 | valid MseLoss:0.22362 | valid PccLoss:0.93651\n",
            "end of epoch:393 | time: 0.05s | train loss:0.20158 | valid loss:0.32815 | train MseLoss:0.13515 | train PccLoss:0.79949 | valid MseLoss:0.25636 | valid PccLoss:0.97423\n",
            "end of epoch:394 | time: 0.05s | train loss:0.20515 | valid loss:0.32031 | train MseLoss:0.13884 | train PccLoss:0.80192 | valid MseLoss:0.24779 | valid PccLoss:0.97300\n",
            "end of epoch:395 | time: 0.05s | train loss:0.20162 | valid loss:0.33310 | train MseLoss:0.13501 | train PccLoss:0.80112 | valid MseLoss:0.26399 | valid PccLoss:0.95507\n",
            "end of epoch:396 | time: 0.07s | train loss:0.20188 | valid loss:0.31276 | train MseLoss:0.13552 | train PccLoss:0.79908 | valid MseLoss:0.24150 | valid PccLoss:0.95404\n",
            "end of epoch:397 | time: 0.06s | train loss:0.20282 | valid loss:0.32100 | train MseLoss:0.13625 | train PccLoss:0.80197 | valid MseLoss:0.24898 | valid PccLoss:0.96911\n",
            "end of epoch:398 | time: 0.06s | train loss:0.20242 | valid loss:0.30265 | train MseLoss:0.13587 | train PccLoss:0.80138 | valid MseLoss:0.23060 | valid PccLoss:0.95104\n",
            "end of epoch:399 | time: 0.06s | train loss:0.20422 | valid loss:0.35088 | train MseLoss:0.13814 | train PccLoss:0.79896 | valid MseLoss:0.28203 | valid PccLoss:0.97051\n",
            "end of epoch:400 | time: 0.06s | train loss:0.20358 | valid loss:0.31121 | train MseLoss:0.13743 | train PccLoss:0.79901 | valid MseLoss:0.24080 | valid PccLoss:0.94488\n",
            "end of epoch:401 | time: 0.06s | train loss:0.19977 | valid loss:0.31719 | train MseLoss:0.13359 | train PccLoss:0.79537 | valid MseLoss:0.24661 | valid PccLoss:0.95238\n",
            "end of epoch:402 | time: 0.06s | train loss:0.20131 | valid loss:0.33644 | train MseLoss:0.13477 | train PccLoss:0.80020 | valid MseLoss:0.26688 | valid PccLoss:0.96250\n",
            "end of epoch:403 | time: 0.06s | train loss:0.20014 | valid loss:0.31489 | train MseLoss:0.13378 | train PccLoss:0.79745 | valid MseLoss:0.24262 | valid PccLoss:0.96539\n",
            "end of epoch:404 | time: 0.08s | train loss:0.20447 | valid loss:0.30498 | train MseLoss:0.13835 | train PccLoss:0.79954 | valid MseLoss:0.23164 | valid PccLoss:0.96501\n",
            "end of epoch:405 | time: 0.08s | train loss:0.20059 | valid loss:0.31136 | train MseLoss:0.13501 | train PccLoss:0.79080 | valid MseLoss:0.23935 | valid PccLoss:0.95946\n",
            "end of epoch:406 | time: 0.07s | train loss:0.19947 | valid loss:0.30619 | train MseLoss:0.13321 | train PccLoss:0.79580 | valid MseLoss:0.23324 | valid PccLoss:0.96272\n",
            "end of epoch:407 | time: 0.09s | train loss:0.20119 | valid loss:0.29620 | train MseLoss:0.13519 | train PccLoss:0.79521 | valid MseLoss:0.22368 | valid PccLoss:0.94890\n",
            "end of epoch:408 | time: 0.06s | train loss:0.20297 | valid loss:0.31311 | train MseLoss:0.13740 | train PccLoss:0.79305 | valid MseLoss:0.24124 | valid PccLoss:0.95993\n",
            "end of epoch:409 | time: 0.06s | train loss:0.19913 | valid loss:0.31776 | train MseLoss:0.13315 | train PccLoss:0.79293 | valid MseLoss:0.24596 | valid PccLoss:0.96393\n",
            "end of epoch:410 | time: 0.07s | train loss:0.20032 | valid loss:0.35510 | train MseLoss:0.13449 | train PccLoss:0.79284 | valid MseLoss:0.28756 | valid PccLoss:0.96295\n",
            "end of epoch:411 | time: 0.06s | train loss:0.20034 | valid loss:0.33053 | train MseLoss:0.13444 | train PccLoss:0.79342 | valid MseLoss:0.25875 | valid PccLoss:0.97656\n",
            "end of epoch:412 | time: 0.06s | train loss:0.19955 | valid loss:0.30236 | train MseLoss:0.13384 | train PccLoss:0.79091 | valid MseLoss:0.23007 | valid PccLoss:0.95298\n",
            "end of epoch:413 | time: 0.06s | train loss:0.19975 | valid loss:0.31701 | train MseLoss:0.13395 | train PccLoss:0.79201 | valid MseLoss:0.24494 | valid PccLoss:0.96562\n",
            "end of epoch:414 | time: 0.06s | train loss:0.19872 | valid loss:0.32314 | train MseLoss:0.13281 | train PccLoss:0.79187 | valid MseLoss:0.25368 | valid PccLoss:0.94827\n",
            "end of epoch:415 | time: 0.07s | train loss:0.19974 | valid loss:0.31266 | train MseLoss:0.13370 | train PccLoss:0.79416 | valid MseLoss:0.24168 | valid PccLoss:0.95152\n",
            "end of epoch:416 | time: 0.06s | train loss:0.20076 | valid loss:0.31957 | train MseLoss:0.13531 | train PccLoss:0.78979 | valid MseLoss:0.24708 | valid PccLoss:0.97205\n",
            "end of epoch:417 | time: 0.06s | train loss:0.19903 | valid loss:0.32869 | train MseLoss:0.13332 | train PccLoss:0.79037 | valid MseLoss:0.25828 | valid PccLoss:0.96240\n",
            "end of epoch:418 | time: 0.06s | train loss:0.20260 | valid loss:0.29444 | train MseLoss:0.13715 | train PccLoss:0.79161 | valid MseLoss:0.22095 | valid PccLoss:0.95589\n",
            "end of epoch:419 | time: 0.07s | train loss:0.19995 | valid loss:0.30846 | train MseLoss:0.13464 | train PccLoss:0.78774 | valid MseLoss:0.23575 | valid PccLoss:0.96283\n",
            "end of epoch:420 | time: 0.06s | train loss:0.19968 | valid loss:0.32229 | train MseLoss:0.13436 | train PccLoss:0.78757 | valid MseLoss:0.25126 | valid PccLoss:0.96157\n",
            "end of epoch:421 | time: 0.06s | train loss:0.20303 | valid loss:0.30260 | train MseLoss:0.13760 | train PccLoss:0.79184 | valid MseLoss:0.23126 | valid PccLoss:0.94462\n",
            "end of epoch:422 | time: 0.06s | train loss:0.19969 | valid loss:0.31961 | train MseLoss:0.13453 | train PccLoss:0.78620 | valid MseLoss:0.24970 | valid PccLoss:0.94874\n",
            "end of epoch:423 | time: 0.06s | train loss:0.19966 | valid loss:0.31711 | train MseLoss:0.13435 | train PccLoss:0.78746 | valid MseLoss:0.24529 | valid PccLoss:0.96352\n",
            "end of epoch:424 | time: 0.06s | train loss:0.20139 | valid loss:0.32550 | train MseLoss:0.13614 | train PccLoss:0.78866 | valid MseLoss:0.25544 | valid PccLoss:0.95611\n",
            "end of epoch:425 | time: 0.06s | train loss:0.20418 | valid loss:0.31118 | train MseLoss:0.13913 | train PccLoss:0.78960 | valid MseLoss:0.23956 | valid PccLoss:0.95574\n",
            "end of epoch:426 | time: 0.05s | train loss:0.19901 | valid loss:0.31809 | train MseLoss:0.13387 | train PccLoss:0.78521 | valid MseLoss:0.24922 | valid PccLoss:0.93791\n",
            "end of epoch:427 | time: 0.05s | train loss:0.20036 | valid loss:0.30724 | train MseLoss:0.13500 | train PccLoss:0.78854 | valid MseLoss:0.23583 | valid PccLoss:0.94994\n",
            "end of epoch:428 | time: 0.05s | train loss:0.20346 | valid loss:0.34138 | train MseLoss:0.13840 | train PccLoss:0.78901 | valid MseLoss:0.27346 | valid PccLoss:0.95271\n",
            "end of epoch:429 | time: 0.05s | train loss:0.20414 | valid loss:0.31658 | train MseLoss:0.13931 | train PccLoss:0.78759 | valid MseLoss:0.24422 | valid PccLoss:0.96775\n",
            "end of epoch:430 | time: 0.05s | train loss:0.20204 | valid loss:0.33509 | train MseLoss:0.13725 | train PccLoss:0.78514 | valid MseLoss:0.26633 | valid PccLoss:0.95391\n",
            "end of epoch:431 | time: 0.05s | train loss:0.19972 | valid loss:0.33275 | train MseLoss:0.13466 | train PccLoss:0.78526 | valid MseLoss:0.26490 | valid PccLoss:0.94342\n",
            "end of epoch:432 | time: 0.05s | train loss:0.20201 | valid loss:0.30324 | train MseLoss:0.13724 | train PccLoss:0.78495 | valid MseLoss:0.22879 | valid PccLoss:0.97331\n",
            "end of epoch:433 | time: 0.05s | train loss:0.19790 | valid loss:0.30843 | train MseLoss:0.13254 | train PccLoss:0.78613 | valid MseLoss:0.23749 | valid PccLoss:0.94684\n",
            "end of epoch:434 | time: 0.05s | train loss:0.19919 | valid loss:0.29683 | train MseLoss:0.13416 | train PccLoss:0.78449 | valid MseLoss:0.22325 | valid PccLoss:0.95911\n",
            "end of epoch:435 | time: 0.05s | train loss:0.19911 | valid loss:0.29888 | train MseLoss:0.13452 | train PccLoss:0.78037 | valid MseLoss:0.22521 | valid PccLoss:0.96192\n",
            "end of epoch:436 | time: 0.05s | train loss:0.19969 | valid loss:0.31279 | train MseLoss:0.13513 | train PccLoss:0.78075 | valid MseLoss:0.24278 | valid PccLoss:0.94287\n",
            "end of epoch:437 | time: 0.05s | train loss:0.19989 | valid loss:0.30748 | train MseLoss:0.13490 | train PccLoss:0.78480 | valid MseLoss:0.23496 | valid PccLoss:0.96014\n",
            "end of epoch:438 | time: 0.06s | train loss:0.20107 | valid loss:0.34032 | train MseLoss:0.13622 | train PccLoss:0.78469 | valid MseLoss:0.26965 | valid PccLoss:0.97635\n",
            "end of epoch:439 | time: 0.05s | train loss:0.19748 | valid loss:0.31595 | train MseLoss:0.13296 | train PccLoss:0.77815 | valid MseLoss:0.24349 | valid PccLoss:0.96808\n",
            "end of epoch:440 | time: 0.05s | train loss:0.19759 | valid loss:0.30610 | train MseLoss:0.13274 | train PccLoss:0.78117 | valid MseLoss:0.23613 | valid PccLoss:0.93582\n",
            "end of epoch:441 | time: 0.05s | train loss:0.19978 | valid loss:0.33083 | train MseLoss:0.13506 | train PccLoss:0.78227 | valid MseLoss:0.26217 | valid PccLoss:0.94877\n",
            "end of epoch:442 | time: 0.05s | train loss:0.20043 | valid loss:0.29871 | train MseLoss:0.13580 | train PccLoss:0.78214 | valid MseLoss:0.22784 | valid PccLoss:0.93653\n",
            "end of epoch:443 | time: 0.05s | train loss:0.19960 | valid loss:0.30788 | train MseLoss:0.13505 | train PccLoss:0.78056 | valid MseLoss:0.23578 | valid PccLoss:0.95679\n",
            "end of epoch:444 | time: 0.05s | train loss:0.19789 | valid loss:0.29729 | train MseLoss:0.13298 | train PccLoss:0.78205 | valid MseLoss:0.22510 | valid PccLoss:0.94701\n",
            "end of epoch:445 | time: 0.05s | train loss:0.19908 | valid loss:0.30023 | train MseLoss:0.13476 | train PccLoss:0.77795 | valid MseLoss:0.22923 | valid PccLoss:0.93921\n",
            "end of epoch:446 | time: 0.05s | train loss:0.19857 | valid loss:0.30341 | train MseLoss:0.13398 | train PccLoss:0.77990 | valid MseLoss:0.23353 | valid PccLoss:0.93231\n",
            "end of epoch:447 | time: 0.05s | train loss:0.19941 | valid loss:0.33526 | train MseLoss:0.13464 | train PccLoss:0.78238 | valid MseLoss:0.26778 | valid PccLoss:0.94252\n",
            "end of epoch:448 | time: 0.05s | train loss:0.20003 | valid loss:0.31682 | train MseLoss:0.13581 | train PccLoss:0.77801 | valid MseLoss:0.24545 | valid PccLoss:0.95916\n",
            "end of epoch:449 | time: 0.05s | train loss:0.20195 | valid loss:0.31399 | train MseLoss:0.13811 | train PccLoss:0.77653 | valid MseLoss:0.24331 | valid PccLoss:0.95006\n",
            "end of epoch:450 | time: 0.05s | train loss:0.19819 | valid loss:0.30250 | train MseLoss:0.13333 | train PccLoss:0.78198 | valid MseLoss:0.23015 | valid PccLoss:0.95369\n",
            "end of epoch:451 | time: 0.05s | train loss:0.19897 | valid loss:0.32279 | train MseLoss:0.13468 | train PccLoss:0.77759 | valid MseLoss:0.25230 | valid PccLoss:0.95723\n",
            "end of epoch:452 | time: 0.05s | train loss:0.19856 | valid loss:0.30897 | train MseLoss:0.13432 | train PccLoss:0.77669 | valid MseLoss:0.23775 | valid PccLoss:0.94994\n",
            "end of epoch:453 | time: 0.05s | train loss:0.19780 | valid loss:0.32736 | train MseLoss:0.13341 | train PccLoss:0.77726 | valid MseLoss:0.25821 | valid PccLoss:0.94972\n",
            "end of epoch:454 | time: 0.05s | train loss:0.19858 | valid loss:0.32785 | train MseLoss:0.13451 | train PccLoss:0.77522 | valid MseLoss:0.25789 | valid PccLoss:0.95744\n",
            "end of epoch:455 | time: 0.05s | train loss:0.19965 | valid loss:0.32807 | train MseLoss:0.13532 | train PccLoss:0.77859 | valid MseLoss:0.25697 | valid PccLoss:0.96800\n",
            "end of epoch:456 | time: 0.05s | train loss:0.19868 | valid loss:0.30720 | train MseLoss:0.13456 | train PccLoss:0.77576 | valid MseLoss:0.23454 | valid PccLoss:0.96116\n",
            "end of epoch:457 | time: 0.05s | train loss:0.19677 | valid loss:0.33339 | train MseLoss:0.13295 | train PccLoss:0.77121 | valid MseLoss:0.26338 | valid PccLoss:0.96343\n",
            "end of epoch:458 | time: 0.05s | train loss:0.19790 | valid loss:0.30299 | train MseLoss:0.13356 | train PccLoss:0.77697 | valid MseLoss:0.23236 | valid PccLoss:0.93866\n",
            "end of epoch:459 | time: 0.05s | train loss:0.19905 | valid loss:0.33633 | train MseLoss:0.13523 | train PccLoss:0.77342 | valid MseLoss:0.26807 | valid PccLoss:0.95065\n",
            "end of epoch:460 | time: 0.05s | train loss:0.19669 | valid loss:0.32910 | train MseLoss:0.13259 | train PccLoss:0.77357 | valid MseLoss:0.26092 | valid PccLoss:0.94273\n",
            "end of epoch:461 | time: 0.05s | train loss:0.19793 | valid loss:0.33260 | train MseLoss:0.13368 | train PccLoss:0.77613 | valid MseLoss:0.26216 | valid PccLoss:0.96655\n",
            "end of epoch:462 | time: 0.05s | train loss:0.19824 | valid loss:0.30934 | train MseLoss:0.13450 | train PccLoss:0.77183 | valid MseLoss:0.23567 | valid PccLoss:0.97236\n",
            "end of epoch:463 | time: 0.05s | train loss:0.19881 | valid loss:0.31499 | train MseLoss:0.13505 | train PccLoss:0.77268 | valid MseLoss:0.24520 | valid PccLoss:0.94307\n",
            "end of epoch:464 | time: 0.05s | train loss:0.20228 | valid loss:0.30480 | train MseLoss:0.13868 | train PccLoss:0.77466 | valid MseLoss:0.23154 | valid PccLoss:0.96413\n",
            "end of epoch:465 | time: 0.05s | train loss:0.19950 | valid loss:0.31658 | train MseLoss:0.13580 | train PccLoss:0.77278 | valid MseLoss:0.24555 | valid PccLoss:0.95585\n",
            "end of epoch:466 | time: 0.05s | train loss:0.20053 | valid loss:0.31039 | train MseLoss:0.13695 | train PccLoss:0.77275 | valid MseLoss:0.23858 | valid PccLoss:0.95665\n",
            "end of epoch:467 | time: 0.05s | train loss:0.20337 | valid loss:0.31509 | train MseLoss:0.13983 | train PccLoss:0.77525 | valid MseLoss:0.24286 | valid PccLoss:0.96518\n",
            "end of epoch:468 | time: 0.05s | train loss:0.19908 | valid loss:0.33890 | train MseLoss:0.13547 | train PccLoss:0.77154 | valid MseLoss:0.26804 | valid PccLoss:0.97661\n",
            "end of epoch:469 | time: 0.05s | train loss:0.20033 | valid loss:0.31909 | train MseLoss:0.13705 | train PccLoss:0.76986 | valid MseLoss:0.24796 | valid PccLoss:0.95928\n",
            "end of epoch:470 | time: 0.05s | train loss:0.19798 | valid loss:0.32172 | train MseLoss:0.13489 | train PccLoss:0.76575 | valid MseLoss:0.24954 | valid PccLoss:0.97131\n",
            "end of epoch:471 | time: 0.05s | train loss:0.19886 | valid loss:0.31632 | train MseLoss:0.13554 | train PccLoss:0.76878 | valid MseLoss:0.24492 | valid PccLoss:0.95898\n",
            "end of epoch:472 | time: 0.05s | train loss:0.19790 | valid loss:0.30156 | train MseLoss:0.13488 | train PccLoss:0.76506 | valid MseLoss:0.23092 | valid PccLoss:0.93736\n",
            "end of epoch:473 | time: 0.05s | train loss:0.19856 | valid loss:0.31743 | train MseLoss:0.13504 | train PccLoss:0.77022 | valid MseLoss:0.24483 | valid PccLoss:0.97084\n",
            "end of epoch:474 | time: 0.05s | train loss:0.19934 | valid loss:0.29624 | train MseLoss:0.13606 | train PccLoss:0.76891 | valid MseLoss:0.22643 | valid PccLoss:0.92452\n",
            "end of epoch:475 | time: 0.05s | train loss:0.19761 | valid loss:0.30298 | train MseLoss:0.13445 | train PccLoss:0.76603 | valid MseLoss:0.23207 | valid PccLoss:0.94121\n",
            "end of epoch:476 | time: 0.05s | train loss:0.19821 | valid loss:0.31249 | train MseLoss:0.13514 | train PccLoss:0.76583 | valid MseLoss:0.24158 | valid PccLoss:0.95067\n",
            "end of epoch:477 | time: 0.05s | train loss:0.19744 | valid loss:0.32652 | train MseLoss:0.13403 | train PccLoss:0.76817 | valid MseLoss:0.25596 | valid PccLoss:0.96156\n",
            "end of epoch:478 | time: 0.05s | train loss:0.19957 | valid loss:0.31822 | train MseLoss:0.13655 | train PccLoss:0.76678 | valid MseLoss:0.24751 | valid PccLoss:0.95462\n",
            "end of epoch:479 | time: 0.05s | train loss:0.19671 | valid loss:0.33202 | train MseLoss:0.13340 | train PccLoss:0.76649 | valid MseLoss:0.26067 | valid PccLoss:0.97414\n",
            "end of epoch:480 | time: 0.05s | train loss:0.19681 | valid loss:0.30943 | train MseLoss:0.13397 | train PccLoss:0.76235 | valid MseLoss:0.23627 | valid PccLoss:0.96786\n",
            "end of epoch:481 | time: 0.05s | train loss:0.19988 | valid loss:0.30522 | train MseLoss:0.13698 | train PccLoss:0.76604 | valid MseLoss:0.23509 | valid PccLoss:0.93642\n",
            "end of epoch:482 | time: 0.05s | train loss:0.19910 | valid loss:0.32259 | train MseLoss:0.13615 | train PccLoss:0.76566 | valid MseLoss:0.24940 | valid PccLoss:0.98126\n",
            "end of epoch:483 | time: 0.05s | train loss:0.19893 | valid loss:0.31232 | train MseLoss:0.13599 | train PccLoss:0.76537 | valid MseLoss:0.23883 | valid PccLoss:0.97369\n",
            "end of epoch:484 | time: 0.05s | train loss:0.19704 | valid loss:0.32693 | train MseLoss:0.13428 | train PccLoss:0.76184 | valid MseLoss:0.25606 | valid PccLoss:0.96479\n",
            "end of epoch:485 | time: 0.05s | train loss:0.19638 | valid loss:0.31570 | train MseLoss:0.13315 | train PccLoss:0.76551 | valid MseLoss:0.24339 | valid PccLoss:0.96653\n",
            "end of epoch:486 | time: 0.05s | train loss:0.19778 | valid loss:0.34294 | train MseLoss:0.13497 | train PccLoss:0.76302 | valid MseLoss:0.27277 | valid PccLoss:0.97447\n",
            "end of epoch:487 | time: 0.05s | train loss:0.19589 | valid loss:0.31525 | train MseLoss:0.13284 | train PccLoss:0.76332 | valid MseLoss:0.24476 | valid PccLoss:0.94970\n",
            "end of epoch:488 | time: 0.05s | train loss:0.19765 | valid loss:0.30755 | train MseLoss:0.13497 | train PccLoss:0.76170 | valid MseLoss:0.23821 | valid PccLoss:0.93158\n",
            "end of epoch:489 | time: 0.05s | train loss:0.19583 | valid loss:0.32264 | train MseLoss:0.13286 | train PccLoss:0.76259 | valid MseLoss:0.25211 | valid PccLoss:0.95739\n",
            "end of epoch:490 | time: 0.05s | train loss:0.19710 | valid loss:0.33478 | train MseLoss:0.13402 | train PccLoss:0.76483 | valid MseLoss:0.26489 | valid PccLoss:0.96376\n",
            "end of epoch:491 | time: 0.05s | train loss:0.19705 | valid loss:0.33798 | train MseLoss:0.13421 | train PccLoss:0.76266 | valid MseLoss:0.27107 | valid PccLoss:0.94016\n",
            "end of epoch:492 | time: 0.05s | train loss:0.19643 | valid loss:0.30705 | train MseLoss:0.13391 | train PccLoss:0.75909 | valid MseLoss:0.23670 | valid PccLoss:0.94022\n",
            "end of epoch:493 | time: 0.05s | train loss:0.19688 | valid loss:0.30646 | train MseLoss:0.13424 | train PccLoss:0.76058 | valid MseLoss:0.23402 | valid PccLoss:0.95839\n",
            "end of epoch:494 | time: 0.05s | train loss:0.19754 | valid loss:0.30002 | train MseLoss:0.13513 | train PccLoss:0.75926 | valid MseLoss:0.22702 | valid PccLoss:0.95710\n",
            "end of epoch:495 | time: 0.05s | train loss:0.19802 | valid loss:0.31700 | train MseLoss:0.13561 | train PccLoss:0.75966 | valid MseLoss:0.24552 | valid PccLoss:0.96034\n",
            "end of epoch:496 | time: 0.05s | train loss:0.19727 | valid loss:0.29696 | train MseLoss:0.13464 | train PccLoss:0.76103 | valid MseLoss:0.22429 | valid PccLoss:0.95097\n",
            "end of epoch:497 | time: 0.05s | train loss:0.19920 | valid loss:0.30635 | train MseLoss:0.13707 | train PccLoss:0.75832 | valid MseLoss:0.23617 | valid PccLoss:0.93793\n",
            "end of epoch:498 | time: 0.05s | train loss:0.19596 | valid loss:0.33595 | train MseLoss:0.13368 | train PccLoss:0.75643 | valid MseLoss:0.26684 | valid PccLoss:0.95798\n",
            "end of epoch:499 | time: 0.05s | train loss:0.19552 | valid loss:0.32607 | train MseLoss:0.13304 | train PccLoss:0.75779 | valid MseLoss:0.25717 | valid PccLoss:0.94618\n",
            "end of epoch:500 | time: 0.06s | train loss:0.19584 | valid loss:0.33477 | train MseLoss:0.13344 | train PccLoss:0.75745 | valid MseLoss:0.26684 | valid PccLoss:0.94618\n",
            "end of epoch:501 | time: 0.05s | train loss:0.19646 | valid loss:0.31826 | train MseLoss:0.13354 | train PccLoss:0.76280 | valid MseLoss:0.24839 | valid PccLoss:0.94705\n",
            "end of epoch:502 | time: 0.05s | train loss:0.19557 | valid loss:0.32219 | train MseLoss:0.13295 | train PccLoss:0.75920 | valid MseLoss:0.25230 | valid PccLoss:0.95114\n",
            "end of epoch:503 | time: 0.05s | train loss:0.19965 | valid loss:0.32565 | train MseLoss:0.13748 | train PccLoss:0.75914 | valid MseLoss:0.25769 | valid PccLoss:0.93729\n",
            "end of epoch:504 | time: 0.05s | train loss:0.19536 | valid loss:0.31252 | train MseLoss:0.13314 | train PccLoss:0.75530 | valid MseLoss:0.24131 | valid PccLoss:0.95344\n",
            "end of epoch:505 | time: 0.05s | train loss:0.19524 | valid loss:0.31958 | train MseLoss:0.13317 | train PccLoss:0.75385 | valid MseLoss:0.25064 | valid PccLoss:0.94007\n",
            "end of epoch:506 | time: 0.05s | train loss:0.19720 | valid loss:0.33169 | train MseLoss:0.13458 | train PccLoss:0.76084 | valid MseLoss:0.25960 | valid PccLoss:0.98052\n",
            "end of epoch:507 | time: 0.05s | train loss:0.19718 | valid loss:0.30310 | train MseLoss:0.13489 | train PccLoss:0.75784 | valid MseLoss:0.23202 | valid PccLoss:0.94286\n",
            "end of epoch:508 | time: 0.05s | train loss:0.19594 | valid loss:0.32525 | train MseLoss:0.13363 | train PccLoss:0.75671 | valid MseLoss:0.25515 | valid PccLoss:0.95623\n",
            "end of epoch:509 | time: 0.05s | train loss:0.19522 | valid loss:0.29062 | train MseLoss:0.13299 | train PccLoss:0.75529 | valid MseLoss:0.21661 | valid PccLoss:0.95673\n",
            "end of epoch:510 | time: 0.05s | train loss:0.19705 | valid loss:0.30475 | train MseLoss:0.13530 | train PccLoss:0.75277 | valid MseLoss:0.23364 | valid PccLoss:0.94476\n",
            "end of epoch:511 | time: 0.05s | train loss:0.20063 | valid loss:0.29882 | train MseLoss:0.13836 | train PccLoss:0.76102 | valid MseLoss:0.22409 | valid PccLoss:0.97142\n",
            "end of epoch:512 | time: 0.05s | train loss:0.19566 | valid loss:0.29256 | train MseLoss:0.13377 | train PccLoss:0.75273 | valid MseLoss:0.21955 | valid PccLoss:0.94958\n",
            "end of epoch:513 | time: 0.05s | train loss:0.19734 | valid loss:0.30781 | train MseLoss:0.13522 | train PccLoss:0.75641 | valid MseLoss:0.23524 | valid PccLoss:0.96102\n",
            "end of epoch:514 | time: 0.05s | train loss:0.19564 | valid loss:0.30334 | train MseLoss:0.13349 | train PccLoss:0.75496 | valid MseLoss:0.23263 | valid PccLoss:0.93972\n",
            "end of epoch:515 | time: 0.05s | train loss:0.19612 | valid loss:0.29496 | train MseLoss:0.13387 | train PccLoss:0.75637 | valid MseLoss:0.22297 | valid PccLoss:0.94285\n",
            "end of epoch:516 | time: 0.05s | train loss:0.19628 | valid loss:0.31177 | train MseLoss:0.13451 | train PccLoss:0.75224 | valid MseLoss:0.24078 | valid PccLoss:0.95076\n",
            "end of epoch:517 | time: 0.05s | train loss:0.19525 | valid loss:0.30854 | train MseLoss:0.13359 | train PccLoss:0.75024 | valid MseLoss:0.23670 | valid PccLoss:0.95512\n",
            "end of epoch:518 | time: 0.05s | train loss:0.19922 | valid loss:0.31169 | train MseLoss:0.13725 | train PccLoss:0.75697 | valid MseLoss:0.23845 | valid PccLoss:0.97087\n",
            "end of epoch:519 | time: 0.05s | train loss:0.19516 | valid loss:0.30119 | train MseLoss:0.13348 | train PccLoss:0.75029 | valid MseLoss:0.23065 | valid PccLoss:0.93609\n",
            "end of epoch:520 | time: 0.05s | train loss:0.19658 | valid loss:0.32465 | train MseLoss:0.13483 | train PccLoss:0.75233 | valid MseLoss:0.25307 | valid PccLoss:0.96887\n",
            "end of epoch:521 | time: 0.05s | train loss:0.19401 | valid loss:0.32101 | train MseLoss:0.13220 | train PccLoss:0.75031 | valid MseLoss:0.25042 | valid PccLoss:0.95637\n",
            "end of epoch:522 | time: 0.05s | train loss:0.19875 | valid loss:0.33512 | train MseLoss:0.13733 | train PccLoss:0.75154 | valid MseLoss:0.26593 | valid PccLoss:0.95776\n",
            "end of epoch:523 | time: 0.05s | train loss:0.19764 | valid loss:0.29870 | train MseLoss:0.13625 | train PccLoss:0.75019 | valid MseLoss:0.22671 | valid PccLoss:0.94654\n",
            "end of epoch:524 | time: 0.05s | train loss:0.19501 | valid loss:0.30553 | train MseLoss:0.13336 | train PccLoss:0.74991 | valid MseLoss:0.23409 | valid PccLoss:0.94845\n",
            "end of epoch:525 | time: 0.05s | train loss:0.19428 | valid loss:0.33370 | train MseLoss:0.13257 | train PccLoss:0.74961 | valid MseLoss:0.26535 | valid PccLoss:0.94884\n",
            "end of epoch:526 | time: 0.05s | train loss:0.19586 | valid loss:0.32502 | train MseLoss:0.13400 | train PccLoss:0.75260 | valid MseLoss:0.25515 | valid PccLoss:0.95388\n",
            "end of epoch:527 | time: 0.05s | train loss:0.19826 | valid loss:0.31821 | train MseLoss:0.13709 | train PccLoss:0.74873 | valid MseLoss:0.24875 | valid PccLoss:0.94335\n",
            "end of epoch:528 | time: 0.05s | train loss:0.19830 | valid loss:0.30560 | train MseLoss:0.13676 | train PccLoss:0.75212 | valid MseLoss:0.23447 | valid PccLoss:0.94584\n",
            "end of epoch:529 | time: 0.05s | train loss:0.19233 | valid loss:0.31804 | train MseLoss:0.13072 | train PccLoss:0.74679 | valid MseLoss:0.24844 | valid PccLoss:0.94450\n",
            "end of epoch:530 | time: 0.05s | train loss:0.19791 | valid loss:0.32206 | train MseLoss:0.13666 | train PccLoss:0.74916 | valid MseLoss:0.25183 | valid PccLoss:0.95415\n",
            "end of epoch:531 | time: 0.05s | train loss:0.19960 | valid loss:0.32073 | train MseLoss:0.13823 | train PccLoss:0.75188 | valid MseLoss:0.25023 | valid PccLoss:0.95529\n",
            "end of epoch:532 | time: 0.05s | train loss:0.19541 | valid loss:0.32113 | train MseLoss:0.13414 | train PccLoss:0.74682 | valid MseLoss:0.25007 | valid PccLoss:0.96067\n",
            "end of epoch:533 | time: 0.05s | train loss:0.19610 | valid loss:0.29447 | train MseLoss:0.13454 | train PccLoss:0.75015 | valid MseLoss:0.22349 | valid PccLoss:0.93327\n",
            "end of epoch:534 | time: 0.05s | train loss:0.19675 | valid loss:0.31006 | train MseLoss:0.13581 | train PccLoss:0.74520 | valid MseLoss:0.24032 | valid PccLoss:0.93773\n",
            "end of epoch:535 | time: 0.06s | train loss:0.19808 | valid loss:0.32694 | train MseLoss:0.13669 | train PccLoss:0.75057 | valid MseLoss:0.25914 | valid PccLoss:0.93721\n",
            "end of epoch:536 | time: 0.05s | train loss:0.19262 | valid loss:0.33273 | train MseLoss:0.13149 | train PccLoss:0.74286 | valid MseLoss:0.26440 | valid PccLoss:0.94768\n",
            "end of epoch:537 | time: 0.05s | train loss:0.19745 | valid loss:0.31736 | train MseLoss:0.13657 | train PccLoss:0.74529 | valid MseLoss:0.24807 | valid PccLoss:0.94090\n",
            "end of epoch:538 | time: 0.05s | train loss:0.19323 | valid loss:0.34641 | train MseLoss:0.13209 | train PccLoss:0.74345 | valid MseLoss:0.27712 | valid PccLoss:0.97003\n",
            "end of epoch:539 | time: 0.05s | train loss:0.19523 | valid loss:0.32050 | train MseLoss:0.13445 | train PccLoss:0.74223 | valid MseLoss:0.25312 | valid PccLoss:0.92684\n",
            "end of epoch:540 | time: 0.05s | train loss:0.19676 | valid loss:0.31003 | train MseLoss:0.13560 | train PccLoss:0.74714 | valid MseLoss:0.23812 | valid PccLoss:0.95720\n",
            "end of epoch:541 | time: 0.05s | train loss:0.19416 | valid loss:0.33569 | train MseLoss:0.13275 | train PccLoss:0.74692 | valid MseLoss:0.26460 | valid PccLoss:0.97548\n",
            "end of epoch:542 | time: 0.05s | train loss:0.19702 | valid loss:0.32590 | train MseLoss:0.13596 | train PccLoss:0.74652 | valid MseLoss:0.25299 | valid PccLoss:0.98212\n",
            "end of epoch:543 | time: 0.05s | train loss:0.19358 | valid loss:0.31943 | train MseLoss:0.13249 | train PccLoss:0.74333 | valid MseLoss:0.24798 | valid PccLoss:0.96252\n",
            "end of epoch:544 | time: 0.05s | train loss:0.19390 | valid loss:0.29834 | train MseLoss:0.13297 | train PccLoss:0.74227 | valid MseLoss:0.22673 | valid PccLoss:0.94281\n",
            "end of epoch:545 | time: 0.05s | train loss:0.19729 | valid loss:0.30451 | train MseLoss:0.13644 | train PccLoss:0.74486 | valid MseLoss:0.23554 | valid PccLoss:0.92518\n",
            "end of epoch:546 | time: 0.05s | train loss:0.19483 | valid loss:0.31167 | train MseLoss:0.13332 | train PccLoss:0.74845 | valid MseLoss:0.24072 | valid PccLoss:0.95018\n",
            "end of epoch:547 | time: 0.05s | train loss:0.19524 | valid loss:0.30346 | train MseLoss:0.13431 | train PccLoss:0.74359 | valid MseLoss:0.23169 | valid PccLoss:0.94942\n",
            "end of epoch:548 | time: 0.05s | train loss:0.19578 | valid loss:0.33173 | train MseLoss:0.13512 | train PccLoss:0.74179 | valid MseLoss:0.25981 | valid PccLoss:0.97892\n",
            "end of epoch:549 | time: 0.05s | train loss:0.19574 | valid loss:0.31563 | train MseLoss:0.13526 | train PccLoss:0.74003 | valid MseLoss:0.24681 | valid PccLoss:0.93505\n",
            "end of epoch:550 | time: 0.05s | train loss:0.19502 | valid loss:0.30010 | train MseLoss:0.13456 | train PccLoss:0.73915 | valid MseLoss:0.23077 | valid PccLoss:0.92402\n",
            "end of epoch:551 | time: 0.05s | train loss:0.19416 | valid loss:0.32475 | train MseLoss:0.13303 | train PccLoss:0.74432 | valid MseLoss:0.25616 | valid PccLoss:0.94214\n",
            "end of epoch:552 | time: 0.05s | train loss:0.19454 | valid loss:0.31982 | train MseLoss:0.13391 | train PccLoss:0.74027 | valid MseLoss:0.25050 | valid PccLoss:0.94367\n",
            "end of epoch:553 | time: 0.05s | train loss:0.19568 | valid loss:0.31795 | train MseLoss:0.13506 | train PccLoss:0.74119 | valid MseLoss:0.24697 | valid PccLoss:0.95672\n",
            "end of epoch:554 | time: 0.05s | train loss:0.19187 | valid loss:0.32833 | train MseLoss:0.13114 | train PccLoss:0.73843 | valid MseLoss:0.25786 | valid PccLoss:0.96250\n",
            "end of epoch:555 | time: 0.05s | train loss:0.19438 | valid loss:0.33203 | train MseLoss:0.13420 | train PccLoss:0.73598 | valid MseLoss:0.26221 | valid PccLoss:0.96036\n",
            "end of epoch:556 | time: 0.05s | train loss:0.19313 | valid loss:0.30211 | train MseLoss:0.13274 | train PccLoss:0.73663 | valid MseLoss:0.23283 | valid PccLoss:0.92562\n",
            "end of epoch:557 | time: 0.05s | train loss:0.19214 | valid loss:0.33661 | train MseLoss:0.13155 | train PccLoss:0.73752 | valid MseLoss:0.26942 | valid PccLoss:0.94127\n",
            "end of epoch:558 | time: 0.05s | train loss:0.19510 | valid loss:0.30737 | train MseLoss:0.13427 | train PccLoss:0.74256 | valid MseLoss:0.23625 | valid PccLoss:0.94748\n",
            "end of epoch:559 | time: 0.05s | train loss:0.19410 | valid loss:0.29926 | train MseLoss:0.13367 | train PccLoss:0.73793 | valid MseLoss:0.22643 | valid PccLoss:0.95478\n",
            "end of epoch:560 | time: 0.05s | train loss:0.19302 | valid loss:0.32081 | train MseLoss:0.13255 | train PccLoss:0.73730 | valid MseLoss:0.25234 | valid PccLoss:0.93699\n",
            "end of epoch:561 | time: 0.05s | train loss:0.19511 | valid loss:0.31231 | train MseLoss:0.13473 | train PccLoss:0.73852 | valid MseLoss:0.24103 | valid PccLoss:0.95384\n",
            "end of epoch:562 | time: 0.05s | train loss:0.19384 | valid loss:0.30426 | train MseLoss:0.13383 | train PccLoss:0.73391 | valid MseLoss:0.23223 | valid PccLoss:0.95254\n",
            "end of epoch:563 | time: 0.05s | train loss:0.19349 | valid loss:0.32171 | train MseLoss:0.13308 | train PccLoss:0.73714 | valid MseLoss:0.25314 | valid PccLoss:0.93891\n",
            "end of epoch:564 | time: 0.05s | train loss:0.19330 | valid loss:0.31211 | train MseLoss:0.13315 | train PccLoss:0.73468 | valid MseLoss:0.24220 | valid PccLoss:0.94130\n",
            "end of epoch:565 | time: 0.05s | train loss:0.19605 | valid loss:0.33007 | train MseLoss:0.13638 | train PccLoss:0.73306 | valid MseLoss:0.26082 | valid PccLoss:0.95336\n",
            "end of epoch:566 | time: 0.05s | train loss:0.19547 | valid loss:0.31367 | train MseLoss:0.13539 | train PccLoss:0.73618 | valid MseLoss:0.24580 | valid PccLoss:0.92447\n",
            "end of epoch:567 | time: 0.05s | train loss:0.19534 | valid loss:0.30536 | train MseLoss:0.13527 | train PccLoss:0.73600 | valid MseLoss:0.23365 | valid PccLoss:0.95081\n",
            "end of epoch:568 | time: 0.05s | train loss:0.19354 | valid loss:0.31790 | train MseLoss:0.13372 | train PccLoss:0.73191 | valid MseLoss:0.24625 | valid PccLoss:0.96268\n",
            "end of epoch:569 | time: 0.05s | train loss:0.19465 | valid loss:0.30180 | train MseLoss:0.13470 | train PccLoss:0.73415 | valid MseLoss:0.23073 | valid PccLoss:0.94146\n",
            "end of epoch:570 | time: 0.05s | train loss:0.19496 | valid loss:0.32950 | train MseLoss:0.13511 | train PccLoss:0.73367 | valid MseLoss:0.26133 | valid PccLoss:0.94308\n",
            "end of epoch:571 | time: 0.05s | train loss:0.19300 | valid loss:0.32323 | train MseLoss:0.13276 | train PccLoss:0.73513 | valid MseLoss:0.25323 | valid PccLoss:0.95324\n",
            "end of epoch:572 | time: 0.05s | train loss:0.19596 | valid loss:0.30462 | train MseLoss:0.13567 | train PccLoss:0.73855 | valid MseLoss:0.23236 | valid PccLoss:0.95491\n",
            "end of epoch:573 | time: 0.05s | train loss:0.19414 | valid loss:0.31976 | train MseLoss:0.13373 | train PccLoss:0.73783 | valid MseLoss:0.24951 | valid PccLoss:0.95202\n",
            "end of epoch:574 | time: 0.05s | train loss:0.19341 | valid loss:0.32354 | train MseLoss:0.13394 | train PccLoss:0.72871 | valid MseLoss:0.25471 | valid PccLoss:0.94295\n",
            "end of epoch:575 | time: 0.05s | train loss:0.19183 | valid loss:0.33394 | train MseLoss:0.13165 | train PccLoss:0.73343 | valid MseLoss:0.26804 | valid PccLoss:0.92702\n",
            "end of epoch:576 | time: 0.05s | train loss:0.19206 | valid loss:0.31455 | train MseLoss:0.13218 | train PccLoss:0.73101 | valid MseLoss:0.24536 | valid PccLoss:0.93727\n",
            "end of epoch:577 | time: 0.05s | train loss:0.19274 | valid loss:0.32456 | train MseLoss:0.13308 | train PccLoss:0.72971 | valid MseLoss:0.25559 | valid PccLoss:0.94530\n",
            "end of epoch:578 | time: 0.05s | train loss:0.19385 | valid loss:0.31657 | train MseLoss:0.13384 | train PccLoss:0.73393 | valid MseLoss:0.24585 | valid PccLoss:0.95303\n",
            "end of epoch:579 | time: 0.05s | train loss:0.19531 | valid loss:0.30280 | train MseLoss:0.13561 | train PccLoss:0.73258 | valid MseLoss:0.23121 | valid PccLoss:0.94708\n",
            "end of epoch:580 | time: 0.05s | train loss:0.19167 | valid loss:0.32552 | train MseLoss:0.13195 | train PccLoss:0.72912 | valid MseLoss:0.25411 | valid PccLoss:0.96825\n",
            "end of epoch:581 | time: 0.05s | train loss:0.19331 | valid loss:0.31246 | train MseLoss:0.13354 | train PccLoss:0.73127 | valid MseLoss:0.24114 | valid PccLoss:0.95433\n",
            "end of epoch:582 | time: 0.05s | train loss:0.19655 | valid loss:0.33067 | train MseLoss:0.13689 | train PccLoss:0.73348 | valid MseLoss:0.26329 | valid PccLoss:0.93701\n",
            "end of epoch:583 | time: 0.05s | train loss:0.19365 | valid loss:0.31285 | train MseLoss:0.13399 | train PccLoss:0.73055 | valid MseLoss:0.24186 | valid PccLoss:0.95175\n",
            "end of epoch:584 | time: 0.05s | train loss:0.19291 | valid loss:0.30334 | train MseLoss:0.13351 | train PccLoss:0.72755 | valid MseLoss:0.23124 | valid PccLoss:0.95228\n",
            "end of epoch:585 | time: 0.05s | train loss:0.19238 | valid loss:0.29309 | train MseLoss:0.13301 | train PccLoss:0.72674 | valid MseLoss:0.21966 | valid PccLoss:0.95403\n",
            "end of epoch:586 | time: 0.05s | train loss:0.19245 | valid loss:0.31333 | train MseLoss:0.13299 | train PccLoss:0.72761 | valid MseLoss:0.24215 | valid PccLoss:0.95391\n",
            "end of epoch:587 | time: 0.05s | train loss:0.19157 | valid loss:0.31779 | train MseLoss:0.13223 | train PccLoss:0.72564 | valid MseLoss:0.24709 | valid PccLoss:0.95414\n",
            "end of epoch:588 | time: 0.05s | train loss:0.19149 | valid loss:0.30863 | train MseLoss:0.13207 | train PccLoss:0.72622 | valid MseLoss:0.23832 | valid PccLoss:0.94146\n",
            "end of epoch:589 | time: 0.05s | train loss:0.19298 | valid loss:0.31973 | train MseLoss:0.13338 | train PccLoss:0.72941 | valid MseLoss:0.24954 | valid PccLoss:0.95152\n",
            "end of epoch:590 | time: 0.05s | train loss:0.19342 | valid loss:0.29693 | train MseLoss:0.13382 | train PccLoss:0.72977 | valid MseLoss:0.22376 | valid PccLoss:0.95553\n",
            "end of epoch:591 | time: 0.05s | train loss:0.19646 | valid loss:0.33887 | train MseLoss:0.13703 | train PccLoss:0.73135 | valid MseLoss:0.27178 | valid PccLoss:0.94268\n",
            "end of epoch:592 | time: 0.05s | train loss:0.19744 | valid loss:0.31855 | train MseLoss:0.13834 | train PccLoss:0.72941 | valid MseLoss:0.24745 | valid PccLoss:0.95838\n",
            "end of epoch:593 | time: 0.05s | train loss:0.19742 | valid loss:0.31247 | train MseLoss:0.13826 | train PccLoss:0.72983 | valid MseLoss:0.24041 | valid PccLoss:0.96104\n",
            "end of epoch:594 | time: 0.05s | train loss:0.19202 | valid loss:0.30941 | train MseLoss:0.13311 | train PccLoss:0.72215 | valid MseLoss:0.23810 | valid PccLoss:0.95124\n",
            "end of epoch:595 | time: 0.05s | train loss:0.19337 | valid loss:0.33255 | train MseLoss:0.13369 | train PccLoss:0.73055 | valid MseLoss:0.26388 | valid PccLoss:0.95057\n",
            "end of epoch:596 | time: 0.05s | train loss:0.19092 | valid loss:0.33401 | train MseLoss:0.13172 | train PccLoss:0.72373 | valid MseLoss:0.26606 | valid PccLoss:0.94555\n",
            "end of epoch:597 | time: 0.05s | train loss:0.19290 | valid loss:0.30650 | train MseLoss:0.13382 | train PccLoss:0.72464 | valid MseLoss:0.23654 | valid PccLoss:0.93606\n",
            "end of epoch:598 | time: 0.05s | train loss:0.19170 | valid loss:0.31518 | train MseLoss:0.13266 | train PccLoss:0.72308 | valid MseLoss:0.24430 | valid PccLoss:0.95310\n",
            "end of epoch:599 | time: 0.05s | train loss:0.19094 | valid loss:0.32402 | train MseLoss:0.13200 | train PccLoss:0.72144 | valid MseLoss:0.25521 | valid PccLoss:0.94337\n",
            "end of epoch:600 | time: 0.05s | train loss:0.19310 | valid loss:0.32997 | train MseLoss:0.13413 | train PccLoss:0.72385 | valid MseLoss:0.26413 | valid PccLoss:0.92248\n",
            "end of epoch:601 | time: 0.05s | train loss:0.19468 | valid loss:0.31691 | train MseLoss:0.13562 | train PccLoss:0.72623 | valid MseLoss:0.24567 | valid PccLoss:0.95809\n",
            "end of epoch:602 | time: 0.05s | train loss:0.19297 | valid loss:0.33178 | train MseLoss:0.13438 | train PccLoss:0.72026 | valid MseLoss:0.26343 | valid PccLoss:0.94697\n",
            "end of epoch:603 | time: 0.07s | train loss:0.19029 | valid loss:0.31481 | train MseLoss:0.13114 | train PccLoss:0.72262 | valid MseLoss:0.24695 | valid PccLoss:0.92556\n",
            "end of epoch:604 | time: 0.07s | train loss:0.19324 | valid loss:0.30808 | train MseLoss:0.13456 | train PccLoss:0.72138 | valid MseLoss:0.23991 | valid PccLoss:0.92163\n",
            "end of epoch:605 | time: 0.06s | train loss:0.19643 | valid loss:0.31790 | train MseLoss:0.13749 | train PccLoss:0.72691 | valid MseLoss:0.25231 | valid PccLoss:0.90819\n",
            "end of epoch:606 | time: 0.05s | train loss:0.19096 | valid loss:0.30662 | train MseLoss:0.13215 | train PccLoss:0.72021 | valid MseLoss:0.23702 | valid PccLoss:0.93299\n",
            "end of epoch:607 | time: 0.05s | train loss:0.19025 | valid loss:0.31343 | train MseLoss:0.13133 | train PccLoss:0.72049 | valid MseLoss:0.24110 | valid PccLoss:0.96442\n",
            "end of epoch:608 | time: 0.05s | train loss:0.19293 | valid loss:0.30094 | train MseLoss:0.13471 | train PccLoss:0.71693 | valid MseLoss:0.22842 | valid PccLoss:0.95363\n",
            "end of epoch:609 | time: 0.05s | train loss:0.19195 | valid loss:0.31882 | train MseLoss:0.13297 | train PccLoss:0.72274 | valid MseLoss:0.24765 | valid PccLoss:0.95942\n",
            "end of epoch:610 | time: 0.05s | train loss:0.19381 | valid loss:0.29400 | train MseLoss:0.13496 | train PccLoss:0.72341 | valid MseLoss:0.22389 | valid PccLoss:0.92498\n",
            "end of epoch:611 | time: 0.05s | train loss:0.18958 | valid loss:0.31246 | train MseLoss:0.13045 | train PccLoss:0.72178 | valid MseLoss:0.24289 | valid PccLoss:0.93861\n",
            "end of epoch:612 | time: 0.05s | train loss:0.19074 | valid loss:0.28866 | train MseLoss:0.13248 | train PccLoss:0.71512 | valid MseLoss:0.21520 | valid PccLoss:0.94980\n",
            "end of epoch:613 | time: 0.05s | train loss:0.19462 | valid loss:0.31544 | train MseLoss:0.13578 | train PccLoss:0.72423 | valid MseLoss:0.24541 | valid PccLoss:0.94570\n",
            "end of epoch:614 | time: 0.05s | train loss:0.19128 | valid loss:0.34488 | train MseLoss:0.13314 | train PccLoss:0.71455 | valid MseLoss:0.27419 | valid PccLoss:0.98107\n",
            "end of epoch:615 | time: 0.05s | train loss:0.19065 | valid loss:0.29384 | train MseLoss:0.13163 | train PccLoss:0.72182 | valid MseLoss:0.22015 | valid PccLoss:0.95706\n",
            "end of epoch:616 | time: 0.05s | train loss:0.19449 | valid loss:0.34183 | train MseLoss:0.13641 | train PccLoss:0.71720 | valid MseLoss:0.27095 | valid PccLoss:0.97970\n",
            "end of epoch:617 | time: 0.05s | train loss:0.18966 | valid loss:0.31125 | train MseLoss:0.13091 | train PccLoss:0.71845 | valid MseLoss:0.24173 | valid PccLoss:0.93700\n",
            "end of epoch:618 | time: 0.05s | train loss:0.19180 | valid loss:0.32410 | train MseLoss:0.13325 | train PccLoss:0.71881 | valid MseLoss:0.25496 | valid PccLoss:0.94631\n",
            "end of epoch:619 | time: 0.05s | train loss:0.19035 | valid loss:0.30019 | train MseLoss:0.13217 | train PccLoss:0.71399 | valid MseLoss:0.22872 | valid PccLoss:0.94348\n",
            "end of epoch:620 | time: 0.05s | train loss:0.19370 | valid loss:0.30780 | train MseLoss:0.13527 | train PccLoss:0.71958 | valid MseLoss:0.23686 | valid PccLoss:0.94622\n",
            "end of epoch:621 | time: 0.05s | train loss:0.19153 | valid loss:0.33180 | train MseLoss:0.13303 | train PccLoss:0.71805 | valid MseLoss:0.26470 | valid PccLoss:0.93569\n",
            "end of epoch:622 | time: 0.05s | train loss:0.18931 | valid loss:0.33120 | train MseLoss:0.13103 | train PccLoss:0.71380 | valid MseLoss:0.26125 | valid PccLoss:0.96074\n",
            "end of epoch:623 | time: 0.05s | train loss:0.19092 | valid loss:0.31985 | train MseLoss:0.13244 | train PccLoss:0.71727 | valid MseLoss:0.24720 | valid PccLoss:0.97373\n",
            "end of epoch:624 | time: 0.05s | train loss:0.19051 | valid loss:0.31570 | train MseLoss:0.13221 | train PccLoss:0.71528 | valid MseLoss:0.24341 | valid PccLoss:0.96635\n",
            "end of epoch:625 | time: 0.05s | train loss:0.19135 | valid loss:0.32826 | train MseLoss:0.13311 | train PccLoss:0.71557 | valid MseLoss:0.25839 | valid PccLoss:0.95710\n",
            "end of epoch:626 | time: 0.05s | train loss:0.19073 | valid loss:0.33041 | train MseLoss:0.13233 | train PccLoss:0.71634 | valid MseLoss:0.26212 | valid PccLoss:0.94506\n",
            "end of epoch:627 | time: 0.05s | train loss:0.19174 | valid loss:0.32138 | train MseLoss:0.13307 | train PccLoss:0.71977 | valid MseLoss:0.25003 | valid PccLoss:0.96352\n",
            "end of epoch:628 | time: 0.06s | train loss:0.18997 | valid loss:0.30689 | train MseLoss:0.13180 | train PccLoss:0.71349 | valid MseLoss:0.23502 | valid PccLoss:0.95370\n",
            "end of epoch:629 | time: 0.06s | train loss:0.19069 | valid loss:0.32376 | train MseLoss:0.13248 | train PccLoss:0.71457 | valid MseLoss:0.25378 | valid PccLoss:0.95360\n",
            "end of epoch:630 | time: 0.06s | train loss:0.19461 | valid loss:0.31246 | train MseLoss:0.13640 | train PccLoss:0.71844 | valid MseLoss:0.24607 | valid PccLoss:0.90996\n",
            "end of epoch:631 | time: 0.06s | train loss:0.19052 | valid loss:0.31639 | train MseLoss:0.13263 | train PccLoss:0.71151 | valid MseLoss:0.24736 | valid PccLoss:0.93758\n",
            "end of epoch:632 | time: 0.06s | train loss:0.18922 | valid loss:0.30847 | train MseLoss:0.13058 | train PccLoss:0.71698 | valid MseLoss:0.23839 | valid PccLoss:0.93915\n",
            "end of epoch:633 | time: 0.06s | train loss:0.19179 | valid loss:0.30244 | train MseLoss:0.13405 | train PccLoss:0.71145 | valid MseLoss:0.23497 | valid PccLoss:0.90966\n",
            "end of epoch:634 | time: 0.07s | train loss:0.19301 | valid loss:0.33376 | train MseLoss:0.13516 | train PccLoss:0.71366 | valid MseLoss:0.26694 | valid PccLoss:0.93515\n",
            "end of epoch:635 | time: 0.06s | train loss:0.19199 | valid loss:0.31317 | train MseLoss:0.13422 | train PccLoss:0.71193 | valid MseLoss:0.24186 | valid PccLoss:0.95493\n",
            "end of epoch:636 | time: 0.06s | train loss:0.18955 | valid loss:0.34448 | train MseLoss:0.13190 | train PccLoss:0.70842 | valid MseLoss:0.27667 | valid PccLoss:0.95476\n",
            "end of epoch:637 | time: 0.06s | train loss:0.18995 | valid loss:0.29606 | train MseLoss:0.13220 | train PccLoss:0.70964 | valid MseLoss:0.22412 | valid PccLoss:0.94352\n",
            "end of epoch:638 | time: 0.06s | train loss:0.19094 | valid loss:0.32038 | train MseLoss:0.13246 | train PccLoss:0.71726 | valid MseLoss:0.25137 | valid PccLoss:0.94151\n",
            "end of epoch:639 | time: 0.06s | train loss:0.18988 | valid loss:0.31363 | train MseLoss:0.13240 | train PccLoss:0.70719 | valid MseLoss:0.24454 | valid PccLoss:0.93538\n",
            "end of epoch:640 | time: 0.05s | train loss:0.18913 | valid loss:0.30596 | train MseLoss:0.13132 | train PccLoss:0.70948 | valid MseLoss:0.23288 | valid PccLoss:0.96369\n",
            "end of epoch:641 | time: 0.06s | train loss:0.19166 | valid loss:0.31444 | train MseLoss:0.13361 | train PccLoss:0.71410 | valid MseLoss:0.24375 | valid PccLoss:0.95063\n",
            "end of epoch:642 | time: 0.07s | train loss:0.19093 | valid loss:0.31074 | train MseLoss:0.13335 | train PccLoss:0.70916 | valid MseLoss:0.24084 | valid PccLoss:0.93981\n",
            "end of epoch:643 | time: 0.06s | train loss:0.18961 | valid loss:0.32427 | train MseLoss:0.13203 | train PccLoss:0.70778 | valid MseLoss:0.25272 | valid PccLoss:0.96817\n",
            "end of epoch:644 | time: 0.05s | train loss:0.19032 | valid loss:0.31377 | train MseLoss:0.13245 | train PccLoss:0.71114 | valid MseLoss:0.24653 | valid PccLoss:0.91891\n",
            "end of epoch:645 | time: 0.05s | train loss:0.19101 | valid loss:0.32442 | train MseLoss:0.13364 | train PccLoss:0.70729 | valid MseLoss:0.25695 | valid PccLoss:0.93166\n",
            "end of epoch:646 | time: 0.06s | train loss:0.18960 | valid loss:0.31141 | train MseLoss:0.13253 | train PccLoss:0.70318 | valid MseLoss:0.24322 | valid PccLoss:0.92506\n",
            "end of epoch:647 | time: 0.05s | train loss:0.19316 | valid loss:0.30294 | train MseLoss:0.13557 | train PccLoss:0.71144 | valid MseLoss:0.23560 | valid PccLoss:0.90896\n",
            "end of epoch:648 | time: 0.05s | train loss:0.18952 | valid loss:0.30797 | train MseLoss:0.13208 | train PccLoss:0.70645 | valid MseLoss:0.23655 | valid PccLoss:0.95071\n",
            "end of epoch:649 | time: 0.05s | train loss:0.19814 | valid loss:0.30776 | train MseLoss:0.14025 | train PccLoss:0.71921 | valid MseLoss:0.23566 | valid PccLoss:0.95669\n",
            "end of epoch:650 | time: 0.06s | train loss:0.18922 | valid loss:0.30495 | train MseLoss:0.13148 | train PccLoss:0.70880 | valid MseLoss:0.23351 | valid PccLoss:0.94795\n",
            "end of epoch:651 | time: 0.06s | train loss:0.19338 | valid loss:0.32022 | train MseLoss:0.13571 | train PccLoss:0.71241 | valid MseLoss:0.25014 | valid PccLoss:0.95088\n",
            "end of epoch:652 | time: 0.05s | train loss:0.18874 | valid loss:0.30405 | train MseLoss:0.13153 | train PccLoss:0.70368 | valid MseLoss:0.23199 | valid PccLoss:0.95256\n",
            "end of epoch:653 | time: 0.06s | train loss:0.18961 | valid loss:0.30789 | train MseLoss:0.13208 | train PccLoss:0.70743 | valid MseLoss:0.23809 | valid PccLoss:0.93609\n",
            "end of epoch:654 | time: 0.06s | train loss:0.19160 | valid loss:0.31623 | train MseLoss:0.13427 | train PccLoss:0.70754 | valid MseLoss:0.25031 | valid PccLoss:0.90951\n",
            "end of epoch:655 | time: 0.06s | train loss:0.19049 | valid loss:0.28656 | train MseLoss:0.13308 | train PccLoss:0.70719 | valid MseLoss:0.21439 | valid PccLoss:0.93612\n",
            "end of epoch:656 | time: 0.05s | train loss:0.19052 | valid loss:0.33046 | train MseLoss:0.13351 | train PccLoss:0.70355 | valid MseLoss:0.25826 | valid PccLoss:0.98025\n",
            "end of epoch:657 | time: 0.06s | train loss:0.19210 | valid loss:0.33153 | train MseLoss:0.13511 | train PccLoss:0.70497 | valid MseLoss:0.26128 | valid PccLoss:0.96381\n",
            "end of epoch:658 | time: 0.05s | train loss:0.19007 | valid loss:0.33066 | train MseLoss:0.13281 | train PccLoss:0.70547 | valid MseLoss:0.25958 | valid PccLoss:0.97040\n",
            "end of epoch:659 | time: 0.06s | train loss:0.18904 | valid loss:0.32167 | train MseLoss:0.13159 | train PccLoss:0.70609 | valid MseLoss:0.25406 | valid PccLoss:0.93018\n",
            "end of epoch:660 | time: 0.06s | train loss:0.19232 | valid loss:0.32079 | train MseLoss:0.13502 | train PccLoss:0.70799 | valid MseLoss:0.25349 | valid PccLoss:0.92657\n",
            "end of epoch:661 | time: 0.06s | train loss:0.18805 | valid loss:0.30852 | train MseLoss:0.13081 | train PccLoss:0.70321 | valid MseLoss:0.24048 | valid PccLoss:0.92091\n",
            "end of epoch:662 | time: 0.06s | train loss:0.18962 | valid loss:0.29978 | train MseLoss:0.13246 | train PccLoss:0.70413 | valid MseLoss:0.22896 | valid PccLoss:0.93710\n",
            "end of epoch:663 | time: 0.06s | train loss:0.19044 | valid loss:0.30841 | train MseLoss:0.13371 | train PccLoss:0.70106 | valid MseLoss:0.23954 | valid PccLoss:0.92824\n",
            "end of epoch:664 | time: 0.06s | train loss:0.19040 | valid loss:0.29877 | train MseLoss:0.13307 | train PccLoss:0.70633 | valid MseLoss:0.23156 | valid PccLoss:0.90369\n",
            "end of epoch:665 | time: 0.06s | train loss:0.19142 | valid loss:0.32288 | train MseLoss:0.13436 | train PccLoss:0.70497 | valid MseLoss:0.25576 | valid PccLoss:0.92702\n",
            "end of epoch:666 | time: 0.06s | train loss:0.18847 | valid loss:0.29445 | train MseLoss:0.13172 | train PccLoss:0.69915 | valid MseLoss:0.22443 | valid PccLoss:0.92461\n",
            "end of epoch:667 | time: 0.06s | train loss:0.19197 | valid loss:0.32562 | train MseLoss:0.13535 | train PccLoss:0.70155 | valid MseLoss:0.25362 | valid PccLoss:0.97369\n",
            "end of epoch:668 | time: 0.06s | train loss:0.18845 | valid loss:0.32421 | train MseLoss:0.13152 | train PccLoss:0.70074 | valid MseLoss:0.25618 | valid PccLoss:0.93645\n",
            "end of epoch:669 | time: 0.06s | train loss:0.18822 | valid loss:0.31500 | train MseLoss:0.13152 | train PccLoss:0.69852 | valid MseLoss:0.24453 | valid PccLoss:0.94925\n",
            "end of epoch:670 | time: 0.06s | train loss:0.18766 | valid loss:0.32537 | train MseLoss:0.13070 | train PccLoss:0.70029 | valid MseLoss:0.25739 | valid PccLoss:0.93717\n",
            "end of epoch:671 | time: 0.06s | train loss:0.19008 | valid loss:0.32493 | train MseLoss:0.13327 | train PccLoss:0.70137 | valid MseLoss:0.25511 | valid PccLoss:0.95339\n",
            "end of epoch:672 | time: 0.06s | train loss:0.18988 | valid loss:0.30107 | train MseLoss:0.13249 | train PccLoss:0.70645 | valid MseLoss:0.22971 | valid PccLoss:0.94337\n",
            "end of epoch:673 | time: 0.06s | train loss:0.18841 | valid loss:0.29841 | train MseLoss:0.13164 | train PccLoss:0.69935 | valid MseLoss:0.22750 | valid PccLoss:0.93659\n",
            "end of epoch:674 | time: 0.06s | train loss:0.19071 | valid loss:0.30849 | train MseLoss:0.13359 | train PccLoss:0.70483 | valid MseLoss:0.24142 | valid PccLoss:0.91213\n",
            "end of epoch:675 | time: 0.06s | train loss:0.18888 | valid loss:0.30883 | train MseLoss:0.13198 | train PccLoss:0.70098 | valid MseLoss:0.23772 | valid PccLoss:0.94884\n",
            "end of epoch:676 | time: 0.06s | train loss:0.18989 | valid loss:0.30538 | train MseLoss:0.13315 | train PccLoss:0.70060 | valid MseLoss:0.23722 | valid PccLoss:0.91880\n",
            "end of epoch:677 | time: 0.06s | train loss:0.18938 | valid loss:0.30163 | train MseLoss:0.13240 | train PccLoss:0.70215 | valid MseLoss:0.23477 | valid PccLoss:0.90333\n",
            "end of epoch:678 | time: 0.08s | train loss:0.18938 | valid loss:0.32385 | train MseLoss:0.13259 | train PccLoss:0.70048 | valid MseLoss:0.25370 | valid PccLoss:0.95517\n",
            "end of epoch:679 | time: 0.06s | train loss:0.19329 | valid loss:0.30928 | train MseLoss:0.13683 | train PccLoss:0.70149 | valid MseLoss:0.24391 | valid PccLoss:0.89764\n",
            "end of epoch:680 | time: 0.05s | train loss:0.18640 | valid loss:0.30002 | train MseLoss:0.12966 | train PccLoss:0.69706 | valid MseLoss:0.22709 | valid PccLoss:0.95642\n",
            "end of epoch:681 | time: 0.05s | train loss:0.18924 | valid loss:0.30867 | train MseLoss:0.13238 | train PccLoss:0.70097 | valid MseLoss:0.23826 | valid PccLoss:0.94241\n",
            "end of epoch:682 | time: 0.05s | train loss:0.18705 | valid loss:0.33269 | train MseLoss:0.13066 | train PccLoss:0.69455 | valid MseLoss:0.26326 | valid PccLoss:0.95756\n",
            "end of epoch:683 | time: 0.05s | train loss:0.18767 | valid loss:0.30519 | train MseLoss:0.13126 | train PccLoss:0.69533 | valid MseLoss:0.23319 | valid PccLoss:0.95324\n",
            "end of epoch:684 | time: 0.05s | train loss:0.18944 | valid loss:0.31737 | train MseLoss:0.13343 | train PccLoss:0.69346 | valid MseLoss:0.25018 | valid PccLoss:0.92210\n",
            "end of epoch:685 | time: 0.05s | train loss:0.18913 | valid loss:0.32172 | train MseLoss:0.13299 | train PccLoss:0.69437 | valid MseLoss:0.25106 | valid PccLoss:0.95765\n",
            "end of epoch:686 | time: 0.05s | train loss:0.18994 | valid loss:0.31765 | train MseLoss:0.13331 | train PccLoss:0.69959 | valid MseLoss:0.24696 | valid PccLoss:0.95378\n",
            "end of epoch:687 | time: 0.05s | train loss:0.18875 | valid loss:0.31246 | train MseLoss:0.13292 | train PccLoss:0.69123 | valid MseLoss:0.24206 | valid PccLoss:0.94612\n",
            "end of epoch:688 | time: 0.05s | train loss:0.18983 | valid loss:0.32014 | train MseLoss:0.13350 | train PccLoss:0.69684 | valid MseLoss:0.24913 | valid PccLoss:0.95923\n",
            "end of epoch:689 | time: 0.05s | train loss:0.19070 | valid loss:0.30956 | train MseLoss:0.13439 | train PccLoss:0.69748 | valid MseLoss:0.23992 | valid PccLoss:0.93623\n",
            "end of epoch:690 | time: 0.05s | train loss:0.19063 | valid loss:0.33098 | train MseLoss:0.13456 | train PccLoss:0.69528 | valid MseLoss:0.25974 | valid PccLoss:0.97218\n",
            "end of epoch:691 | time: 0.05s | train loss:0.19002 | valid loss:0.30597 | train MseLoss:0.13416 | train PccLoss:0.69277 | valid MseLoss:0.23598 | valid PccLoss:0.93591\n",
            "end of epoch:692 | time: 0.05s | train loss:0.18878 | valid loss:0.31499 | train MseLoss:0.13242 | train PccLoss:0.69599 | valid MseLoss:0.24584 | valid PccLoss:0.93736\n",
            "end of epoch:693 | time: 0.05s | train loss:0.19191 | valid loss:0.31082 | train MseLoss:0.13535 | train PccLoss:0.70091 | valid MseLoss:0.24059 | valid PccLoss:0.94292\n",
            "end of epoch:694 | time: 0.05s | train loss:0.18908 | valid loss:0.31527 | train MseLoss:0.13290 | train PccLoss:0.69474 | valid MseLoss:0.24172 | valid PccLoss:0.97728\n",
            "end of epoch:695 | time: 0.05s | train loss:0.18785 | valid loss:0.29882 | train MseLoss:0.13199 | train PccLoss:0.69064 | valid MseLoss:0.22919 | valid PccLoss:0.92543\n",
            "end of epoch:696 | time: 0.05s | train loss:0.18946 | valid loss:0.33362 | train MseLoss:0.13374 | train PccLoss:0.69096 | valid MseLoss:0.26457 | valid PccLoss:0.95504\n",
            "end of epoch:697 | time: 0.05s | train loss:0.18683 | valid loss:0.34951 | train MseLoss:0.13112 | train PccLoss:0.68827 | valid MseLoss:0.28015 | valid PccLoss:0.97375\n",
            "end of epoch:698 | time: 0.05s | train loss:0.18892 | valid loss:0.29941 | train MseLoss:0.13259 | train PccLoss:0.69590 | valid MseLoss:0.23069 | valid PccLoss:0.91787\n",
            "end of epoch:699 | time: 0.05s | train loss:0.18838 | valid loss:0.31391 | train MseLoss:0.13248 | train PccLoss:0.69150 | valid MseLoss:0.24752 | valid PccLoss:0.91137\n",
            "end of epoch:700 | time: 0.05s | train loss:0.18884 | valid loss:0.32979 | train MseLoss:0.13296 | train PccLoss:0.69168 | valid MseLoss:0.25913 | valid PccLoss:0.96576\n",
            "end of epoch:701 | time: 0.05s | train loss:0.18907 | valid loss:0.29726 | train MseLoss:0.13348 | train PccLoss:0.68941 | valid MseLoss:0.22601 | valid PccLoss:0.93855\n",
            "end of epoch:702 | time: 0.05s | train loss:0.19029 | valid loss:0.31561 | train MseLoss:0.13457 | train PccLoss:0.69175 | valid MseLoss:0.24431 | valid PccLoss:0.95733\n",
            "end of epoch:703 | time: 0.05s | train loss:0.18814 | valid loss:0.31065 | train MseLoss:0.13248 | train PccLoss:0.68910 | valid MseLoss:0.24027 | valid PccLoss:0.94411\n",
            "end of epoch:704 | time: 0.05s | train loss:0.18643 | valid loss:0.29400 | train MseLoss:0.13071 | train PccLoss:0.68791 | valid MseLoss:0.22596 | valid PccLoss:0.90640\n",
            "end of epoch:705 | time: 0.05s | train loss:0.18640 | valid loss:0.30271 | train MseLoss:0.13030 | train PccLoss:0.69129 | valid MseLoss:0.23251 | valid PccLoss:0.93449\n",
            "end of epoch:706 | time: 0.05s | train loss:0.18987 | valid loss:0.29596 | train MseLoss:0.13397 | train PccLoss:0.69298 | valid MseLoss:0.22385 | valid PccLoss:0.94489\n",
            "end of epoch:707 | time: 0.05s | train loss:0.18745 | valid loss:0.31537 | train MseLoss:0.13163 | train PccLoss:0.68976 | valid MseLoss:0.24832 | valid PccLoss:0.91889\n",
            "end of epoch:708 | time: 0.05s | train loss:0.18691 | valid loss:0.30579 | train MseLoss:0.13084 | train PccLoss:0.69146 | valid MseLoss:0.23719 | valid PccLoss:0.92326\n",
            "end of epoch:709 | time: 0.05s | train loss:0.19074 | valid loss:0.31652 | train MseLoss:0.13541 | train PccLoss:0.68877 | valid MseLoss:0.24736 | valid PccLoss:0.93894\n",
            "end of epoch:710 | time: 0.05s | train loss:0.18969 | valid loss:0.33253 | train MseLoss:0.13388 | train PccLoss:0.69205 | valid MseLoss:0.26457 | valid PccLoss:0.94419\n",
            "end of epoch:711 | time: 0.05s | train loss:0.18850 | valid loss:0.33255 | train MseLoss:0.13238 | train PccLoss:0.69354 | valid MseLoss:0.26456 | valid PccLoss:0.94445\n",
            "end of epoch:712 | time: 0.05s | train loss:0.18645 | valid loss:0.32465 | train MseLoss:0.13096 | train PccLoss:0.68587 | valid MseLoss:0.25631 | valid PccLoss:0.93973\n",
            "end of epoch:713 | time: 0.05s | train loss:0.18662 | valid loss:0.31374 | train MseLoss:0.13099 | train PccLoss:0.68736 | valid MseLoss:0.24071 | valid PccLoss:0.97104\n",
            "end of epoch:714 | time: 0.05s | train loss:0.18740 | valid loss:0.30746 | train MseLoss:0.13182 | train PccLoss:0.68758 | valid MseLoss:0.23940 | valid PccLoss:0.92007\n",
            "end of epoch:715 | time: 0.05s | train loss:0.18606 | valid loss:0.31338 | train MseLoss:0.13017 | train PccLoss:0.68903 | valid MseLoss:0.24151 | valid PccLoss:0.96029\n",
            "end of epoch:716 | time: 0.05s | train loss:0.18726 | valid loss:0.30376 | train MseLoss:0.13191 | train PccLoss:0.68539 | valid MseLoss:0.23288 | valid PccLoss:0.94165\n",
            "end of epoch:717 | time: 0.05s | train loss:0.18769 | valid loss:0.29053 | train MseLoss:0.13182 | train PccLoss:0.69055 | valid MseLoss:0.22155 | valid PccLoss:0.91133\n",
            "end of epoch:718 | time: 0.05s | train loss:0.19346 | valid loss:0.29232 | train MseLoss:0.13837 | train PccLoss:0.68924 | valid MseLoss:0.22206 | valid PccLoss:0.92463\n",
            "end of epoch:719 | time: 0.05s | train loss:0.18767 | valid loss:0.31360 | train MseLoss:0.13176 | train PccLoss:0.69087 | valid MseLoss:0.24523 | valid PccLoss:0.92894\n",
            "end of epoch:720 | time: 0.05s | train loss:0.18828 | valid loss:0.29869 | train MseLoss:0.13332 | train PccLoss:0.68295 | valid MseLoss:0.22671 | valid PccLoss:0.94653\n",
            "end of epoch:721 | time: 0.05s | train loss:0.18685 | valid loss:0.29099 | train MseLoss:0.13168 | train PccLoss:0.68332 | valid MseLoss:0.22035 | valid PccLoss:0.92680\n",
            "end of epoch:722 | time: 0.05s | train loss:0.18623 | valid loss:0.30073 | train MseLoss:0.13087 | train PccLoss:0.68447 | valid MseLoss:0.23055 | valid PccLoss:0.93233\n",
            "end of epoch:723 | time: 0.05s | train loss:0.18912 | valid loss:0.30662 | train MseLoss:0.13425 | train PccLoss:0.68294 | valid MseLoss:0.23707 | valid PccLoss:0.93258\n",
            "end of epoch:724 | time: 0.05s | train loss:0.18886 | valid loss:0.31047 | train MseLoss:0.13372 | train PccLoss:0.68512 | valid MseLoss:0.24143 | valid PccLoss:0.93184\n",
            "end of epoch:725 | time: 0.05s | train loss:0.18785 | valid loss:0.32149 | train MseLoss:0.13281 | train PccLoss:0.68324 | valid MseLoss:0.25329 | valid PccLoss:0.93534\n",
            "end of epoch:726 | time: 0.05s | train loss:0.18675 | valid loss:0.32020 | train MseLoss:0.13146 | train PccLoss:0.68438 | valid MseLoss:0.25254 | valid PccLoss:0.92917\n",
            "end of epoch:727 | time: 0.05s | train loss:0.18553 | valid loss:0.32995 | train MseLoss:0.13037 | train PccLoss:0.68191 | valid MseLoss:0.26071 | valid PccLoss:0.95308\n",
            "end of epoch:728 | time: 0.05s | train loss:0.18646 | valid loss:0.32557 | train MseLoss:0.13085 | train PccLoss:0.68692 | valid MseLoss:0.25554 | valid PccLoss:0.95585\n",
            "end of epoch:729 | time: 0.05s | train loss:0.18735 | valid loss:0.31658 | train MseLoss:0.13212 | train PccLoss:0.68442 | valid MseLoss:0.24550 | valid PccLoss:0.95632\n",
            "end of epoch:730 | time: 0.05s | train loss:0.18805 | valid loss:0.29998 | train MseLoss:0.13287 | train PccLoss:0.68474 | valid MseLoss:0.22783 | valid PccLoss:0.94941\n",
            "end of epoch:731 | time: 0.05s | train loss:0.18768 | valid loss:0.30694 | train MseLoss:0.13249 | train PccLoss:0.68440 | valid MseLoss:0.23699 | valid PccLoss:0.93652\n",
            "end of epoch:732 | time: 0.05s | train loss:0.18982 | valid loss:0.31412 | train MseLoss:0.13464 | train PccLoss:0.68639 | valid MseLoss:0.24256 | valid PccLoss:0.95814\n",
            "end of epoch:733 | time: 0.05s | train loss:0.19274 | valid loss:0.32781 | train MseLoss:0.13768 | train PccLoss:0.68822 | valid MseLoss:0.26247 | valid PccLoss:0.91583\n",
            "end of epoch:734 | time: 0.05s | train loss:0.18564 | valid loss:0.32028 | train MseLoss:0.13012 | train PccLoss:0.68526 | valid MseLoss:0.25407 | valid PccLoss:0.91617\n",
            "end of epoch:735 | time: 0.05s | train loss:0.18768 | valid loss:0.31524 | train MseLoss:0.13317 | train PccLoss:0.67833 | valid MseLoss:0.24374 | valid PccLoss:0.95875\n",
            "end of epoch:736 | time: 0.05s | train loss:0.18693 | valid loss:0.31695 | train MseLoss:0.13261 | train PccLoss:0.67581 | valid MseLoss:0.24637 | valid PccLoss:0.95221\n",
            "end of epoch:737 | time: 0.05s | train loss:0.18689 | valid loss:0.29394 | train MseLoss:0.13203 | train PccLoss:0.68065 | valid MseLoss:0.22480 | valid PccLoss:0.91623\n",
            "end of epoch:738 | time: 0.05s | train loss:0.18794 | valid loss:0.33226 | train MseLoss:0.13348 | train PccLoss:0.67812 | valid MseLoss:0.26613 | valid PccLoss:0.92744\n",
            "end of epoch:739 | time: 0.05s | train loss:0.18824 | valid loss:0.32107 | train MseLoss:0.13329 | train PccLoss:0.68276 | valid MseLoss:0.25251 | valid PccLoss:0.93810\n",
            "end of epoch:740 | time: 0.05s | train loss:0.18824 | valid loss:0.31541 | train MseLoss:0.13381 | train PccLoss:0.67810 | valid MseLoss:0.24527 | valid PccLoss:0.94666\n",
            "end of epoch:741 | time: 0.05s | train loss:0.18827 | valid loss:0.30844 | train MseLoss:0.13356 | train PccLoss:0.68069 | valid MseLoss:0.23920 | valid PccLoss:0.93164\n",
            "end of epoch:742 | time: 0.05s | train loss:0.18868 | valid loss:0.30568 | train MseLoss:0.13407 | train PccLoss:0.68020 | valid MseLoss:0.23500 | valid PccLoss:0.94177\n",
            "end of epoch:743 | time: 0.05s | train loss:0.18790 | valid loss:0.29820 | train MseLoss:0.13324 | train PccLoss:0.67984 | valid MseLoss:0.22543 | valid PccLoss:0.95311\n",
            "end of epoch:744 | time: 0.05s | train loss:0.18628 | valid loss:0.32454 | train MseLoss:0.13139 | train PccLoss:0.68031 | valid MseLoss:0.25786 | valid PccLoss:0.92461\n",
            "end of epoch:745 | time: 0.05s | train loss:0.18768 | valid loss:0.32104 | train MseLoss:0.13303 | train PccLoss:0.67953 | valid MseLoss:0.25292 | valid PccLoss:0.93405\n",
            "end of epoch:746 | time: 0.05s | train loss:0.18425 | valid loss:0.33617 | train MseLoss:0.12944 | train PccLoss:0.67751 | valid MseLoss:0.26556 | valid PccLoss:0.97164\n",
            "end of epoch:747 | time: 0.05s | train loss:0.18728 | valid loss:0.31806 | train MseLoss:0.13273 | train PccLoss:0.67830 | valid MseLoss:0.25010 | valid PccLoss:0.92972\n",
            "end of epoch:748 | time: 0.05s | train loss:0.18622 | valid loss:0.31179 | train MseLoss:0.13165 | train PccLoss:0.67731 | valid MseLoss:0.24027 | valid PccLoss:0.95549\n",
            "end of epoch:749 | time: 0.05s | train loss:0.18376 | valid loss:0.30019 | train MseLoss:0.12898 | train PccLoss:0.67675 | valid MseLoss:0.22945 | valid PccLoss:0.93682\n",
            "end of epoch:750 | time: 0.05s | train loss:0.18730 | valid loss:0.32953 | train MseLoss:0.13308 | train PccLoss:0.67534 | valid MseLoss:0.26077 | valid PccLoss:0.94830\n",
            "end of epoch:751 | time: 0.05s | train loss:0.18723 | valid loss:0.29468 | train MseLoss:0.13234 | train PccLoss:0.68117 | valid MseLoss:0.22427 | valid PccLoss:0.92831\n",
            "end of epoch:752 | time: 0.05s | train loss:0.18629 | valid loss:0.30098 | train MseLoss:0.13133 | train PccLoss:0.68101 | valid MseLoss:0.23242 | valid PccLoss:0.91801\n",
            "end of epoch:753 | time: 0.05s | train loss:0.18584 | valid loss:0.30989 | train MseLoss:0.13140 | train PccLoss:0.67577 | valid MseLoss:0.24175 | valid PccLoss:0.92312\n",
            "end of epoch:754 | time: 0.05s | train loss:0.18739 | valid loss:0.30166 | train MseLoss:0.13323 | train PccLoss:0.67485 | valid MseLoss:0.22978 | valid PccLoss:0.94852\n",
            "end of epoch:755 | time: 0.05s | train loss:0.18703 | valid loss:0.33511 | train MseLoss:0.13268 | train PccLoss:0.67620 | valid MseLoss:0.26732 | valid PccLoss:0.94526\n",
            "end of epoch:756 | time: 0.05s | train loss:0.18658 | valid loss:0.32174 | train MseLoss:0.13264 | train PccLoss:0.67205 | valid MseLoss:0.25127 | valid PccLoss:0.95605\n",
            "end of epoch:757 | time: 0.05s | train loss:0.18815 | valid loss:0.30684 | train MseLoss:0.13391 | train PccLoss:0.67627 | valid MseLoss:0.23733 | valid PccLoss:0.93245\n",
            "end of epoch:758 | time: 0.05s | train loss:0.18646 | valid loss:0.29383 | train MseLoss:0.13232 | train PccLoss:0.67371 | valid MseLoss:0.22441 | valid PccLoss:0.91868\n",
            "end of epoch:759 | time: 0.05s | train loss:0.18908 | valid loss:0.31063 | train MseLoss:0.13440 | train PccLoss:0.68129 | valid MseLoss:0.23928 | valid PccLoss:0.95275\n",
            "end of epoch:760 | time: 0.05s | train loss:0.18487 | valid loss:0.31360 | train MseLoss:0.13074 | train PccLoss:0.67206 | valid MseLoss:0.24585 | valid PccLoss:0.92343\n",
            "end of epoch:761 | time: 0.05s | train loss:0.18571 | valid loss:0.30090 | train MseLoss:0.13106 | train PccLoss:0.67749 | valid MseLoss:0.23103 | valid PccLoss:0.92980\n",
            "end of epoch:762 | time: 0.06s | train loss:0.19011 | valid loss:0.29552 | train MseLoss:0.13618 | train PccLoss:0.67549 | valid MseLoss:0.22880 | valid PccLoss:0.89607\n",
            "end of epoch:763 | time: 0.05s | train loss:0.18922 | valid loss:0.33783 | train MseLoss:0.13429 | train PccLoss:0.68352 | valid MseLoss:0.27178 | valid PccLoss:0.93226\n",
            "end of epoch:764 | time: 0.05s | train loss:0.18498 | valid loss:0.31975 | train MseLoss:0.13070 | train PccLoss:0.67354 | valid MseLoss:0.25120 | valid PccLoss:0.93674\n",
            "end of epoch:765 | time: 0.05s | train loss:0.18456 | valid loss:0.33580 | train MseLoss:0.13038 | train PccLoss:0.67218 | valid MseLoss:0.26772 | valid PccLoss:0.94849\n",
            "end of epoch:766 | time: 0.05s | train loss:0.18584 | valid loss:0.33458 | train MseLoss:0.13206 | train PccLoss:0.66988 | valid MseLoss:0.26501 | valid PccLoss:0.96066\n",
            "end of epoch:767 | time: 0.05s | train loss:0.18592 | valid loss:0.31575 | train MseLoss:0.13170 | train PccLoss:0.67398 | valid MseLoss:0.25071 | valid PccLoss:0.90105\n",
            "end of epoch:768 | time: 0.05s | train loss:0.18447 | valid loss:0.29737 | train MseLoss:0.13037 | train PccLoss:0.67141 | valid MseLoss:0.22902 | valid PccLoss:0.91255\n",
            "end of epoch:769 | time: 0.05s | train loss:0.18628 | valid loss:0.31698 | train MseLoss:0.13203 | train PccLoss:0.67454 | valid MseLoss:0.24472 | valid PccLoss:0.96730\n",
            "end of epoch:770 | time: 0.05s | train loss:0.18523 | valid loss:0.29130 | train MseLoss:0.13115 | train PccLoss:0.67190 | valid MseLoss:0.21880 | valid PccLoss:0.94377\n",
            "end of epoch:771 | time: 0.05s | train loss:0.18667 | valid loss:0.30105 | train MseLoss:0.13298 | train PccLoss:0.66992 | valid MseLoss:0.23001 | valid PccLoss:0.94043\n",
            "end of epoch:772 | time: 0.05s | train loss:0.18494 | valid loss:0.30680 | train MseLoss:0.13081 | train PccLoss:0.67212 | valid MseLoss:0.23577 | valid PccLoss:0.94604\n",
            "end of epoch:773 | time: 0.05s | train loss:0.18665 | valid loss:0.31832 | train MseLoss:0.13283 | train PccLoss:0.67102 | valid MseLoss:0.25152 | valid PccLoss:0.91960\n",
            "end of epoch:774 | time: 0.05s | train loss:0.18638 | valid loss:0.31028 | train MseLoss:0.13229 | train PccLoss:0.67317 | valid MseLoss:0.23971 | valid PccLoss:0.94535\n",
            "end of epoch:775 | time: 0.05s | train loss:0.18293 | valid loss:0.34065 | train MseLoss:0.12918 | train PccLoss:0.66666 | valid MseLoss:0.27170 | valid PccLoss:0.96123\n",
            "end of epoch:776 | time: 0.05s | train loss:0.18504 | valid loss:0.32710 | train MseLoss:0.13071 | train PccLoss:0.67401 | valid MseLoss:0.26008 | valid PccLoss:0.93028\n",
            "end of epoch:777 | time: 0.05s | train loss:0.18484 | valid loss:0.30791 | train MseLoss:0.13078 | train PccLoss:0.67135 | valid MseLoss:0.23824 | valid PccLoss:0.93492\n",
            "end of epoch:778 | time: 0.05s | train loss:0.18569 | valid loss:0.33247 | train MseLoss:0.13144 | train PccLoss:0.67391 | valid MseLoss:0.26391 | valid PccLoss:0.94950\n",
            "end of epoch:779 | time: 0.05s | train loss:0.18525 | valid loss:0.30715 | train MseLoss:0.13176 | train PccLoss:0.66662 | valid MseLoss:0.23754 | valid PccLoss:0.93361\n",
            "end of epoch:780 | time: 0.05s | train loss:0.18747 | valid loss:0.32320 | train MseLoss:0.13360 | train PccLoss:0.67224 | valid MseLoss:0.25566 | valid PccLoss:0.93103\n",
            "end of epoch:781 | time: 0.05s | train loss:0.18519 | valid loss:0.29908 | train MseLoss:0.13137 | train PccLoss:0.66958 | valid MseLoss:0.22552 | valid PccLoss:0.96109\n",
            "end of epoch:782 | time: 0.05s | train loss:0.18480 | valid loss:0.32373 | train MseLoss:0.13099 | train PccLoss:0.66910 | valid MseLoss:0.25849 | valid PccLoss:0.91097\n",
            "end of epoch:783 | time: 0.06s | train loss:0.19097 | valid loss:0.30407 | train MseLoss:0.13778 | train PccLoss:0.66965 | valid MseLoss:0.23361 | valid PccLoss:0.93823\n",
            "end of epoch:784 | time: 0.05s | train loss:0.18439 | valid loss:0.32798 | train MseLoss:0.13067 | train PccLoss:0.66786 | valid MseLoss:0.25660 | valid PccLoss:0.97036\n",
            "end of epoch:785 | time: 0.05s | train loss:0.18534 | valid loss:0.31204 | train MseLoss:0.13189 | train PccLoss:0.66634 | valid MseLoss:0.24148 | valid PccLoss:0.94712\n",
            "end of epoch:786 | time: 0.05s | train loss:0.18778 | valid loss:0.31191 | train MseLoss:0.13445 | train PccLoss:0.66779 | valid MseLoss:0.24406 | valid PccLoss:0.92247\n",
            "end of epoch:787 | time: 0.05s | train loss:0.18555 | valid loss:0.32488 | train MseLoss:0.13204 | train PccLoss:0.66712 | valid MseLoss:0.25915 | valid PccLoss:0.91643\n",
            "end of epoch:788 | time: 0.05s | train loss:0.18356 | valid loss:0.32672 | train MseLoss:0.12970 | train PccLoss:0.66831 | valid MseLoss:0.25696 | valid PccLoss:0.95459\n",
            "end of epoch:789 | time: 0.05s | train loss:0.18403 | valid loss:0.32599 | train MseLoss:0.13058 | train PccLoss:0.66507 | valid MseLoss:0.25925 | valid PccLoss:0.92669\n",
            "end of epoch:790 | time: 0.05s | train loss:0.18531 | valid loss:0.31828 | train MseLoss:0.13172 | train PccLoss:0.66761 | valid MseLoss:0.25141 | valid PccLoss:0.92014\n",
            "end of epoch:791 | time: 0.05s | train loss:0.18612 | valid loss:0.30079 | train MseLoss:0.13218 | train PccLoss:0.67156 | valid MseLoss:0.23174 | valid PccLoss:0.92224\n",
            "end of epoch:792 | time: 0.05s | train loss:0.18652 | valid loss:0.31931 | train MseLoss:0.13358 | train PccLoss:0.66298 | valid MseLoss:0.25092 | valid PccLoss:0.93491\n",
            "end of epoch:793 | time: 0.05s | train loss:0.18525 | valid loss:0.31905 | train MseLoss:0.13204 | train PccLoss:0.66416 | valid MseLoss:0.24994 | valid PccLoss:0.94105\n",
            "end of epoch:794 | time: 0.05s | train loss:0.18778 | valid loss:0.30428 | train MseLoss:0.13401 | train PccLoss:0.67169 | valid MseLoss:0.23504 | valid PccLoss:0.92752\n",
            "end of epoch:795 | time: 0.05s | train loss:0.19193 | valid loss:0.31445 | train MseLoss:0.13827 | train PccLoss:0.67488 | valid MseLoss:0.24567 | valid PccLoss:0.93349\n",
            "end of epoch:796 | time: 0.05s | train loss:0.18708 | valid loss:0.33617 | train MseLoss:0.13377 | train PccLoss:0.66696 | valid MseLoss:0.26728 | valid PccLoss:0.95616\n",
            "end of epoch:797 | time: 0.07s | train loss:0.18430 | valid loss:0.32320 | train MseLoss:0.13078 | train PccLoss:0.66602 | valid MseLoss:0.25489 | valid PccLoss:0.93804\n",
            "end of epoch:798 | time: 0.07s | train loss:0.18609 | valid loss:0.32023 | train MseLoss:0.13252 | train PccLoss:0.66824 | valid MseLoss:0.25064 | valid PccLoss:0.94658\n",
            "end of epoch:799 | time: 0.05s | train loss:0.18486 | valid loss:0.28403 | train MseLoss:0.13163 | train PccLoss:0.66393 | valid MseLoss:0.21222 | valid PccLoss:0.93033\n",
            "end of epoch:800 | time: 0.05s | train loss:0.18267 | valid loss:0.30661 | train MseLoss:0.12921 | train PccLoss:0.66377 | valid MseLoss:0.23765 | valid PccLoss:0.92719\n",
            "end of epoch:801 | time: 0.05s | train loss:0.18438 | valid loss:0.32974 | train MseLoss:0.13108 | train PccLoss:0.66406 | valid MseLoss:0.26067 | valid PccLoss:0.95138\n",
            "end of epoch:802 | time: 0.06s | train loss:0.18383 | valid loss:0.29162 | train MseLoss:0.13058 | train PccLoss:0.66309 | valid MseLoss:0.22029 | valid PccLoss:0.93359\n",
            "end of epoch:803 | time: 0.05s | train loss:0.18432 | valid loss:0.31259 | train MseLoss:0.13103 | train PccLoss:0.66394 | valid MseLoss:0.24411 | valid PccLoss:0.92882\n",
            "end of epoch:804 | time: 0.05s | train loss:0.18326 | valid loss:0.31330 | train MseLoss:0.12992 | train PccLoss:0.66336 | valid MseLoss:0.24638 | valid PccLoss:0.91565\n",
            "end of epoch:805 | time: 0.05s | train loss:0.18427 | valid loss:0.30700 | train MseLoss:0.13099 | train PccLoss:0.66375 | valid MseLoss:0.23981 | valid PccLoss:0.91176\n",
            "end of epoch:806 | time: 0.05s | train loss:0.18388 | valid loss:0.31317 | train MseLoss:0.13094 | train PccLoss:0.66037 | valid MseLoss:0.24186 | valid PccLoss:0.95498\n",
            "end of epoch:807 | time: 0.05s | train loss:0.18216 | valid loss:0.32658 | train MseLoss:0.12889 | train PccLoss:0.66157 | valid MseLoss:0.25729 | valid PccLoss:0.95021\n",
            "end of epoch:808 | time: 0.05s | train loss:0.18818 | valid loss:0.30440 | train MseLoss:0.13466 | train PccLoss:0.66984 | valid MseLoss:0.23211 | valid PccLoss:0.95502\n",
            "end of epoch:809 | time: 0.05s | train loss:0.18534 | valid loss:0.33068 | train MseLoss:0.13220 | train PccLoss:0.66354 | valid MseLoss:0.26366 | valid PccLoss:0.93386\n",
            "end of epoch:810 | time: 0.05s | train loss:0.18482 | valid loss:0.30987 | train MseLoss:0.13186 | train PccLoss:0.66139 | valid MseLoss:0.24074 | valid PccLoss:0.93209\n",
            "end of epoch:811 | time: 0.05s | train loss:0.18231 | valid loss:0.30924 | train MseLoss:0.12936 | train PccLoss:0.65889 | valid MseLoss:0.23731 | valid PccLoss:0.95664\n",
            "end of epoch:812 | time: 0.05s | train loss:0.18367 | valid loss:0.29981 | train MseLoss:0.13048 | train PccLoss:0.66243 | valid MseLoss:0.22691 | valid PccLoss:0.95585\n",
            "end of epoch:813 | time: 0.05s | train loss:0.18182 | valid loss:0.31550 | train MseLoss:0.12908 | train PccLoss:0.65648 | valid MseLoss:0.24828 | valid PccLoss:0.92049\n",
            "end of epoch:814 | time: 0.05s | train loss:0.18365 | valid loss:0.31207 | train MseLoss:0.13111 | train PccLoss:0.65648 | valid MseLoss:0.24031 | valid PccLoss:0.95784\n",
            "end of epoch:815 | time: 0.05s | train loss:0.18502 | valid loss:0.32929 | train MseLoss:0.13219 | train PccLoss:0.66044 | valid MseLoss:0.26178 | valid PccLoss:0.93686\n",
            "end of epoch:816 | time: 0.05s | train loss:0.18649 | valid loss:0.31901 | train MseLoss:0.13316 | train PccLoss:0.66647 | valid MseLoss:0.24985 | valid PccLoss:0.94153\n",
            "end of epoch:817 | time: 0.05s | train loss:0.18507 | valid loss:0.32512 | train MseLoss:0.13221 | train PccLoss:0.66082 | valid MseLoss:0.25843 | valid PccLoss:0.92535\n",
            "end of epoch:818 | time: 0.05s | train loss:0.18384 | valid loss:0.31931 | train MseLoss:0.13133 | train PccLoss:0.65641 | valid MseLoss:0.24866 | valid PccLoss:0.95517\n",
            "end of epoch:819 | time: 0.05s | train loss:0.18140 | valid loss:0.29439 | train MseLoss:0.12864 | train PccLoss:0.65628 | valid MseLoss:0.22711 | valid PccLoss:0.89986\n",
            "end of epoch:820 | time: 0.05s | train loss:0.18648 | valid loss:0.31157 | train MseLoss:0.13418 | train PccLoss:0.65720 | valid MseLoss:0.24550 | valid PccLoss:0.90619\n",
            "end of epoch:821 | time: 0.05s | train loss:0.18431 | valid loss:0.33188 | train MseLoss:0.13175 | train PccLoss:0.65736 | valid MseLoss:0.26443 | valid PccLoss:0.93891\n",
            "end of epoch:822 | time: 0.05s | train loss:0.18331 | valid loss:0.30707 | train MseLoss:0.13044 | train PccLoss:0.65920 | valid MseLoss:0.23726 | valid PccLoss:0.93534\n",
            "end of epoch:823 | time: 0.05s | train loss:0.18495 | valid loss:0.30405 | train MseLoss:0.13226 | train PccLoss:0.65918 | valid MseLoss:0.23597 | valid PccLoss:0.91676\n",
            "end of epoch:824 | time: 0.05s | train loss:0.18561 | valid loss:0.32104 | train MseLoss:0.13277 | train PccLoss:0.66110 | valid MseLoss:0.25093 | valid PccLoss:0.95206\n",
            "end of epoch:825 | time: 0.05s | train loss:0.18377 | valid loss:0.31697 | train MseLoss:0.13090 | train PccLoss:0.65964 | valid MseLoss:0.24377 | valid PccLoss:0.97571\n",
            "end of epoch:826 | time: 0.05s | train loss:0.18406 | valid loss:0.31030 | train MseLoss:0.13197 | train PccLoss:0.65286 | valid MseLoss:0.24300 | valid PccLoss:0.91602\n",
            "end of epoch:827 | time: 0.05s | train loss:0.18575 | valid loss:0.31065 | train MseLoss:0.13311 | train PccLoss:0.65954 | valid MseLoss:0.23979 | valid PccLoss:0.94843\n",
            "end of epoch:828 | time: 0.05s | train loss:0.18500 | valid loss:0.32018 | train MseLoss:0.13229 | train PccLoss:0.65936 | valid MseLoss:0.24918 | valid PccLoss:0.95917\n",
            "end of epoch:829 | time: 0.05s | train loss:0.18402 | valid loss:0.30004 | train MseLoss:0.13187 | train PccLoss:0.65339 | valid MseLoss:0.23059 | valid PccLoss:0.92504\n",
            "end of epoch:830 | time: 0.05s | train loss:0.18201 | valid loss:0.29664 | train MseLoss:0.12919 | train PccLoss:0.65736 | valid MseLoss:0.22353 | valid PccLoss:0.95463\n",
            "end of epoch:831 | time: 0.05s | train loss:0.18463 | valid loss:0.31620 | train MseLoss:0.13272 | train PccLoss:0.65186 | valid MseLoss:0.24556 | valid PccLoss:0.95198\n",
            "end of epoch:832 | time: 0.05s | train loss:0.18456 | valid loss:0.31210 | train MseLoss:0.13223 | train PccLoss:0.65553 | valid MseLoss:0.24224 | valid PccLoss:0.94084\n",
            "end of epoch:833 | time: 0.05s | train loss:0.18492 | valid loss:0.31288 | train MseLoss:0.13283 | train PccLoss:0.65378 | valid MseLoss:0.24589 | valid PccLoss:0.91583\n",
            "end of epoch:834 | time: 0.05s | train loss:0.18628 | valid loss:0.31494 | train MseLoss:0.13430 | train PccLoss:0.65406 | valid MseLoss:0.24556 | valid PccLoss:0.93940\n",
            "end of epoch:835 | time: 0.05s | train loss:0.18169 | valid loss:0.34664 | train MseLoss:0.12968 | train PccLoss:0.64976 | valid MseLoss:0.27758 | valid PccLoss:0.96810\n",
            "end of epoch:836 | time: 0.05s | train loss:0.18183 | valid loss:0.30425 | train MseLoss:0.12940 | train PccLoss:0.65368 | valid MseLoss:0.23584 | valid PccLoss:0.91987\n",
            "end of epoch:837 | time: 0.05s | train loss:0.18512 | valid loss:0.29571 | train MseLoss:0.13250 | train PccLoss:0.65871 | valid MseLoss:0.22748 | valid PccLoss:0.90973\n",
            "end of epoch:838 | time: 0.05s | train loss:0.18324 | valid loss:0.30997 | train MseLoss:0.13104 | train PccLoss:0.65304 | valid MseLoss:0.24297 | valid PccLoss:0.91293\n",
            "end of epoch:839 | time: 0.06s | train loss:0.18503 | valid loss:0.29949 | train MseLoss:0.13294 | train PccLoss:0.65377 | valid MseLoss:0.23025 | valid PccLoss:0.92264\n",
            "end of epoch:840 | time: 0.05s | train loss:0.18477 | valid loss:0.29859 | train MseLoss:0.13248 | train PccLoss:0.65539 | valid MseLoss:0.22641 | valid PccLoss:0.94816\n",
            "end of epoch:841 | time: 0.05s | train loss:0.18159 | valid loss:0.31709 | train MseLoss:0.12955 | train PccLoss:0.65002 | valid MseLoss:0.24867 | valid PccLoss:0.93290\n",
            "end of epoch:842 | time: 0.05s | train loss:0.18330 | valid loss:0.31113 | train MseLoss:0.13127 | train PccLoss:0.65164 | valid MseLoss:0.24069 | valid PccLoss:0.94510\n",
            "end of epoch:843 | time: 0.05s | train loss:0.18482 | valid loss:0.31553 | train MseLoss:0.13208 | train PccLoss:0.65948 | valid MseLoss:0.25207 | valid PccLoss:0.88665\n",
            "end of epoch:844 | time: 0.06s | train loss:0.18189 | valid loss:0.33785 | train MseLoss:0.12939 | train PccLoss:0.65437 | valid MseLoss:0.26976 | valid PccLoss:0.95069\n",
            "end of epoch:845 | time: 0.05s | train loss:0.18560 | valid loss:0.32004 | train MseLoss:0.13333 | train PccLoss:0.65611 | valid MseLoss:0.25102 | valid PccLoss:0.94127\n",
            "end of epoch:846 | time: 0.05s | train loss:0.18331 | valid loss:0.31488 | train MseLoss:0.13118 | train PccLoss:0.65245 | valid MseLoss:0.24720 | valid PccLoss:0.92407\n",
            "end of epoch:847 | time: 0.05s | train loss:0.18404 | valid loss:0.33038 | train MseLoss:0.13207 | train PccLoss:0.65186 | valid MseLoss:0.26245 | valid PccLoss:0.94173\n",
            "end of epoch:848 | time: 0.05s | train loss:0.18149 | valid loss:0.30409 | train MseLoss:0.12943 | train PccLoss:0.65004 | valid MseLoss:0.23531 | valid PccLoss:0.92306\n",
            "end of epoch:849 | time: 0.05s | train loss:0.18304 | valid loss:0.30289 | train MseLoss:0.13057 | train PccLoss:0.65527 | valid MseLoss:0.23381 | valid PccLoss:0.92455\n",
            "end of epoch:850 | time: 0.05s | train loss:0.18164 | valid loss:0.29502 | train MseLoss:0.12920 | train PccLoss:0.65360 | valid MseLoss:0.22261 | valid PccLoss:0.94670\n",
            "end of epoch:851 | time: 0.05s | train loss:0.18556 | valid loss:0.32569 | train MseLoss:0.13352 | train PccLoss:0.65388 | valid MseLoss:0.25582 | valid PccLoss:0.95445\n",
            "end of epoch:852 | time: 0.05s | train loss:0.18344 | valid loss:0.29487 | train MseLoss:0.13150 | train PccLoss:0.65091 | valid MseLoss:0.22622 | valid PccLoss:0.91281\n",
            "end of epoch:853 | time: 0.05s | train loss:0.18362 | valid loss:0.30085 | train MseLoss:0.13145 | train PccLoss:0.65313 | valid MseLoss:0.22976 | valid PccLoss:0.94071\n",
            "end of epoch:854 | time: 0.05s | train loss:0.18563 | valid loss:0.30894 | train MseLoss:0.13439 | train PccLoss:0.64683 | valid MseLoss:0.23833 | valid PccLoss:0.94447\n",
            "end of epoch:855 | time: 0.05s | train loss:0.18250 | valid loss:0.31739 | train MseLoss:0.13064 | train PccLoss:0.64918 | valid MseLoss:0.25348 | valid PccLoss:0.89253\n",
            "end of epoch:856 | time: 0.05s | train loss:0.18068 | valid loss:0.33472 | train MseLoss:0.12804 | train PccLoss:0.65445 | valid MseLoss:0.26696 | valid PccLoss:0.94461\n",
            "end of epoch:857 | time: 0.05s | train loss:0.18431 | valid loss:0.33431 | train MseLoss:0.13208 | train PccLoss:0.65439 | valid MseLoss:0.26775 | valid PccLoss:0.93333\n",
            "end of epoch:858 | time: 0.05s | train loss:0.18307 | valid loss:0.31476 | train MseLoss:0.13135 | train PccLoss:0.64857 | valid MseLoss:0.24438 | valid PccLoss:0.94821\n",
            "end of epoch:859 | time: 0.05s | train loss:0.18248 | valid loss:0.30996 | train MseLoss:0.13068 | train PccLoss:0.64863 | valid MseLoss:0.24461 | valid PccLoss:0.89807\n",
            "end of epoch:860 | time: 0.05s | train loss:0.18221 | valid loss:0.32670 | train MseLoss:0.13016 | train PccLoss:0.65062 | valid MseLoss:0.25967 | valid PccLoss:0.92995\n",
            "end of epoch:861 | time: 0.05s | train loss:0.18313 | valid loss:0.33189 | train MseLoss:0.13140 | train PccLoss:0.64863 | valid MseLoss:0.26624 | valid PccLoss:0.92274\n",
            "end of epoch:862 | time: 0.05s | train loss:0.18096 | valid loss:0.32157 | train MseLoss:0.12891 | train PccLoss:0.64934 | valid MseLoss:0.25041 | valid PccLoss:0.96204\n",
            "end of epoch:863 | time: 0.05s | train loss:0.18240 | valid loss:0.31221 | train MseLoss:0.13071 | train PccLoss:0.64761 | valid MseLoss:0.24360 | valid PccLoss:0.92969\n",
            "end of epoch:864 | time: 0.05s | train loss:0.18132 | valid loss:0.32247 | train MseLoss:0.12985 | train PccLoss:0.64459 | valid MseLoss:0.25424 | valid PccLoss:0.93652\n",
            "end of epoch:865 | time: 0.05s | train loss:0.18325 | valid loss:0.33442 | train MseLoss:0.13197 | train PccLoss:0.64478 | valid MseLoss:0.26442 | valid PccLoss:0.96445\n",
            "end of epoch:866 | time: 0.05s | train loss:0.18403 | valid loss:0.30525 | train MseLoss:0.13227 | train PccLoss:0.64991 | valid MseLoss:0.23415 | valid PccLoss:0.94514\n",
            "end of epoch:867 | time: 0.05s | train loss:0.18229 | valid loss:0.31155 | train MseLoss:0.13075 | train PccLoss:0.64614 | valid MseLoss:0.24099 | valid PccLoss:0.94657\n",
            "end of epoch:868 | time: 0.05s | train loss:0.18578 | valid loss:0.28440 | train MseLoss:0.13396 | train PccLoss:0.65217 | valid MseLoss:0.21484 | valid PccLoss:0.91045\n",
            "end of epoch:869 | time: 0.05s | train loss:0.18187 | valid loss:0.29822 | train MseLoss:0.13018 | train PccLoss:0.64704 | valid MseLoss:0.22916 | valid PccLoss:0.91981\n",
            "end of epoch:870 | time: 0.05s | train loss:0.18208 | valid loss:0.29807 | train MseLoss:0.13021 | train PccLoss:0.64890 | valid MseLoss:0.22869 | valid PccLoss:0.92250\n",
            "end of epoch:871 | time: 0.05s | train loss:0.18097 | valid loss:0.31977 | train MseLoss:0.12965 | train PccLoss:0.64280 | valid MseLoss:0.25042 | valid PccLoss:0.94388\n",
            "end of epoch:872 | time: 0.05s | train loss:0.18414 | valid loss:0.32415 | train MseLoss:0.13260 | train PccLoss:0.64798 | valid MseLoss:0.25977 | valid PccLoss:0.90362\n",
            "end of epoch:873 | time: 0.05s | train loss:0.18396 | valid loss:0.30964 | train MseLoss:0.13220 | train PccLoss:0.64979 | valid MseLoss:0.24067 | valid PccLoss:0.93035\n",
            "end of epoch:874 | time: 0.05s | train loss:0.18342 | valid loss:0.30273 | train MseLoss:0.13206 | train PccLoss:0.64559 | valid MseLoss:0.23343 | valid PccLoss:0.92644\n",
            "end of epoch:875 | time: 0.05s | train loss:0.18471 | valid loss:0.31487 | train MseLoss:0.13338 | train PccLoss:0.64671 | valid MseLoss:0.24509 | valid PccLoss:0.94289\n",
            "end of epoch:876 | time: 0.05s | train loss:0.17940 | valid loss:0.32119 | train MseLoss:0.12760 | train PccLoss:0.64567 | valid MseLoss:0.25476 | valid PccLoss:0.91904\n",
            "end of epoch:877 | time: 0.05s | train loss:0.18289 | valid loss:0.34182 | train MseLoss:0.13133 | train PccLoss:0.64694 | valid MseLoss:0.27525 | valid PccLoss:0.94102\n",
            "end of epoch:878 | time: 0.05s | train loss:0.18549 | valid loss:0.32268 | train MseLoss:0.13462 | train PccLoss:0.64330 | valid MseLoss:0.25181 | valid PccLoss:0.96056\n",
            "end of epoch:879 | time: 0.05s | train loss:0.18103 | valid loss:0.30402 | train MseLoss:0.12978 | train PccLoss:0.64225 | valid MseLoss:0.23723 | valid PccLoss:0.90513\n",
            "end of epoch:880 | time: 0.05s | train loss:0.18140 | valid loss:0.31468 | train MseLoss:0.13001 | train PccLoss:0.64393 | valid MseLoss:0.24859 | valid PccLoss:0.90948\n",
            "end of epoch:881 | time: 0.05s | train loss:0.18140 | valid loss:0.31135 | train MseLoss:0.12994 | train PccLoss:0.64455 | valid MseLoss:0.24570 | valid PccLoss:0.90213\n",
            "end of epoch:882 | time: 0.05s | train loss:0.18591 | valid loss:0.32322 | train MseLoss:0.13439 | train PccLoss:0.64957 | valid MseLoss:0.25285 | valid PccLoss:0.95654\n",
            "end of epoch:883 | time: 0.07s | train loss:0.18160 | valid loss:0.31675 | train MseLoss:0.13028 | train PccLoss:0.64345 | valid MseLoss:0.25241 | valid PccLoss:0.89576\n",
            "end of epoch:884 | time: 0.07s | train loss:0.18541 | valid loss:0.29615 | train MseLoss:0.13443 | train PccLoss:0.64429 | valid MseLoss:0.22618 | valid PccLoss:0.92592\n",
            "end of epoch:885 | time: 0.07s | train loss:0.18290 | valid loss:0.32547 | train MseLoss:0.13144 | train PccLoss:0.64601 | valid MseLoss:0.25409 | valid PccLoss:0.96795\n",
            "end of epoch:886 | time: 0.06s | train loss:0.18219 | valid loss:0.30339 | train MseLoss:0.13140 | train PccLoss:0.63930 | valid MseLoss:0.23461 | valid PccLoss:0.92241\n",
            "end of epoch:887 | time: 0.06s | train loss:0.18500 | valid loss:0.31064 | train MseLoss:0.13367 | train PccLoss:0.64705 | valid MseLoss:0.23983 | valid PccLoss:0.94794\n",
            "end of epoch:888 | time: 0.06s | train loss:0.18209 | valid loss:0.31448 | train MseLoss:0.13072 | train PccLoss:0.64440 | valid MseLoss:0.24344 | valid PccLoss:0.95381\n",
            "end of epoch:889 | time: 0.06s | train loss:0.18322 | valid loss:0.30345 | train MseLoss:0.13183 | train PccLoss:0.64566 | valid MseLoss:0.23317 | valid PccLoss:0.93593\n",
            "end of epoch:890 | time: 0.06s | train loss:0.18162 | valid loss:0.30616 | train MseLoss:0.13022 | train PccLoss:0.64426 | valid MseLoss:0.23803 | valid PccLoss:0.91933\n",
            "end of epoch:891 | time: 0.06s | train loss:0.18456 | valid loss:0.31457 | train MseLoss:0.13287 | train PccLoss:0.64978 | valid MseLoss:0.24834 | valid PccLoss:0.91061\n",
            "end of epoch:892 | time: 0.06s | train loss:0.18298 | valid loss:0.30254 | train MseLoss:0.13232 | train PccLoss:0.63894 | valid MseLoss:0.22966 | valid PccLoss:0.95845\n",
            "end of epoch:893 | time: 0.05s | train loss:0.18129 | valid loss:0.32723 | train MseLoss:0.13003 | train PccLoss:0.64267 | valid MseLoss:0.26381 | valid PccLoss:0.89806\n",
            "end of epoch:894 | time: 0.06s | train loss:0.18368 | valid loss:0.33509 | train MseLoss:0.13289 | train PccLoss:0.64083 | valid MseLoss:0.26674 | valid PccLoss:0.95023\n",
            "end of epoch:895 | time: 0.06s | train loss:0.18162 | valid loss:0.30494 | train MseLoss:0.13037 | train PccLoss:0.64288 | valid MseLoss:0.23530 | valid PccLoss:0.93166\n",
            "end of epoch:896 | time: 0.06s | train loss:0.18329 | valid loss:0.32312 | train MseLoss:0.13220 | train PccLoss:0.64318 | valid MseLoss:0.25224 | valid PccLoss:0.96105\n",
            "end of epoch:897 | time: 0.06s | train loss:0.18133 | valid loss:0.33617 | train MseLoss:0.13033 | train PccLoss:0.64038 | valid MseLoss:0.26628 | valid PccLoss:0.96524\n",
            "end of epoch:898 | time: 0.05s | train loss:0.18193 | valid loss:0.30099 | train MseLoss:0.13127 | train PccLoss:0.63793 | valid MseLoss:0.23325 | valid PccLoss:0.91068\n",
            "end of epoch:899 | time: 0.06s | train loss:0.18208 | valid loss:0.29616 | train MseLoss:0.13109 | train PccLoss:0.64095 | valid MseLoss:0.22970 | valid PccLoss:0.89431\n",
            "end of epoch:900 | time: 0.05s | train loss:0.18148 | valid loss:0.28993 | train MseLoss:0.13094 | train PccLoss:0.63633 | valid MseLoss:0.22058 | valid PccLoss:0.91404\n",
            "end of epoch:901 | time: 0.06s | train loss:0.18479 | valid loss:0.31650 | train MseLoss:0.13328 | train PccLoss:0.64839 | valid MseLoss:0.24574 | valid PccLoss:0.95333\n",
            "end of epoch:902 | time: 0.05s | train loss:0.18476 | valid loss:0.31275 | train MseLoss:0.13395 | train PccLoss:0.64204 | valid MseLoss:0.24269 | valid PccLoss:0.94334\n",
            "end of epoch:903 | time: 0.07s | train loss:0.18259 | valid loss:0.30455 | train MseLoss:0.13190 | train PccLoss:0.63873 | valid MseLoss:0.23577 | valid PccLoss:0.92357\n",
            "end of epoch:904 | time: 0.06s | train loss:0.18115 | valid loss:0.32029 | train MseLoss:0.13005 | train PccLoss:0.64102 | valid MseLoss:0.25161 | valid PccLoss:0.93841\n",
            "end of epoch:905 | time: 0.06s | train loss:0.18144 | valid loss:0.28636 | train MseLoss:0.13119 | train PccLoss:0.63368 | valid MseLoss:0.21541 | valid PccLoss:0.92484\n",
            "end of epoch:906 | time: 0.06s | train loss:0.18156 | valid loss:0.30818 | train MseLoss:0.13092 | train PccLoss:0.63730 | valid MseLoss:0.23993 | valid PccLoss:0.92245\n",
            "end of epoch:907 | time: 0.06s | train loss:0.18175 | valid loss:0.31379 | train MseLoss:0.13110 | train PccLoss:0.63757 | valid MseLoss:0.24608 | valid PccLoss:0.92314\n",
            "end of epoch:908 | time: 0.06s | train loss:0.18047 | valid loss:0.31751 | train MseLoss:0.12987 | train PccLoss:0.63590 | valid MseLoss:0.24767 | valid PccLoss:0.94607\n",
            "end of epoch:909 | time: 0.05s | train loss:0.18057 | valid loss:0.31256 | train MseLoss:0.12973 | train PccLoss:0.63806 | valid MseLoss:0.24518 | valid PccLoss:0.91891\n",
            "end of epoch:910 | time: 0.05s | train loss:0.18090 | valid loss:0.31910 | train MseLoss:0.13041 | train PccLoss:0.63532 | valid MseLoss:0.24886 | valid PccLoss:0.95128\n",
            "end of epoch:911 | time: 0.06s | train loss:0.18024 | valid loss:0.32403 | train MseLoss:0.12959 | train PccLoss:0.63608 | valid MseLoss:0.25522 | valid PccLoss:0.94326\n",
            "end of epoch:912 | time: 0.05s | train loss:0.17811 | valid loss:0.30916 | train MseLoss:0.12725 | train PccLoss:0.63587 | valid MseLoss:0.23694 | valid PccLoss:0.95917\n",
            "end of epoch:913 | time: 0.05s | train loss:0.18188 | valid loss:0.33126 | train MseLoss:0.13095 | train PccLoss:0.64022 | valid MseLoss:0.26773 | valid PccLoss:0.90297\n",
            "end of epoch:914 | time: 0.05s | train loss:0.18244 | valid loss:0.31379 | train MseLoss:0.13136 | train PccLoss:0.64221 | valid MseLoss:0.24636 | valid PccLoss:0.92063\n",
            "end of epoch:915 | time: 0.06s | train loss:0.18119 | valid loss:0.32362 | train MseLoss:0.13076 | train PccLoss:0.63505 | valid MseLoss:0.25681 | valid PccLoss:0.92492\n",
            "end of epoch:916 | time: 0.05s | train loss:0.18398 | valid loss:0.30186 | train MseLoss:0.13341 | train PccLoss:0.63909 | valid MseLoss:0.22872 | valid PccLoss:0.96017\n",
            "end of epoch:917 | time: 0.06s | train loss:0.18150 | valid loss:0.31833 | train MseLoss:0.13105 | train PccLoss:0.63556 | valid MseLoss:0.25038 | valid PccLoss:0.92992\n",
            "end of epoch:918 | time: 0.06s | train loss:0.17935 | valid loss:0.31053 | train MseLoss:0.12905 | train PccLoss:0.63204 | valid MseLoss:0.24465 | valid PccLoss:0.90349\n",
            "end of epoch:919 | time: 0.06s | train loss:0.18240 | valid loss:0.31801 | train MseLoss:0.13176 | train PccLoss:0.63817 | valid MseLoss:0.24715 | valid PccLoss:0.95573\n",
            "end of epoch:920 | time: 0.06s | train loss:0.17933 | valid loss:0.30523 | train MseLoss:0.12867 | train PccLoss:0.63520 | valid MseLoss:0.23791 | valid PccLoss:0.91108\n",
            "end of epoch:921 | time: 0.07s | train loss:0.18198 | valid loss:0.30136 | train MseLoss:0.13139 | train PccLoss:0.63726 | valid MseLoss:0.23386 | valid PccLoss:0.90882\n",
            "end of epoch:922 | time: 0.07s | train loss:0.18106 | valid loss:0.31315 | train MseLoss:0.12979 | train PccLoss:0.64250 | valid MseLoss:0.24373 | valid PccLoss:0.93797\n",
            "end of epoch:923 | time: 0.06s | train loss:0.18029 | valid loss:0.33331 | train MseLoss:0.13036 | train PccLoss:0.62968 | valid MseLoss:0.26876 | valid PccLoss:0.91420\n",
            "end of epoch:924 | time: 0.06s | train loss:0.18079 | valid loss:0.29883 | train MseLoss:0.12986 | train PccLoss:0.63918 | valid MseLoss:0.23270 | valid PccLoss:0.89399\n",
            "end of epoch:925 | time: 0.06s | train loss:0.18188 | valid loss:0.32024 | train MseLoss:0.13206 | train PccLoss:0.63022 | valid MseLoss:0.25418 | valid PccLoss:0.91473\n",
            "end of epoch:926 | time: 0.06s | train loss:0.18063 | valid loss:0.32499 | train MseLoss:0.13082 | train PccLoss:0.62889 | valid MseLoss:0.26001 | valid PccLoss:0.90975\n",
            "end of epoch:927 | time: 0.06s | train loss:0.18034 | valid loss:0.29547 | train MseLoss:0.12969 | train PccLoss:0.63617 | valid MseLoss:0.22596 | valid PccLoss:0.92109\n",
            "end of epoch:928 | time: 0.06s | train loss:0.18220 | valid loss:0.33416 | train MseLoss:0.13216 | train PccLoss:0.63256 | valid MseLoss:0.26955 | valid PccLoss:0.91565\n",
            "end of epoch:929 | time: 0.06s | train loss:0.18051 | valid loss:0.30560 | train MseLoss:0.13024 | train PccLoss:0.63301 | valid MseLoss:0.23366 | valid PccLoss:0.95305\n",
            "end of epoch:930 | time: 0.06s | train loss:0.18262 | valid loss:0.30666 | train MseLoss:0.13260 | train PccLoss:0.63283 | valid MseLoss:0.23974 | valid PccLoss:0.90900\n",
            "end of epoch:931 | time: 0.06s | train loss:0.18183 | valid loss:0.32555 | train MseLoss:0.13173 | train PccLoss:0.63273 | valid MseLoss:0.25513 | valid PccLoss:0.95934\n",
            "end of epoch:932 | time: 0.06s | train loss:0.17859 | valid loss:0.29761 | train MseLoss:0.12875 | train PccLoss:0.62715 | valid MseLoss:0.22592 | valid PccLoss:0.94278\n",
            "end of epoch:933 | time: 0.06s | train loss:0.18003 | valid loss:0.30644 | train MseLoss:0.12967 | train PccLoss:0.63329 | valid MseLoss:0.23936 | valid PccLoss:0.91009\n",
            "end of epoch:934 | time: 0.05s | train loss:0.17963 | valid loss:0.32207 | train MseLoss:0.12970 | train PccLoss:0.62907 | valid MseLoss:0.25199 | valid PccLoss:0.95279\n",
            "end of epoch:935 | time: 0.05s | train loss:0.17972 | valid loss:0.32370 | train MseLoss:0.12920 | train PccLoss:0.63444 | valid MseLoss:0.25249 | valid PccLoss:0.96464\n",
            "end of epoch:936 | time: 0.05s | train loss:0.17976 | valid loss:0.29434 | train MseLoss:0.12955 | train PccLoss:0.63169 | valid MseLoss:0.22267 | valid PccLoss:0.93935\n",
            "end of epoch:937 | time: 0.05s | train loss:0.18363 | valid loss:0.31974 | train MseLoss:0.13367 | train PccLoss:0.63327 | valid MseLoss:0.25192 | valid PccLoss:0.93020\n",
            "end of epoch:938 | time: 0.05s | train loss:0.18108 | valid loss:0.32349 | train MseLoss:0.13085 | train PccLoss:0.63317 | valid MseLoss:0.25283 | valid PccLoss:0.95946\n",
            "end of epoch:939 | time: 0.06s | train loss:0.18003 | valid loss:0.29540 | train MseLoss:0.12954 | train PccLoss:0.63436 | valid MseLoss:0.22726 | valid PccLoss:0.90863\n",
            "end of epoch:940 | time: 0.05s | train loss:0.18057 | valid loss:0.30943 | train MseLoss:0.13060 | train PccLoss:0.63030 | valid MseLoss:0.23811 | valid PccLoss:0.95134\n",
            "end of epoch:941 | time: 0.05s | train loss:0.18131 | valid loss:0.31509 | train MseLoss:0.13133 | train PccLoss:0.63108 | valid MseLoss:0.24719 | valid PccLoss:0.92616\n",
            "end of epoch:942 | time: 0.05s | train loss:0.17861 | valid loss:0.30926 | train MseLoss:0.12858 | train PccLoss:0.62881 | valid MseLoss:0.24236 | valid PccLoss:0.91137\n",
            "end of epoch:943 | time: 0.05s | train loss:0.18488 | valid loss:0.31794 | train MseLoss:0.13499 | train PccLoss:0.63382 | valid MseLoss:0.24916 | valid PccLoss:0.93700\n",
            "end of epoch:944 | time: 0.05s | train loss:0.17826 | valid loss:0.31997 | train MseLoss:0.12835 | train PccLoss:0.62746 | valid MseLoss:0.25222 | valid PccLoss:0.92978\n",
            "end of epoch:945 | time: 0.05s | train loss:0.18155 | valid loss:0.32290 | train MseLoss:0.13167 | train PccLoss:0.63045 | valid MseLoss:0.25329 | valid PccLoss:0.94946\n",
            "end of epoch:946 | time: 0.05s | train loss:0.18158 | valid loss:0.30132 | train MseLoss:0.13144 | train PccLoss:0.63286 | valid MseLoss:0.23469 | valid PccLoss:0.90098\n",
            "end of epoch:947 | time: 0.05s | train loss:0.18557 | valid loss:0.31438 | train MseLoss:0.13534 | train PccLoss:0.63765 | valid MseLoss:0.24861 | valid PccLoss:0.90629\n",
            "end of epoch:948 | time: 0.05s | train loss:0.17921 | valid loss:0.31183 | train MseLoss:0.12923 | train PccLoss:0.62903 | valid MseLoss:0.24316 | valid PccLoss:0.92990\n",
            "end of epoch:949 | time: 0.05s | train loss:0.18062 | valid loss:0.31623 | train MseLoss:0.13052 | train PccLoss:0.63158 | valid MseLoss:0.24761 | valid PccLoss:0.93380\n",
            "end of epoch:950 | time: 0.05s | train loss:0.18265 | valid loss:0.32533 | train MseLoss:0.13261 | train PccLoss:0.63301 | valid MseLoss:0.25717 | valid PccLoss:0.93877\n",
            "end of epoch:951 | time: 0.05s | train loss:0.18190 | valid loss:0.32316 | train MseLoss:0.13165 | train PccLoss:0.63415 | valid MseLoss:0.25391 | valid PccLoss:0.94641\n",
            "end of epoch:952 | time: 0.05s | train loss:0.18288 | valid loss:0.29913 | train MseLoss:0.13273 | train PccLoss:0.63426 | valid MseLoss:0.22900 | valid PccLoss:0.93028\n",
            "end of epoch:953 | time: 0.05s | train loss:0.17870 | valid loss:0.30975 | train MseLoss:0.12937 | train PccLoss:0.62269 | valid MseLoss:0.23793 | valid PccLoss:0.95613\n",
            "end of epoch:954 | time: 0.05s | train loss:0.18424 | valid loss:0.30571 | train MseLoss:0.13455 | train PccLoss:0.63149 | valid MseLoss:0.23416 | valid PccLoss:0.94966\n",
            "end of epoch:955 | time: 0.05s | train loss:0.18014 | valid loss:0.31012 | train MseLoss:0.13008 | train PccLoss:0.63065 | valid MseLoss:0.24371 | valid PccLoss:0.90789\n",
            "end of epoch:956 | time: 0.05s | train loss:0.17763 | valid loss:0.35099 | train MseLoss:0.12754 | train PccLoss:0.62848 | valid MseLoss:0.28576 | valid PccLoss:0.93807\n",
            "end of epoch:957 | time: 0.05s | train loss:0.18130 | valid loss:0.30874 | train MseLoss:0.13174 | train PccLoss:0.62728 | valid MseLoss:0.23745 | valid PccLoss:0.95032\n",
            "end of epoch:958 | time: 0.05s | train loss:0.17992 | valid loss:0.30188 | train MseLoss:0.12963 | train PccLoss:0.63251 | valid MseLoss:0.23552 | valid PccLoss:0.89908\n",
            "end of epoch:959 | time: 0.06s | train loss:0.18086 | valid loss:0.30024 | train MseLoss:0.13124 | train PccLoss:0.62745 | valid MseLoss:0.23078 | valid PccLoss:0.92540\n",
            "end of epoch:960 | time: 0.05s | train loss:0.18236 | valid loss:0.32329 | train MseLoss:0.13246 | train PccLoss:0.63148 | valid MseLoss:0.25552 | valid PccLoss:0.93322\n",
            "end of epoch:961 | time: 0.05s | train loss:0.17816 | valid loss:0.30342 | train MseLoss:0.12863 | train PccLoss:0.62390 | valid MseLoss:0.23314 | valid PccLoss:0.93596\n",
            "end of epoch:962 | time: 0.05s | train loss:0.17995 | valid loss:0.33055 | train MseLoss:0.13016 | train PccLoss:0.62808 | valid MseLoss:0.26561 | valid PccLoss:0.91494\n",
            "end of epoch:963 | time: 0.05s | train loss:0.17879 | valid loss:0.31266 | train MseLoss:0.12943 | train PccLoss:0.62307 | valid MseLoss:0.24420 | valid PccLoss:0.92886\n",
            "end of epoch:964 | time: 0.05s | train loss:0.18165 | valid loss:0.31226 | train MseLoss:0.13173 | train PccLoss:0.63089 | valid MseLoss:0.24185 | valid PccLoss:0.94599\n",
            "end of epoch:965 | time: 0.05s | train loss:0.17991 | valid loss:0.30743 | train MseLoss:0.13055 | train PccLoss:0.62416 | valid MseLoss:0.23609 | valid PccLoss:0.94954\n",
            "end of epoch:966 | time: 0.05s | train loss:0.18096 | valid loss:0.31793 | train MseLoss:0.13177 | train PccLoss:0.62367 | valid MseLoss:0.25351 | valid PccLoss:0.89769\n",
            "end of epoch:967 | time: 0.05s | train loss:0.17830 | valid loss:0.30661 | train MseLoss:0.12872 | train PccLoss:0.62453 | valid MseLoss:0.24051 | valid PccLoss:0.90151\n",
            "end of epoch:968 | time: 0.05s | train loss:0.17910 | valid loss:0.31259 | train MseLoss:0.13010 | train PccLoss:0.62010 | valid MseLoss:0.24519 | valid PccLoss:0.91913\n",
            "end of epoch:969 | time: 0.05s | train loss:0.18175 | valid loss:0.30735 | train MseLoss:0.13171 | train PccLoss:0.63211 | valid MseLoss:0.23538 | valid PccLoss:0.95508\n",
            "end of epoch:970 | time: 0.05s | train loss:0.18336 | valid loss:0.29929 | train MseLoss:0.13426 | train PccLoss:0.62534 | valid MseLoss:0.23224 | valid PccLoss:0.90271\n",
            "end of epoch:971 | time: 0.05s | train loss:0.17872 | valid loss:0.32457 | train MseLoss:0.12907 | train PccLoss:0.62555 | valid MseLoss:0.25472 | valid PccLoss:0.95324\n",
            "end of epoch:972 | time: 0.05s | train loss:0.17773 | valid loss:0.30556 | train MseLoss:0.12783 | train PccLoss:0.62682 | valid MseLoss:0.23925 | valid PccLoss:0.90234\n",
            "end of epoch:973 | time: 0.05s | train loss:0.18013 | valid loss:0.29542 | train MseLoss:0.13051 | train PccLoss:0.62673 | valid MseLoss:0.22394 | valid PccLoss:0.93873\n",
            "end of epoch:974 | time: 0.05s | train loss:0.17953 | valid loss:0.30237 | train MseLoss:0.12983 | train PccLoss:0.62683 | valid MseLoss:0.23439 | valid PccLoss:0.91420\n",
            "end of epoch:975 | time: 0.05s | train loss:0.17852 | valid loss:0.31243 | train MseLoss:0.12934 | train PccLoss:0.62114 | valid MseLoss:0.24695 | valid PccLoss:0.90179\n",
            "end of epoch:976 | time: 0.05s | train loss:0.18076 | valid loss:0.29168 | train MseLoss:0.13180 | train PccLoss:0.62137 | valid MseLoss:0.22368 | valid PccLoss:0.90371\n",
            "end of epoch:977 | time: 0.05s | train loss:0.17771 | valid loss:0.29288 | train MseLoss:0.12869 | train PccLoss:0.61890 | valid MseLoss:0.22470 | valid PccLoss:0.90649\n",
            "end of epoch:978 | time: 0.05s | train loss:0.18053 | valid loss:0.30708 | train MseLoss:0.13170 | train PccLoss:0.62003 | valid MseLoss:0.23937 | valid PccLoss:0.91650\n",
            "end of epoch:979 | time: 0.05s | train loss:0.17890 | valid loss:0.30975 | train MseLoss:0.12959 | train PccLoss:0.62270 | valid MseLoss:0.24377 | valid PccLoss:0.90357\n",
            "end of epoch:980 | time: 0.05s | train loss:0.17871 | valid loss:0.30041 | train MseLoss:0.12920 | train PccLoss:0.62426 | valid MseLoss:0.23111 | valid PccLoss:0.92410\n",
            "end of epoch:981 | time: 0.05s | train loss:0.18152 | valid loss:0.30589 | train MseLoss:0.13214 | train PccLoss:0.62602 | valid MseLoss:0.23395 | valid PccLoss:0.95327\n",
            "end of epoch:982 | time: 0.05s | train loss:0.18121 | valid loss:0.32176 | train MseLoss:0.13192 | train PccLoss:0.62481 | valid MseLoss:0.24952 | valid PccLoss:0.97184\n",
            "end of epoch:983 | time: 0.05s | train loss:0.18143 | valid loss:0.30354 | train MseLoss:0.13174 | train PccLoss:0.62864 | valid MseLoss:0.23603 | valid PccLoss:0.91115\n",
            "end of epoch:984 | time: 0.05s | train loss:0.17764 | valid loss:0.30810 | train MseLoss:0.12837 | train PccLoss:0.62108 | valid MseLoss:0.23902 | valid PccLoss:0.92979\n",
            "end of epoch:985 | time: 0.05s | train loss:0.17768 | valid loss:0.29189 | train MseLoss:0.12822 | train PccLoss:0.62283 | valid MseLoss:0.21905 | valid PccLoss:0.94745\n",
            "end of epoch:986 | time: 0.05s | train loss:0.18076 | valid loss:0.31545 | train MseLoss:0.13183 | train PccLoss:0.62110 | valid MseLoss:0.24886 | valid PccLoss:0.91482\n",
            "end of epoch:987 | time: 0.05s | train loss:0.17944 | valid loss:0.30987 | train MseLoss:0.13060 | train PccLoss:0.61909 | valid MseLoss:0.23802 | valid PccLoss:0.95649\n",
            "end of epoch:988 | time: 0.05s | train loss:0.17832 | valid loss:0.30684 | train MseLoss:0.12990 | train PccLoss:0.61414 | valid MseLoss:0.24137 | valid PccLoss:0.89603\n",
            "end of epoch:989 | time: 0.05s | train loss:0.17939 | valid loss:0.32314 | train MseLoss:0.13003 | train PccLoss:0.62357 | valid MseLoss:0.25679 | valid PccLoss:0.92024\n",
            "end of epoch:990 | time: 0.05s | train loss:0.17813 | valid loss:0.29628 | train MseLoss:0.12900 | train PccLoss:0.62028 | valid MseLoss:0.22392 | valid PccLoss:0.94756\n",
            "end of epoch:991 | time: 0.07s | train loss:0.17836 | valid loss:0.31897 | train MseLoss:0.12963 | train PccLoss:0.61692 | valid MseLoss:0.25618 | valid PccLoss:0.88414\n",
            "end of epoch:992 | time: 0.07s | train loss:0.17790 | valid loss:0.32292 | train MseLoss:0.12873 | train PccLoss:0.62051 | valid MseLoss:0.25204 | valid PccLoss:0.96087\n",
            "end of epoch:993 | time: 0.05s | train loss:0.17911 | valid loss:0.30306 | train MseLoss:0.13046 | train PccLoss:0.61698 | valid MseLoss:0.23111 | valid PccLoss:0.95070\n",
            "end of epoch:994 | time: 0.05s | train loss:0.17694 | valid loss:0.29912 | train MseLoss:0.12781 | train PccLoss:0.61917 | valid MseLoss:0.23184 | valid PccLoss:0.90460\n",
            "end of epoch:995 | time: 0.05s | train loss:0.17955 | valid loss:0.30165 | train MseLoss:0.13087 | train PccLoss:0.61769 | valid MseLoss:0.23097 | valid PccLoss:0.93778\n",
            "end of epoch:996 | time: 0.05s | train loss:0.17930 | valid loss:0.31223 | train MseLoss:0.12991 | train PccLoss:0.62382 | valid MseLoss:0.24084 | valid PccLoss:0.95479\n",
            "end of epoch:997 | time: 0.05s | train loss:0.17838 | valid loss:0.30611 | train MseLoss:0.12958 | train PccLoss:0.61761 | valid MseLoss:0.23572 | valid PccLoss:0.93960\n",
            "end of epoch:998 | time: 0.05s | train loss:0.17894 | valid loss:0.32708 | train MseLoss:0.13022 | train PccLoss:0.61738 | valid MseLoss:0.26014 | valid PccLoss:0.92961\n",
            "end of epoch:999 | time: 0.05s | train loss:0.17808 | valid loss:0.33083 | train MseLoss:0.12894 | train PccLoss:0.62031 | valid MseLoss:0.26586 | valid PccLoss:0.91561\n",
            "==============================\n",
            "Test evaluate result:\n",
            "Average PCC: 0.12567726306974844\n",
            "Average MSE: 0.1975027471780777\n",
            "Average D: 0.4440184049079755\n",
            "==============================\n",
            "\n",
            "predict finish:\n",
            "pre_GECs size:(2000, 978)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "CRISPR_feature_train, CRISPR_feature_valid, CRISPR_feature_test = load_data(\"drive/MyDrive/CRISPR/\",\n",
        "                                                                            \"feature_dict.pkl\")\n",
        "\n",
        "CRISPR_GECs_train, CRISPR_GECs_valid, CRISPR_GECs_test = load_data(\"drive/MyDrive/CRISPR/\", \"GECs_dict.pkl\")\n",
        "\n",
        "CRISPR_train_dataloader, CRISPR_valid_dataloader = get_dataloader(32, CRISPR_feature_train,\n",
        "                                                                  CRISPR_GECs_train,\n",
        "                                                                  CRISPR_feature_valid, CRISPR_GECs_valid)\n",
        "\n",
        "CRISPR_pre_GECs = run_model(num_layers=5,\n",
        "                            hidden_nodes=1024,\n",
        "                            activate_func=nn.LeakyReLU,\n",
        "                            dropout_rate=0.1,\n",
        "                            epochs=1000,\n",
        "                            train_dataloader=CRISPR_train_dataloader,\n",
        "                            valid_dataloader=CRISPR_valid_dataloader,\n",
        "                            beta=0.1,\n",
        "                            feature_test=CRISPR_feature_test,\n",
        "                            gecs_test=CRISPR_GECs_test,\n",
        "                            save_path=\"drive/MyDrive/CRISPR/results/\",\n",
        "                            node_feature=node_feature,\n",
        "                            name=\"CRISPR\",\n",
        "                            warmup_epoch=10,\n",
        "                            learning_rate=0.00035)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "Dp57WojQiFPq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp57WojQiFPq",
        "outputId": "e552b445-d4a8-4c0e-9afd-f3a39c694c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CRISPR GenSAN model\n",
            "pre-GECS dimension:\n",
            "train data:torch.Size([264, 3, 978])\n",
            "valid data:torch.Size([38, 3, 978])\n",
            "test data:torch.Size([76, 3, 978])\n",
            "\n",
            "end of epoch:  0 | time: 1.58s | train loss:0.82121 | valid loss:0.85952 | train MseLoss:0.80324 | train PccLoss:0.98295 | valid MseLoss:0.84591 | valid PccLoss:0.98206\n",
            "end of epoch:  1 | time: 1.54s | train loss:0.79145 | valid loss:0.86226 | train MseLoss:0.77172 | train PccLoss:0.96902 | valid MseLoss:0.84865 | valid PccLoss:0.98481\n",
            "end of epoch:  2 | time: 1.60s | train loss:0.78747 | valid loss:0.84952 | train MseLoss:0.76754 | train PccLoss:0.96677 | valid MseLoss:0.83417 | valid PccLoss:0.98774\n",
            "end of epoch:  3 | time: 1.56s | train loss:0.77817 | valid loss:0.81502 | train MseLoss:0.75795 | train PccLoss:0.96010 | valid MseLoss:0.79806 | valid PccLoss:0.96763\n",
            "end of epoch:  4 | time: 1.57s | train loss:0.75887 | valid loss:0.78820 | train MseLoss:0.73795 | train PccLoss:0.94724 | valid MseLoss:0.76940 | valid PccLoss:0.95743\n",
            "end of epoch:  5 | time: 1.54s | train loss:0.73303 | valid loss:0.80074 | train MseLoss:0.71135 | train PccLoss:0.92816 | valid MseLoss:0.78225 | valid PccLoss:0.96716\n",
            "end of epoch:  6 | time: 1.56s | train loss:0.70370 | valid loss:0.76452 | train MseLoss:0.68086 | train PccLoss:0.90925 | valid MseLoss:0.74406 | valid PccLoss:0.94866\n",
            "end of epoch:  7 | time: 1.56s | train loss:0.67110 | valid loss:0.73445 | train MseLoss:0.64766 | train PccLoss:0.88209 | valid MseLoss:0.71269 | valid PccLoss:0.93033\n",
            "end of epoch:  8 | time: 1.56s | train loss:0.63908 | valid loss:0.71844 | train MseLoss:0.61506 | train PccLoss:0.85530 | valid MseLoss:0.69598 | valid PccLoss:0.92062\n",
            "end of epoch:  9 | time: 1.60s | train loss:0.60658 | valid loss:0.70597 | train MseLoss:0.58117 | train PccLoss:0.83529 | valid MseLoss:0.68298 | valid PccLoss:0.91291\n",
            "end of epoch: 10 | time: 1.61s | train loss:0.58097 | valid loss:0.67744 | train MseLoss:0.55541 | train PccLoss:0.81096 | valid MseLoss:0.65347 | valid PccLoss:0.89320\n",
            "end of epoch: 11 | time: 1.60s | train loss:0.55023 | valid loss:0.64099 | train MseLoss:0.52435 | train PccLoss:0.78320 | valid MseLoss:0.61649 | valid PccLoss:0.86148\n",
            "end of epoch: 12 | time: 1.57s | train loss:0.52439 | valid loss:0.60121 | train MseLoss:0.49797 | train PccLoss:0.76216 | valid MseLoss:0.57486 | valid PccLoss:0.83833\n",
            "end of epoch: 13 | time: 1.56s | train loss:0.49513 | valid loss:0.59764 | train MseLoss:0.46792 | train PccLoss:0.74006 | valid MseLoss:0.56954 | valid PccLoss:0.85061\n",
            "end of epoch: 14 | time: 1.56s | train loss:0.46395 | valid loss:0.55879 | train MseLoss:0.43604 | train PccLoss:0.71514 | valid MseLoss:0.53104 | valid PccLoss:0.80857\n",
            "end of epoch: 15 | time: 1.54s | train loss:0.44166 | valid loss:0.57351 | train MseLoss:0.41317 | train PccLoss:0.69808 | valid MseLoss:0.54191 | valid PccLoss:0.85789\n",
            "end of epoch: 16 | time: 1.57s | train loss:0.41414 | valid loss:0.53330 | train MseLoss:0.38501 | train PccLoss:0.67626 | valid MseLoss:0.50305 | valid PccLoss:0.80557\n",
            "end of epoch: 17 | time: 1.56s | train loss:0.38813 | valid loss:0.50823 | train MseLoss:0.35894 | train PccLoss:0.65083 | valid MseLoss:0.47358 | valid PccLoss:0.82011\n",
            "end of epoch: 18 | time: 1.59s | train loss:0.36355 | valid loss:0.49603 | train MseLoss:0.33394 | train PccLoss:0.63002 | valid MseLoss:0.46195 | valid PccLoss:0.80276\n",
            "end of epoch: 19 | time: 1.57s | train loss:0.34327 | valid loss:0.48797 | train MseLoss:0.31293 | train PccLoss:0.61637 | valid MseLoss:0.45067 | valid PccLoss:0.82371\n",
            "end of epoch: 20 | time: 1.55s | train loss:0.32166 | valid loss:0.44536 | train MseLoss:0.29120 | train PccLoss:0.59579 | valid MseLoss:0.41121 | valid PccLoss:0.75273\n",
            "end of epoch: 21 | time: 1.56s | train loss:0.30257 | valid loss:0.44095 | train MseLoss:0.27200 | train PccLoss:0.57770 | valid MseLoss:0.40213 | valid PccLoss:0.79030\n",
            "end of epoch: 22 | time: 1.56s | train loss:0.28397 | valid loss:0.43320 | train MseLoss:0.25318 | train PccLoss:0.56109 | valid MseLoss:0.39344 | valid PccLoss:0.79107\n",
            "end of epoch: 23 | time: 1.55s | train loss:0.26917 | valid loss:0.42312 | train MseLoss:0.23817 | train PccLoss:0.54818 | valid MseLoss:0.38238 | valid PccLoss:0.78980\n",
            "end of epoch: 24 | time: 1.73s | train loss:0.25819 | valid loss:0.41536 | train MseLoss:0.22653 | train PccLoss:0.54311 | valid MseLoss:0.37722 | valid PccLoss:0.75868\n",
            "end of epoch: 25 | time: 1.55s | train loss:0.24711 | valid loss:0.38717 | train MseLoss:0.21545 | train PccLoss:0.53201 | valid MseLoss:0.34708 | valid PccLoss:0.74803\n",
            "end of epoch: 26 | time: 1.57s | train loss:0.23935 | valid loss:0.37254 | train MseLoss:0.20771 | train PccLoss:0.52407 | valid MseLoss:0.33118 | valid PccLoss:0.74471\n",
            "end of epoch: 27 | time: 1.56s | train loss:0.23187 | valid loss:0.40392 | train MseLoss:0.20042 | train PccLoss:0.51493 | valid MseLoss:0.36513 | valid PccLoss:0.75295\n",
            "end of epoch: 28 | time: 1.55s | train loss:0.22006 | valid loss:0.35096 | train MseLoss:0.18845 | train PccLoss:0.50449 | valid MseLoss:0.30956 | valid PccLoss:0.72354\n",
            "end of epoch: 29 | time: 1.53s | train loss:0.21323 | valid loss:0.36112 | train MseLoss:0.18160 | train PccLoss:0.49790 | valid MseLoss:0.31908 | valid PccLoss:0.73951\n",
            "end of epoch: 30 | time: 1.53s | train loss:0.20813 | valid loss:0.37400 | train MseLoss:0.17667 | train PccLoss:0.49132 | valid MseLoss:0.33208 | valid PccLoss:0.75133\n",
            "end of epoch: 31 | time: 1.55s | train loss:0.20053 | valid loss:0.34276 | train MseLoss:0.16895 | train PccLoss:0.48474 | valid MseLoss:0.30140 | valid PccLoss:0.71500\n",
            "end of epoch: 32 | time: 1.53s | train loss:0.19731 | valid loss:0.34860 | train MseLoss:0.16622 | train PccLoss:0.47704 | valid MseLoss:0.30227 | valid PccLoss:0.76555\n",
            "end of epoch: 33 | time: 1.53s | train loss:0.19278 | valid loss:0.35363 | train MseLoss:0.16146 | train PccLoss:0.47472 | valid MseLoss:0.30583 | valid PccLoss:0.78385\n",
            "end of epoch: 34 | time: 1.58s | train loss:0.18927 | valid loss:0.32173 | train MseLoss:0.15806 | train PccLoss:0.47014 | valid MseLoss:0.27684 | valid PccLoss:0.72575\n",
            "end of epoch: 35 | time: 1.55s | train loss:0.19139 | valid loss:0.34012 | train MseLoss:0.16009 | train PccLoss:0.47310 | valid MseLoss:0.29892 | valid PccLoss:0.71091\n",
            "end of epoch: 36 | time: 1.54s | train loss:0.18219 | valid loss:0.32552 | train MseLoss:0.15109 | train PccLoss:0.46208 | valid MseLoss:0.27908 | valid PccLoss:0.74344\n",
            "end of epoch: 37 | time: 1.53s | train loss:0.18253 | valid loss:0.33128 | train MseLoss:0.15154 | train PccLoss:0.46148 | valid MseLoss:0.29013 | valid PccLoss:0.70168\n",
            "end of epoch: 38 | time: 1.54s | train loss:0.17920 | valid loss:0.31132 | train MseLoss:0.14767 | train PccLoss:0.46297 | valid MseLoss:0.26889 | valid PccLoss:0.69324\n",
            "end of epoch: 39 | time: 1.54s | train loss:0.17749 | valid loss:0.29532 | train MseLoss:0.14608 | train PccLoss:0.46014 | valid MseLoss:0.25172 | valid PccLoss:0.68772\n",
            "end of epoch: 40 | time: 1.52s | train loss:0.17210 | valid loss:0.32731 | train MseLoss:0.14145 | train PccLoss:0.44797 | valid MseLoss:0.28197 | valid PccLoss:0.73544\n",
            "end of epoch: 41 | time: 1.53s | train loss:0.17293 | valid loss:0.31400 | train MseLoss:0.14164 | train PccLoss:0.45456 | valid MseLoss:0.27145 | valid PccLoss:0.69693\n",
            "end of epoch: 42 | time: 1.53s | train loss:0.17015 | valid loss:0.30819 | train MseLoss:0.13900 | train PccLoss:0.45045 | valid MseLoss:0.26537 | valid PccLoss:0.69357\n",
            "end of epoch: 43 | time: 1.63s | train loss:0.16752 | valid loss:0.31402 | train MseLoss:0.13709 | train PccLoss:0.44141 | valid MseLoss:0.27258 | valid PccLoss:0.68696\n",
            "end of epoch: 44 | time: 1.56s | train loss:0.16716 | valid loss:0.30212 | train MseLoss:0.13660 | train PccLoss:0.44226 | valid MseLoss:0.26114 | valid PccLoss:0.67101\n",
            "end of epoch: 45 | time: 1.53s | train loss:0.16935 | valid loss:0.32235 | train MseLoss:0.13850 | train PccLoss:0.44706 | valid MseLoss:0.27900 | valid PccLoss:0.71245\n",
            "end of epoch: 46 | time: 1.52s | train loss:0.16648 | valid loss:0.30720 | train MseLoss:0.13550 | train PccLoss:0.44530 | valid MseLoss:0.26306 | valid PccLoss:0.70438\n",
            "end of epoch: 47 | time: 1.53s | train loss:0.16750 | valid loss:0.32543 | train MseLoss:0.13655 | train PccLoss:0.44606 | valid MseLoss:0.27691 | valid PccLoss:0.76212\n",
            "end of epoch: 48 | time: 1.53s | train loss:0.16321 | valid loss:0.30924 | train MseLoss:0.13272 | train PccLoss:0.43766 | valid MseLoss:0.26236 | valid PccLoss:0.73111\n",
            "end of epoch: 49 | time: 1.53s | train loss:0.16295 | valid loss:0.31259 | train MseLoss:0.13200 | train PccLoss:0.44149 | valid MseLoss:0.26651 | valid PccLoss:0.72728\n",
            "end of epoch: 50 | time: 1.54s | train loss:0.16253 | valid loss:0.29837 | train MseLoss:0.13202 | train PccLoss:0.43711 | valid MseLoss:0.25024 | valid PccLoss:0.73157\n",
            "end of epoch: 51 | time: 1.55s | train loss:0.16091 | valid loss:0.32469 | train MseLoss:0.13049 | train PccLoss:0.43473 | valid MseLoss:0.28146 | valid PccLoss:0.71373\n",
            "end of epoch: 52 | time: 1.56s | train loss:0.16225 | valid loss:0.30877 | train MseLoss:0.13208 | train PccLoss:0.43372 | valid MseLoss:0.26086 | valid PccLoss:0.73995\n",
            "end of epoch: 53 | time: 1.56s | train loss:0.15863 | valid loss:0.28855 | train MseLoss:0.12812 | train PccLoss:0.43329 | valid MseLoss:0.24584 | valid PccLoss:0.67302\n",
            "end of epoch: 54 | time: 1.53s | train loss:0.15773 | valid loss:0.30336 | train MseLoss:0.12753 | train PccLoss:0.42951 | valid MseLoss:0.25709 | valid PccLoss:0.71976\n",
            "end of epoch: 55 | time: 1.55s | train loss:0.15677 | valid loss:0.28160 | train MseLoss:0.12658 | train PccLoss:0.42853 | valid MseLoss:0.23755 | valid PccLoss:0.67805\n",
            "end of epoch: 56 | time: 1.53s | train loss:0.15643 | valid loss:0.30234 | train MseLoss:0.12613 | train PccLoss:0.42914 | valid MseLoss:0.25880 | valid PccLoss:0.69417\n",
            "end of epoch: 57 | time: 1.53s | train loss:0.16002 | valid loss:0.30163 | train MseLoss:0.12951 | train PccLoss:0.43464 | valid MseLoss:0.25700 | valid PccLoss:0.70337\n",
            "end of epoch: 58 | time: 1.53s | train loss:0.15501 | valid loss:0.28920 | train MseLoss:0.12502 | train PccLoss:0.42493 | valid MseLoss:0.24583 | valid PccLoss:0.67954\n",
            "end of epoch: 59 | time: 1.53s | train loss:0.15525 | valid loss:0.28777 | train MseLoss:0.12496 | train PccLoss:0.42784 | valid MseLoss:0.24328 | valid PccLoss:0.68813\n",
            "end of epoch: 60 | time: 1.56s | train loss:0.15412 | valid loss:0.29202 | train MseLoss:0.12375 | train PccLoss:0.42746 | valid MseLoss:0.24698 | valid PccLoss:0.69734\n",
            "end of epoch: 61 | time: 1.58s | train loss:0.15378 | valid loss:0.28906 | train MseLoss:0.12383 | train PccLoss:0.42341 | valid MseLoss:0.24317 | valid PccLoss:0.70210\n",
            "end of epoch: 62 | time: 1.54s | train loss:0.15421 | valid loss:0.30283 | train MseLoss:0.12406 | train PccLoss:0.42559 | valid MseLoss:0.25281 | valid PccLoss:0.75302\n",
            "end of epoch: 63 | time: 1.62s | train loss:0.15424 | valid loss:0.30269 | train MseLoss:0.12395 | train PccLoss:0.42679 | valid MseLoss:0.25897 | valid PccLoss:0.69610\n",
            "end of epoch: 64 | time: 1.54s | train loss:0.15468 | valid loss:0.29921 | train MseLoss:0.12503 | train PccLoss:0.42157 | valid MseLoss:0.25107 | valid PccLoss:0.73250\n",
            "end of epoch: 65 | time: 1.54s | train loss:0.15330 | valid loss:0.29495 | train MseLoss:0.12340 | train PccLoss:0.42235 | valid MseLoss:0.25331 | valid PccLoss:0.66970\n",
            "end of epoch: 66 | time: 1.56s | train loss:0.15499 | valid loss:0.27958 | train MseLoss:0.12501 | train PccLoss:0.42483 | valid MseLoss:0.23920 | valid PccLoss:0.64296\n",
            "end of epoch: 67 | time: 1.53s | train loss:0.15311 | valid loss:0.30204 | train MseLoss:0.12259 | train PccLoss:0.42778 | valid MseLoss:0.25681 | valid PccLoss:0.70913\n",
            "end of epoch: 68 | time: 1.55s | train loss:0.15017 | valid loss:0.28201 | train MseLoss:0.12067 | train PccLoss:0.41570 | valid MseLoss:0.24082 | valid PccLoss:0.65265\n",
            "end of epoch: 69 | time: 1.57s | train loss:0.15173 | valid loss:0.31257 | train MseLoss:0.12153 | train PccLoss:0.42358 | valid MseLoss:0.26789 | valid PccLoss:0.71462\n",
            "end of epoch: 70 | time: 1.54s | train loss:0.15259 | valid loss:0.30063 | train MseLoss:0.12266 | train PccLoss:0.42193 | valid MseLoss:0.26057 | valid PccLoss:0.66126\n",
            "end of epoch: 71 | time: 1.53s | train loss:0.15032 | valid loss:0.29595 | train MseLoss:0.12049 | train PccLoss:0.41884 | valid MseLoss:0.25246 | valid PccLoss:0.68732\n",
            "end of epoch: 72 | time: 1.53s | train loss:0.15224 | valid loss:0.28187 | train MseLoss:0.12202 | train PccLoss:0.42415 | valid MseLoss:0.23935 | valid PccLoss:0.66454\n",
            "end of epoch: 73 | time: 1.53s | train loss:0.15034 | valid loss:0.28172 | train MseLoss:0.12049 | train PccLoss:0.41894 | valid MseLoss:0.23978 | valid PccLoss:0.65917\n",
            "end of epoch: 74 | time: 1.53s | train loss:0.14931 | valid loss:0.30055 | train MseLoss:0.11985 | train PccLoss:0.41439 | valid MseLoss:0.25920 | valid PccLoss:0.67266\n",
            "end of epoch: 75 | time: 1.53s | train loss:0.14926 | valid loss:0.29904 | train MseLoss:0.11971 | train PccLoss:0.41521 | valid MseLoss:0.25737 | valid PccLoss:0.67401\n",
            "end of epoch: 76 | time: 1.56s | train loss:0.14919 | valid loss:0.27785 | train MseLoss:0.11948 | train PccLoss:0.41661 | valid MseLoss:0.23072 | valid PccLoss:0.70204\n",
            "end of epoch: 77 | time: 1.55s | train loss:0.14977 | valid loss:0.30929 | train MseLoss:0.11967 | train PccLoss:0.42066 | valid MseLoss:0.26576 | valid PccLoss:0.70105\n",
            "end of epoch: 78 | time: 1.58s | train loss:0.14868 | valid loss:0.32256 | train MseLoss:0.11892 | train PccLoss:0.41661 | valid MseLoss:0.28121 | valid PccLoss:0.69476\n",
            "end of epoch: 79 | time: 1.71s | train loss:0.15115 | valid loss:0.28142 | train MseLoss:0.12134 | train PccLoss:0.41940 | valid MseLoss:0.23557 | valid PccLoss:0.69407\n",
            "end of epoch: 80 | time: 1.58s | train loss:0.14968 | valid loss:0.29271 | train MseLoss:0.12007 | train PccLoss:0.41621 | valid MseLoss:0.24492 | valid PccLoss:0.72278\n",
            "end of epoch: 81 | time: 1.56s | train loss:0.14757 | valid loss:0.27048 | train MseLoss:0.11806 | train PccLoss:0.41311 | valid MseLoss:0.23154 | valid PccLoss:0.62093\n",
            "end of epoch: 82 | time: 1.53s | train loss:0.14855 | valid loss:0.28920 | train MseLoss:0.11913 | train PccLoss:0.41333 | valid MseLoss:0.25052 | valid PccLoss:0.63736\n",
            "end of epoch: 83 | time: 1.56s | train loss:0.14744 | valid loss:0.26990 | train MseLoss:0.11837 | train PccLoss:0.40904 | valid MseLoss:0.22520 | valid PccLoss:0.67224\n",
            "end of epoch: 84 | time: 1.53s | train loss:0.14832 | valid loss:0.27742 | train MseLoss:0.11846 | train PccLoss:0.41701 | valid MseLoss:0.23208 | valid PccLoss:0.68542\n",
            "end of epoch: 85 | time: 1.55s | train loss:0.14762 | valid loss:0.30588 | train MseLoss:0.11833 | train PccLoss:0.41122 | valid MseLoss:0.26110 | valid PccLoss:0.70890\n",
            "end of epoch: 86 | time: 1.56s | train loss:0.14940 | valid loss:0.27695 | train MseLoss:0.11976 | train PccLoss:0.41615 | valid MseLoss:0.23290 | valid PccLoss:0.67342\n",
            "end of epoch: 87 | time: 1.53s | train loss:0.14755 | valid loss:0.28520 | train MseLoss:0.11791 | train PccLoss:0.41427 | valid MseLoss:0.23907 | valid PccLoss:0.70043\n",
            "end of epoch: 88 | time: 1.53s | train loss:0.14824 | valid loss:0.31091 | train MseLoss:0.11850 | train PccLoss:0.41591 | valid MseLoss:0.26483 | valid PccLoss:0.72566\n",
            "end of epoch: 89 | time: 1.55s | train loss:0.14632 | valid loss:0.27701 | train MseLoss:0.11718 | train PccLoss:0.40857 | valid MseLoss:0.23211 | valid PccLoss:0.68112\n",
            "end of epoch: 90 | time: 1.53s | train loss:0.15024 | valid loss:0.30035 | train MseLoss:0.12038 | train PccLoss:0.41901 | valid MseLoss:0.25607 | valid PccLoss:0.69881\n",
            "end of epoch: 91 | time: 1.53s | train loss:0.14816 | valid loss:0.28245 | train MseLoss:0.11816 | train PccLoss:0.41813 | valid MseLoss:0.23781 | valid PccLoss:0.68424\n",
            "end of epoch: 92 | time: 1.54s | train loss:0.14703 | valid loss:0.29621 | train MseLoss:0.11764 | train PccLoss:0.41157 | valid MseLoss:0.25103 | valid PccLoss:0.70284\n",
            "end of epoch: 93 | time: 1.59s | train loss:0.14855 | valid loss:0.26552 | train MseLoss:0.11887 | train PccLoss:0.41568 | valid MseLoss:0.22306 | valid PccLoss:0.64773\n",
            "end of epoch: 94 | time: 1.55s | train loss:0.15068 | valid loss:0.31055 | train MseLoss:0.12110 | train PccLoss:0.41685 | valid MseLoss:0.26524 | valid PccLoss:0.71829\n",
            "end of epoch: 95 | time: 1.55s | train loss:0.14713 | valid loss:0.30210 | train MseLoss:0.11782 | train PccLoss:0.41087 | valid MseLoss:0.25884 | valid PccLoss:0.69151\n",
            "end of epoch: 96 | time: 1.53s | train loss:0.14753 | valid loss:0.28480 | train MseLoss:0.11815 | train PccLoss:0.41193 | valid MseLoss:0.24275 | valid PccLoss:0.66322\n",
            "end of epoch: 97 | time: 1.53s | train loss:0.14729 | valid loss:0.28467 | train MseLoss:0.11763 | train PccLoss:0.41423 | valid MseLoss:0.24197 | valid PccLoss:0.66898\n",
            "end of epoch: 98 | time: 1.53s | train loss:0.14695 | valid loss:0.28317 | train MseLoss:0.11749 | train PccLoss:0.41210 | valid MseLoss:0.23984 | valid PccLoss:0.67310\n",
            "end of epoch: 99 | time: 1.53s | train loss:0.14795 | valid loss:0.28925 | train MseLoss:0.11833 | train PccLoss:0.41459 | valid MseLoss:0.24708 | valid PccLoss:0.66878\n",
            "end of epoch:100 | time: 1.53s | train loss:0.14694 | valid loss:0.31943 | train MseLoss:0.11751 | train PccLoss:0.41181 | valid MseLoss:0.27751 | valid PccLoss:0.69675\n",
            "end of epoch:101 | time: 1.53s | train loss:0.14913 | valid loss:0.27849 | train MseLoss:0.11953 | train PccLoss:0.41552 | valid MseLoss:0.23873 | valid PccLoss:0.63640\n",
            "end of epoch:102 | time: 1.55s | train loss:0.14992 | valid loss:0.31133 | train MseLoss:0.12048 | train PccLoss:0.41489 | valid MseLoss:0.27305 | valid PccLoss:0.65588\n",
            "end of epoch:103 | time: 1.56s | train loss:0.14812 | valid loss:0.30165 | train MseLoss:0.11830 | train PccLoss:0.41650 | valid MseLoss:0.25772 | valid PccLoss:0.69701\n",
            "end of epoch:104 | time: 1.54s | train loss:0.14707 | valid loss:0.28492 | train MseLoss:0.11743 | train PccLoss:0.41386 | valid MseLoss:0.23694 | valid PccLoss:0.71673\n",
            "end of epoch:105 | time: 1.54s | train loss:0.14920 | valid loss:0.29794 | train MseLoss:0.11935 | train PccLoss:0.41788 | valid MseLoss:0.25867 | valid PccLoss:0.65138\n",
            "end of epoch:106 | time: 1.53s | train loss:0.14670 | valid loss:0.30263 | train MseLoss:0.11700 | train PccLoss:0.41400 | valid MseLoss:0.26206 | valid PccLoss:0.66773\n",
            "end of epoch:107 | time: 1.53s | train loss:0.14698 | valid loss:0.28943 | train MseLoss:0.11764 | train PccLoss:0.41105 | valid MseLoss:0.24676 | valid PccLoss:0.67350\n",
            "end of epoch:108 | time: 1.53s | train loss:0.14868 | valid loss:0.28472 | train MseLoss:0.11939 | train PccLoss:0.41226 | valid MseLoss:0.24136 | valid PccLoss:0.67490\n",
            "end of epoch:109 | time: 1.53s | train loss:0.14754 | valid loss:0.29034 | train MseLoss:0.11801 | train PccLoss:0.41337 | valid MseLoss:0.24764 | valid PccLoss:0.67466\n",
            "here\n",
            "==============================\n",
            "Test evaluate result:\n",
            "Average PCC: 0.3789558563123914\n",
            "Average MSE: 0.19222909212112427\n",
            "Average D: 0.1045770100096868\n",
            "==============================\n",
            "here2\n",
            "\n",
            "predict finish:\n",
            "predict GECs:(2000, 978)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# CRISPR GenSAN model\n",
        "print(\"CRISPR GenSAN model\")\n",
        "GenSAN_train, GenSAN_valid, GenSAN_test, _, _ = GenSAN_preprocessor(true_GECs1=CRISPR_GECs,\n",
        "                                                                    true_GECs2=CRISPR_GECs,\n",
        "                                                                    predict_GECs1=CRISPR_pre_GECs,\n",
        "                                                                    predict_GECs2=CRISPR_pre_GECs,\n",
        "                                                                    pre_GECS=CRISPR_pre_GECs,\n",
        "                                                                    input_path=\"drive/MyDrive/CRISPR/\",\n",
        "                                                                    file_name=\"feature_dict.pkl\")\n",
        "\n",
        "GenSAN_train_dataloader, GenSAN_valid_dataloader = get_dataloader(batch_size=32,\n",
        "                                                                  node_train=GenSAN_train,\n",
        "                                                                  gecs_train=CRISPR_GECs_train,\n",
        "                                                                  node_valid=GenSAN_valid,\n",
        "                                                                  gecs_valid=CRISPR_GECs_valid)\n",
        "\n",
        "input_matrix = get_pre_GECs(CRISPR_pre_GECs, CRISPR_pre_GECs, CRISPR_pre_GECs)\n",
        "\n",
        "CRISPR_predict_GECs = run_GenSAN_model(blocks=3,\n",
        "                                       GECs_dimension=978,\n",
        "                                       hidden_nodes=1024,\n",
        "                                       heads=2,\n",
        "                                       dropout_rate=0.05,\n",
        "                                       recycles=3,\n",
        "                                       learning_rate=0.0000045,\n",
        "                                       weight_decay=1e-5,\n",
        "                                       epochs=110,\n",
        "                                       train_dataloader=GenSAN_train_dataloader,\n",
        "                                       valid_dataloader=GenSAN_valid_dataloader,\n",
        "                                       beta=0.1,\n",
        "                                       warmup_epoch=5,\n",
        "                                       pre_gecs_test=GenSAN_test,\n",
        "                                       gecs_test=CRISPR_GECs_test,\n",
        "                                       save_path=\"drive/MyDrive/CRISPR/results/\",\n",
        "                                       input_matrix=input_matrix,\n",
        "                                       length=64,\n",
        "                                       pre_GECs=CRISPR_pre_GECs,\n",
        "                                       scaler=CRISPR_MMScaler,\n",
        "                                       name=\"CRISPR\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
